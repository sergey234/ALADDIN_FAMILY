#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Advanced ML Analytics –¥–ª—è ALADDIN Enhanced Services
–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏

–ê–≤—Ç–æ—Ä: ALADDIN Security Team
–í–µ—Ä—Å–∏—è: 3.0
–î–∞—Ç–∞: 2025-01-06
"""

import os
import sys
import json
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import sqlite3
import asyncio

# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
try:
    from sklearn.ensemble import IsolationForest, RandomForestClassifier
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, classification_report
    import joblib
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("Warning: ML libraries not available. Install scikit-learn for ML features.")

class MLPredictor:
    """ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å –¥–ª—è —Å–∏—Å—Ç–µ–º—ã ALADDIN"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_names = []
        self.db_path = "ml_analytics.db"
        self.init_database()
        
    def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML –¥–∞–Ω–Ω—ã—Ö"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS ml_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME NOT NULL,
                service_name TEXT NOT NULL,
                cpu_usage REAL NOT NULL,
                memory_usage REAL NOT NULL,
                response_time REAL NOT NULL,
                error_rate REAL NOT NULL,
                request_count INTEGER NOT NULL,
                anomaly_score REAL,
                predicted_status TEXT
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS ml_predictions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME NOT NULL,
                service_name TEXT NOT NULL,
                prediction_type TEXT NOT NULL,
                prediction_value REAL NOT NULL,
                confidence REAL NOT NULL,
                actual_value REAL,
                accuracy REAL
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def prepare_features(self, metrics_data: List[Dict]) -> np.ndarray:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è ML –º–æ–¥–µ–ª–∏"""
        features = []
        
        for metric in metrics_data:
            feature_vector = [
                metric.get('cpu_usage', 0),
                metric.get('memory_usage', 0),
                metric.get('response_time', 0),
                metric.get('error_rate', 0),
                metric.get('request_count', 0)
            ]
            features.append(feature_vector)
        
        return np.array(features)
    
    def train_anomaly_detection(self, metrics_data: List[Dict]) -> Dict[str, Any]:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π"""
        if not ML_AVAILABLE:
            return {"error": "ML libraries not available"}
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            X = self.prepare_features(metrics_data)
            
            if len(X) < 10:
                return {"error": "Not enough data for training"}
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å Isolation Forest
            model = IsolationForest(contamination=0.1, random_state=42)
            model.fit(X)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            self.models['anomaly_detection'] = model
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –∞–Ω–æ–º–∞–ª–∏–∏
            predictions = model.predict(X)
            scores = model.decision_function(X)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self._save_ml_metrics(metrics_data, scores, predictions)
            
            return {
                "status": "success",
                "model_type": "IsolationForest",
                "training_samples": len(X),
                "anomalies_detected": int(np.sum(predictions == -1)),
                "anomaly_rate": float(np.mean(predictions == -1))
            }
            
        except Exception as e:
            return {"error": f"Training failed: {str(e)}"}
    
    def train_status_prediction(self, metrics_data: List[Dict]) -> Dict[str, Any]:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ —Å–µ—Ä–≤–∏—Å–æ–≤"""
        if not ML_AVAILABLE:
            return {"error": "ML libraries not available"}
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            X = self.prepare_features(metrics_data)
            y = [1 if m.get('status') == 'running' else 0 for m in metrics_data]
            
            if len(X) < 10:
                return {"error": "Not enough data for training"}
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            model = RandomForestClassifier(n_estimators=100, random_state=42)
            model.fit(X_train_scaled, y_train)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º
            y_pred = model.predict(X_test_scaled)
            accuracy = accuracy_score(y_test, y_pred)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
            self.models['status_prediction'] = model
            self.scalers['status_prediction'] = scaler
            
            return {
                "status": "success",
                "model_type": "RandomForestClassifier",
                "training_samples": len(X_train),
                "test_samples": len(X_test),
                "accuracy": float(accuracy)
            }
            
        except Exception as e:
            return {"error": f"Training failed: {str(e)}"}
    
    def predict_anomalies(self, current_metrics: Dict) -> Dict[str, Any]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –¥–ª—è —Ç–µ–∫—É—â–∏—Ö –º–µ—Ç—Ä–∏–∫"""
        if 'anomaly_detection' not in self.models:
            return {"error": "Model not trained"}
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            X = self.prepare_features([current_metrics])
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º
            prediction = self.models['anomaly_detection'].predict(X)[0]
            score = self.models['anomaly_detection'].decision_function(X)[0]
            
            return {
                "is_anomaly": bool(prediction == -1),
                "anomaly_score": float(score),
                "confidence": float(abs(score)),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {"error": f"Prediction failed: {str(e)}"}
    
    def predict_status(self, current_metrics: Dict) -> Dict[str, Any]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–µ—Ä–≤–∏—Å–∞"""
        if 'status_prediction' not in self.models or 'status_prediction' not in self.scalers:
            return {"error": "Model not trained"}
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            X = self.prepare_features([current_metrics])
            X_scaled = self.scalers['status_prediction'].transform(X)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º
            prediction = self.models['status_prediction'].predict(X_scaled)[0]
            probabilities = self.models['status_prediction'].predict_proba(X_scaled)[0]
            
            return {
                "predicted_status": "running" if prediction == 1 else "stopped",
                "confidence": float(max(probabilities)),
                "probabilities": {
                    "running": float(probabilities[1]),
                    "stopped": float(probabilities[0])
                },
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {"error": f"Prediction failed: {str(e)}"}
    
    def generate_insights(self, metrics_data: List[Dict]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Å–∞–π—Ç–æ–≤"""
        if not metrics_data:
            return {"error": "No data available"}
        
        try:
            df = pd.DataFrame(metrics_data)
            
            insights = {
                "performance_trends": {
                    "avg_cpu": float(df['cpu_usage'].mean()),
                    "avg_memory": float(df['memory_usage'].mean()),
                    "avg_response_time": float(df['response_time'].mean()),
                    "total_requests": int(df['request_count'].sum())
                },
                "stability_metrics": {
                    "cpu_variance": float(df['cpu_usage'].var()),
                    "memory_variance": float(df['memory_usage'].var()),
                    "response_time_variance": float(df['response_time'].var())
                },
                "recommendations": []
            }
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            if df['cpu_usage'].mean() > 80:
                insights["recommendations"].append("High CPU usage detected. Consider scaling up.")
            
            if df['memory_usage'].mean() > 90:
                insights["recommendations"].append("High memory usage detected. Check for memory leaks.")
            
            if df['response_time'].mean() > 1.0:
                insights["recommendations"].append("Slow response times detected. Optimize performance.")
            
            if df['error_rate'].mean() > 0.05:
                insights["recommendations"].append("High error rate detected. Check service health.")
            
            return insights
            
        except Exception as e:
            return {"error": f"Insights generation failed: {str(e)}"}
    
    def _save_ml_metrics(self, metrics_data: List[Dict], scores: np.ndarray, predictions: np.ndarray):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ML –º–µ—Ç—Ä–∏–∫ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for i, metric in enumerate(metrics_data):
                cursor.execute('''
                    INSERT INTO ml_metrics 
                    (timestamp, service_name, cpu_usage, memory_usage, response_time, 
                     error_rate, request_count, anomaly_score, predicted_status)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    datetime.now(),
                    metric.get('service_name', 'unknown'),
                    metric.get('cpu_usage', 0),
                    metric.get('memory_usage', 0),
                    metric.get('response_time', 0),
                    metric.get('error_rate', 0),
                    metric.get('request_count', 0),
                    float(scores[i]) if i < len(scores) else 0,
                    'anomaly' if i < len(predictions) and predictions[i] == -1 else 'normal'
                ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            print(f"Error saving ML metrics: {e}")
    
    def get_ml_history(self, hours: int = 24) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ ML –¥–∞–Ω–Ω—ã—Ö"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT * FROM ml_metrics 
                WHERE timestamp > datetime('now', '-{} hours')
                ORDER BY timestamp DESC
            '''.format(hours))
            
            columns = [description[0] for description in cursor.description]
            results = []
            
            for row in cursor.fetchall():
                results.append(dict(zip(columns, row)))
            
            conn.close()
            return results
            
        except Exception as e:
            print(f"Error getting ML history: {e}")
            return []

class AdvancedAnalytics:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –¥–ª—è ALADDIN —Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self):
        self.ml_predictor = MLPredictor()
        self.trends_data = {}
        
    async def analyze_system_health(self, services_data: Dict) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã"""
        analysis = {
            "overall_health": "healthy",
            "critical_issues": [],
            "warnings": [],
            "recommendations": [],
            "ml_predictions": {},
            "timestamp": datetime.now().isoformat()
        }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Å–µ—Ä–≤–∏—Å
        for service_name, service_data in services_data.items():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
            if service_data.get('status') == 'stopped':
                analysis["critical_issues"].append(f"Service {service_name} is stopped")
                analysis["overall_health"] = "critical"
            
            if service_data.get('cpu_usage', 0) > 95:
                analysis["critical_issues"].append(f"Service {service_name} has critical CPU usage")
                analysis["overall_health"] = "critical"
            
            if service_data.get('memory_usage', 0) > 95:
                analysis["critical_issues"].append(f"Service {service_name} has critical memory usage")
                analysis["overall_health"] = "critical"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
            if service_data.get('cpu_usage', 0) > 80:
                analysis["warnings"].append(f"Service {service_name} has high CPU usage")
            
            if service_data.get('memory_usage', 0) > 80:
                analysis["warnings"].append(f"Service {service_name} has high memory usage")
            
            # ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            if ML_AVAILABLE:
                anomaly_prediction = self.ml_predictor.predict_anomalies(service_data)
                status_prediction = self.ml_predictor.predict_status(service_data)
                
                analysis["ml_predictions"][service_name] = {
                    "anomaly": anomaly_prediction,
                    "status": status_prediction
                }
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if analysis["critical_issues"]:
            analysis["recommendations"].append("Immediate action required for critical issues")
        
        if analysis["warnings"]:
            analysis["recommendations"].append("Monitor services with warnings closely")
        
        if not analysis["critical_issues"] and not analysis["warnings"]:
            analysis["recommendations"].append("System is running smoothly")
        
        return analysis
    
    async def generate_trend_analysis(self, historical_data: List[Dict]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤"""
        if not historical_data:
            return {"error": "No historical data available"}
        
        try:
            df = pd.DataFrame(historical_data)
            
            trends = {
                "cpu_trend": self._calculate_trend(df, 'cpu_usage'),
                "memory_trend": self._calculate_trend(df, 'memory_usage'),
                "response_time_trend": self._calculate_trend(df, 'response_time'),
                "error_rate_trend": self._calculate_trend(df, 'error_rate'),
                "forecast": self._generate_forecast(df),
                "seasonality": self._detect_seasonality(df),
                "timestamp": datetime.now().isoformat()
            }
            
            return trends
            
        except Exception as e:
            return {"error": f"Trend analysis failed: {str(e)}"}
    
    def _calculate_trend(self, df: pd.DataFrame, column: str) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç —Ç—Ä–µ–Ω–¥–∞ –¥–ª—è –∫–æ–ª–æ–Ω–∫–∏"""
        if column not in df.columns:
            return {"trend": "unknown", "slope": 0, "r_squared": 0}
        
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –ª–∏–Ω–µ–π–Ω—ã–π —Ç—Ä–µ–Ω–¥
            x = np.arange(len(df))
            y = df[column].values
            
            # –£–¥–∞–ª—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
            mask = ~np.isnan(y)
            x_clean = x[mask]
            y_clean = y[mask]
            
            if len(x_clean) < 2:
                return {"trend": "insufficient_data", "slope": 0, "r_squared": 0}
            
            # –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
            slope = np.polyfit(x_clean, y_clean, 1)[0]
            
            # R-squared
            y_pred = slope * x_clean + np.mean(y_clean)
            ss_res = np.sum((y_clean - y_pred) ** 2)
            ss_tot = np.sum((y_clean - np.mean(y_clean)) ** 2)
            r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç—Ä–µ–Ω–¥
            if abs(slope) < 0.01:
                trend = "stable"
            elif slope > 0:
                trend = "increasing"
            else:
                trend = "decreasing"
            
            return {
                "trend": trend,
                "slope": float(slope),
                "r_squared": float(r_squared),
                "confidence": "high" if r_squared > 0.7 else "medium" if r_squared > 0.4 else "low"
            }
            
        except Exception as e:
            return {"trend": "error", "slope": 0, "r_squared": 0, "error": str(e)}
    
    def _generate_forecast(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–µ–Ω–¥–∞
            forecast = {}
            
            for column in ['cpu_usage', 'memory_usage', 'response_time']:
                if column in df.columns:
                    recent_data = df[column].tail(10).values
                    if len(recent_data) > 0:
                        # –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ —Å —Ç—Ä–µ–Ω–¥–æ–º
                        trend = np.polyfit(range(len(recent_data)), recent_data, 1)[0]
                        forecast[column] = {
                            "next_hour": float(recent_data[-1] + trend),
                            "next_6_hours": float(recent_data[-1] + trend * 6),
                            "next_24_hours": float(recent_data[-1] + trend * 24)
                        }
            
            return forecast
            
        except Exception as e:
            return {"error": str(e)}
    
    def _detect_seasonality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏"""
        try:
            seasonality = {}
            
            for column in ['cpu_usage', 'memory_usage', 'response_time']:
                if column in df.columns and len(df) > 24:  # –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 24 —Ç–æ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö
                    data = df[column].values
                    
                    # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å (—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º –ø–µ—Ä–∏–æ–¥–æ–º)
                    if len(data) >= 48:  # 48 —á–∞—Å–æ–≤ –¥–∞–Ω–Ω—ã—Ö
                        current_period = data[-24:]
                        previous_period = data[-48:-24]
                        
                        correlation = np.corrcoef(current_period, previous_period)[0, 1]
                        
                        seasonality[column] = {
                            "has_seasonality": abs(correlation) > 0.5,
                            "correlation": float(correlation),
                            "pattern": "daily" if abs(correlation) > 0.5 else "none"
                        }
            
            return seasonality
            
        except Exception as e:
            return {"error": str(e)}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
advanced_analytics = AdvancedAnalytics()

if __name__ == "__main__":
    print("ü§ñ ALADDIN Advanced ML Analytics")
    print("=" * 50)
    
    if not ML_AVAILABLE:
        print("‚ö†Ô∏è  ML libraries not available. Install scikit-learn for full functionality.")
    else:
        print("‚úÖ ML libraries available. Full functionality enabled.")
    
    print("üìä Advanced analytics ready for ALADDIN Enhanced Services")