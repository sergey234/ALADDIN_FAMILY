#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ALADDIN Comprehensive Duplicate Analyzer
–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ 405 —Ñ—É–Ω–∫—Ü–∏–π –≤ SFM –∏ –Ω–∞—Ö–æ–¥–∏—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã
"""

import os
import json
import re
from datetime import datetime
from pathlib import Path
from collections import defaultdict
import hashlib

class ComprehensiveDuplicateAnalyzer:
    def __init__(self):
        self.project_root = Path("/Users/sergejhlystov/ALADDIN_NEW")
        self.security_dir = self.project_root / "security"
        self.sfm_registry = self.project_root / "data" / "sfm" / "function_registry.json"
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        self.duplicate_groups = defaultdict(list)
        self.function_analysis = {}
        self.sfm_functions = {}
        self.analysis_results = {
            "total_files_analyzed": 0,
            "duplicate_groups_found": 0,
            "total_duplicates": 0,
            "potential_space_savings": 0,
            "analysis_timestamp": datetime.now().isoformat()
        }
        
    def load_sfm_functions(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ SFM registry"""
        try:
            with open(self.sfm_registry, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.sfm_functions = data
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} —Ñ—É–Ω–∫—Ü–∏–π –∏–∑ SFM registry")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ SFM registry: {e}")
            
    def analyze_file(self, file_path):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª"""
        try:
            stat = file_path.stat()
            size = stat.st_size
            lines = 0
            functions = []
            classes = []
            
            # –ü–æ–¥—Å—á–µ—Ç —Å—Ç—Ä–æ–∫ –∏ —Ñ—É–Ω–∫—Ü–∏–π
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = len(content.splitlines())
                
                # –ü–æ–∏—Å–∫ —Ñ—É–Ω–∫—Ü–∏–π
                func_matches = re.findall(r'def\s+(\w+)\s*\(', content)
                functions = func_matches
                
                # –ü–æ–∏—Å–∫ –∫–ª–∞—Å—Å–æ–≤
                class_matches = re.findall(r'class\s+(\w+)', content)
                classes = class_matches
                
            return {
                "path": str(file_path),
                "name": file_path.name,
                "size_bytes": size,
                "lines": lines,
                "functions": functions,
                "classes": classes,
                "modified_time": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                "created_time": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                "content_hash": hashlib.md5(content.encode()).hexdigest()[:16]
            }
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return None
            
    def find_duplicate_groups(self):
        """–ù–∞—Ö–æ–¥–∏—Ç –≥—Ä—É–ø–ø—ã –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Ñ–∞–π–ª–æ–≤"""
        print("üîç –ü–æ–∏—Å–∫ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ –±–∞–∑–æ–≤–æ–º—É –∏–º–µ–Ω–∏
        name_groups = defaultdict(list)
        
        # –°–∫–∞–Ω–∏—Ä—É–µ–º –≤—Å–µ Python —Ñ–∞–π–ª—ã –≤ security
        for py_file in self.security_dir.rglob("*.py"):
            if self.analyze_file(py_file):
                file_info = self.analyze_file(py_file)
                if file_info:
                    self.analysis_results["total_files_analyzed"] += 1
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤–æ–µ –∏–º—è (–±–µ–∑ —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤)
                    base_name = self.extract_base_name(py_file.name)
                    name_groups[base_name].append(file_info)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≥—Ä—É–ø–ø—ã
        for base_name, files in name_groups.items():
            if len(files) > 1:
                self.analyze_duplicate_group(base_name, files)
                
        self.analysis_results["duplicate_groups_found"] = len(self.duplicate_groups)
        
    def extract_base_name(self, filename):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –±–∞–∑–æ–≤–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤"""
        # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        name = filename.replace('.py', '')
        
        # –£–±–∏—Ä–∞–µ–º —Å—É—Ñ—Ñ–∏–∫—Å—ã backup, extra, main, v2, original
        suffixes_to_remove = [
            r'_backup_\d{8}_\d{6}',
            r'_backup',
            r'_extra',
            r'_main',
            r'_v2',
            r'_original_backup_\d{8}_\d{6}',
            r'_original',
            r'_a_plus',
            r'_enhanced',
            r'_old',
            r'_new'
        ]
        
        for suffix in suffixes_to_remove:
            name = re.sub(suffix, '', name)
            
        return name
        
    def analyze_duplicate_group(self, base_name, files):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥—Ä—É–ø–ø—É –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Ñ–∞–π–ª–æ–≤"""
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
        files.sort(key=lambda x: x['modified_time'], reverse=True)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        functionality_analysis = self.compare_functionality(files)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª
        main_file = self.determine_main_file(files)
        
        group_info = {
            "base_name": base_name,
            "total_files": len(files),
            "main_file": main_file,
            "files": files,
            "functionality_analysis": functionality_analysis,
            "recommendations": self.generate_recommendations(files, main_file, functionality_analysis)
        }
        
        self.duplicate_groups[base_name] = group_info
        self.analysis_results["total_duplicates"] += len(files) - 1
        
    def compare_functionality(self, files):
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–æ–≤ –≤ –≥—Ä—É–ø–ø–µ"""
        analysis = {
            "identical_files": [],
            "similar_files": [],
            "unique_files": [],
            "function_overlap": {},
            "class_overlap": {}
        }
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ö–µ—à–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        content_hashes = [f['content_hash'] for f in files]
        unique_hashes = set(content_hashes)
        
        for file_info in files:
            hash_count = content_hashes.count(file_info['content_hash'])
            if hash_count > 1:
                analysis["identical_files"].append(file_info['name'])
            else:
                analysis["unique_files"].append(file_info['name'])
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —Ñ—É–Ω–∫—Ü–∏–π
        all_functions = set()
        for file_info in files:
            all_functions.update(file_info['functions'])
            
        for file_info in files:
            file_functions = set(file_info['functions'])
            overlap = len(file_functions.intersection(all_functions)) / len(all_functions) if all_functions else 0
            analysis["function_overlap"][file_info['name']] = overlap
            
        return analysis
        
    def determine_main_file(self, files):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª –≤ –≥—Ä—É–ø–ø–µ"""
        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ñ–∞–π–ª–∞:
        # 1. –°–∞–º—ã–π –Ω–æ–≤—ã–π
        # 2. –°–∞–º—ã–π –±–æ–ª—å—à–æ–π
        # 3. –°–æ–¥–µ—Ä–∂–∏—Ç "main" –≤ –∏–º–µ–Ω–∏
        # 4. –ù–µ —Å–æ–¥–µ—Ä–∂–∏—Ç "backup" –≤ –∏–º–µ–Ω–∏
        
        main_candidates = []
        
        for file_info in files:
            score = 0
            
            # –ë–æ–Ω—É—Å –∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ backup –≤ –∏–º–µ–Ω–∏
            if 'backup' not in file_info['name'].lower():
                score += 100
                
            # –ë–æ–Ω—É—Å –∑–∞ "main" –≤ –∏–º–µ–Ω–∏
            if 'main' in file_info['name'].lower():
                score += 50
                
            # –ë–æ–Ω—É—Å –∑–∞ "a_plus" –≤ –∏–º–µ–Ω–∏
            if 'a_plus' in file_info['name'].lower():
                score += 75
                
            # –ë–æ–Ω—É—Å –∑–∞ —Ä–∞–∑–º–µ—Ä (–±–æ–ª—å—à–µ = –ª—É—á—à–µ)
            score += file_info['lines'] / 100
            
            # –ë–æ–Ω—É—Å –∑–∞ –Ω–æ–≤–∏–∑–Ω—É (–±–æ–ª–µ–µ –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã)
            try:
                mod_time = datetime.fromisoformat(file_info['modified_time'])
                days_old = (datetime.now() - mod_time).days
                score += max(0, 30 - days_old)
            except:
                pass
                
            main_candidates.append((score, file_info))
            
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–∏–π
        main_candidates.sort(key=lambda x: x[0], reverse=True)
        return main_candidates[0][1] if main_candidates else files[0]
        
    def generate_recommendations(self, files, main_file, functionality_analysis):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        recommendations = {
            "keep": [main_file['name']],
            "delete": [],
            "merge": [],
            "reasoning": []
        }
        
        main_functions = set(main_file['functions'])
        
        for file_info in files:
            if file_info['name'] == main_file['name']:
                continue
                
            file_functions = set(file_info['functions'])
            
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –∏–¥–µ–Ω—Ç–∏—á–µ–Ω –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
            if file_info['content_hash'] == main_file['content_hash']:
                recommendations["delete"].append(file_info['name'])
                recommendations["reasoning"].append(f"{file_info['name']} –∏–¥–µ–Ω—Ç–∏—á–µ–Ω {main_file['name']}")
                
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –∏–º–µ–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
            elif file_functions - main_functions:
                unique_funcs = file_functions - main_functions
                recommendations["merge"].append({
                    "file": file_info['name'],
                    "unique_functions": list(unique_funcs),
                    "reasoning": f"–°–æ–¥–µ—Ä–∂–∏—Ç {len(unique_funcs)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π"
                })
                
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ –∏ –Ω–µ –∏–º–µ–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
            elif file_info['lines'] < main_file['lines'] * 0.7:
                recommendations["delete"].append(file_info['name'])
                recommendations["reasoning"].append(f"{file_info['name']} –º–µ–Ω—å—à–µ –∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π")
                
            else:
                recommendations["keep"].append(file_info['name'])
                recommendations["reasoning"].append(f"{file_info['name']} –∏–º–µ–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å")
                
        return recommendations
        
    def analyze_sfm_functions(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –≤ SFM registry"""
        print("üîç –ê–Ω–∞–ª–∏–∑ —Ñ—É–Ω–∫—Ü–∏–π –≤ SFM registry...")
        
        if not self.sfm_functions:
            print("‚ùå SFM registry –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")
            return
            
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ –∏–º–µ–Ω–∞–º
        function_groups = defaultdict(list)
        
        for func_name, func_data in self.sfm_functions.items():
            if isinstance(func_data, dict):
                base_name = self.extract_base_name(func_name)
                function_groups[base_name].append({
                    "name": func_name,
                    "data": func_data
                })
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Ñ—É–Ω–∫—Ü–∏–∏
        sfm_duplicates = {}
        for base_name, functions in function_groups.items():
            if len(functions) > 1:
                sfm_duplicates[base_name] = functions
                
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(sfm_duplicates)} –≥—Ä—É–ø–ø –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Ñ—É–Ω–∫—Ü–∏–π –≤ SFM")
        return sfm_duplicates
        
    def generate_report(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç"""
        report = {
            "analysis_summary": self.analysis_results,
            "duplicate_groups": dict(self.duplicate_groups),
            "sfm_analysis": self.analyze_sfm_functions(),
            "recommendations": self.generate_overall_recommendations()
        }
        
        return report
        
    def generate_overall_recommendations(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"""
        total_deletable = 0
        total_space_savings = 0
        
        for group_name, group_info in self.duplicate_groups.items():
            for file_info in group_info["files"]:
                if file_info['name'] != group_info["main_file"]['name']:
                    total_deletable += 1
                    total_space_savings += file_info['lines']
                    
        return {
            "total_files_to_delete": total_deletable,
            "estimated_space_savings_lines": total_space_savings,
            "estimated_space_savings_mb": total_space_savings * 50 / 1024 / 1024,  # –ü—Ä–∏–º–µ—Ä–Ω–æ 50 –±–∞–π—Ç –Ω–∞ —Å—Ç—Ä–æ–∫—É
            "priority_actions": [
                "–£–¥–∞–ª–∏—Ç—å –≤—Å–µ backup —Ñ–∞–π–ª—ã",
                "–£–¥–∞–ª–∏—Ç—å identical —Ñ–∞–π–ª—ã",
                "–û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ merge —Ñ–∞–π–ª–æ–≤ –≤ –æ—Å–Ω–æ–≤–Ω—ã–µ",
                "–û—á–∏—Å—Ç–∏—Ç—å SFM registry –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"
            ]
        }
        
    def run_analysis(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑"""
        print("üöÄ –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
        print("=" * 80)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º SFM —Ñ—É–Ω–∫—Ü–∏–∏
        self.load_sfm_functions()
        
        # –ù–∞—Ö–æ–¥–∏–º –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –≥—Ä—É–ø–ø—ã
        self.find_duplicate_groups()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report = self.generate_report()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        report_path = self.project_root / "backups" / f"DUPLICATE_ANALYSIS_REPORT_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
            
        print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
        
        # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É
        self.print_summary(report)
        
        return report
        
    def print_summary(self, report):
        """–í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print("\n" + "=" * 80)
        print("üìä –°–í–û–î–ö–ê –ê–ù–ê–õ–ò–ó–ê –î–£–ë–õ–ò–†–£–Æ–©–ò–•–°–Ø –ö–û–ú–ü–û–ù–ï–ù–¢–û–í")
        print("=" * 80)
        
        summary = report["analysis_summary"]
        print(f"üìÅ –§–∞–π–ª–æ–≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {summary['total_files_analyzed']}")
        print(f"üîç –ì—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {summary['duplicate_groups_found']}")
        print(f"üóëÔ∏è –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è: {summary['total_duplicates']}")
        
        recommendations = report["recommendations"]
        print(f"üíæ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è –º–µ—Å—Ç–∞: {recommendations['estimated_space_savings_lines']:,} —Å—Ç—Ä–æ–∫")
        print(f"üì¶ –≠–∫–æ–Ω–æ–º–∏—è –≤ MB: {recommendations['estimated_space_savings_mb']:.2f} MB")
        
        print("\nüèÜ –¢–û–ü-10 –ì–†–£–ü–ü –î–£–ë–õ–ò–ö–ê–¢–û–í:")
        duplicate_groups = report["duplicate_groups"]
        sorted_groups = sorted(duplicate_groups.items(), 
                             key=lambda x: len(x[1]["files"]), reverse=True)[:10]
        
        for i, (group_name, group_info) in enumerate(sorted_groups, 1):
            print(f"{i:2d}. {group_name.upper()}: {len(group_info['files'])} —Ñ–∞–π–ª–æ–≤")
            main_file = group_info["main_file"]
            print(f"    üìå –û—Å–Ω–æ–≤–Ω–æ–π: {main_file['name']} ({main_file['lines']} —Å—Ç—Ä–æ–∫)")
            
            for file_info in group_info["files"]:
                if file_info['name'] != main_file['name']:
                    status = "üóëÔ∏è –£–î–ê–õ–ò–¢–¨" if file_info['name'] in group_info["recommendations"]["delete"] else "‚ö†Ô∏è –ü–†–û–í–ï–†–ò–¢–¨"
                    print(f"    {status}: {file_info['name']} ({file_info['lines']} —Å—Ç—Ä–æ–∫)")
            print()

if __name__ == "__main__":
    analyzer = ComprehensiveDuplicateAnalyzer()
    analyzer.run_analysis()