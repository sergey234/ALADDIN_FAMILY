#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ NaturalLanguageProcessor
–°–æ–∑–¥–∞–Ω: 2024-09-05
–í–µ—Ä—Å–∏—è: 1.0.0
"""

import os
import sys
import json
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

def test_natural_language_quality():
    """–¢–µ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ NaturalLanguageProcessor"""
    print("üéØ –¢–ï–°–¢ –ö–ê–ß–ï–°–¢–í–ê NATURALLANGUAGEPROCESSOR")
    print("=" * 60)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
    file_path = "security/ai_agents/natural_language_processor.py"
    if not os.path.exists(file_path):
        print("‚ùå –§–∞–π–ª NaturalLanguageProcessor –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return False
    
    print("‚úÖ –§–∞–π–ª NaturalLanguageProcessor –Ω–∞–π–¥–µ–Ω")
    
    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–¥
    lines = content.split('\n')
    total_lines = len(lines)
    code_lines = len([line for line in lines if line.strip() and not line.strip().startswith('#')])
    
    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–û–î–ê:")
    print(f"   üìÑ –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total_lines}")
    print(f"   üíª –°—Ç—Ä–æ–∫ –∫–æ–¥–∞: {code_lines}")
    print(f"   üìà –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–æ–¥–∞: {code_lines/total_lines*100:.1f}%")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    components = {
        "–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫": content.count("try:") + content.count("except"),
        "–ö–ª–∞—Å—Å—ã": content.count("class "),
        "–ú–µ—Ç–æ–¥—ã": content.count("def "),
        "–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è": content.count('"""') + content.count("'''"),
        "–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ": content.count("logger") + content.count("logging"),
        "–¢–∏–ø–∏–∑–∞—Ü–∏—è": content.count(": str") + content.count(": int") + content.count(": bool"),
        "NLP": content.count("nlp") + content.count("NLP") + content.count("natural"),
        "–ù–∞–º–µ—Ä–µ–Ω–∏—è": content.count("intent") + content.count("Intent"),
        "–°—É—â–Ω–æ—Å—Ç–∏": content.count("entity") + content.count("Entity"),
        "–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å": content.count("sentiment") + content.count("Sentiment"),
        "–ö–æ–Ω—Ç–µ–∫—Å—Ç": content.count("context") + content.count("Context"),
        "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": content.count("keyword") + content.count("Keyword"),
        "–≠–º–æ—Ü–∏–∏": content.count("emotion") + content.count("Emotion"),
        "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è": content.count("token") + content.count("Token"),
        "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å": content.count("security") + content.count("Security"),
        "–¶–≤–µ—Ç–æ–≤–∞—è —Å—Ö–µ–º–∞": content.count("color_scheme") + content.count("Matrix AI"),
        "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ": content.count("test_") + content.count("_test_")
    }
    
    print(f"\nüîß –ö–û–ú–ü–û–ù–ï–ù–¢–´ –°–ò–°–¢–ï–ú–´:")
    for component, count in components.items():
        print(f"   {component}: {count}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
    quality_checks = {
        "–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è": components["–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"] > 20,
        "–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫": components["–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫"] > 10,
        "–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ": components["–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"] > 5,
        "–¢–∏–ø–∏–∑–∞—Ü–∏—è": components["–¢–∏–ø–∏–∑–∞—Ü–∏—è"] > 10,
        "NLP": components["NLP"] > 5,
        "–ù–∞–º–µ—Ä–µ–Ω–∏—è": components["–ù–∞–º–µ—Ä–µ–Ω–∏—è"] > 5,
        "–°—É—â–Ω–æ—Å—Ç–∏": components["–°—É—â–Ω–æ—Å—Ç–∏"] > 5,
        "–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å": components["–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å"] > 5,
        "–ö–æ–Ω—Ç–µ–∫—Å—Ç": components["–ö–æ–Ω—Ç–µ–∫—Å—Ç"] > 5,
        "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": components["–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"] > 5,
        "–≠–º–æ—Ü–∏–∏": components["–≠–º–æ—Ü–∏–∏"] > 5,
        "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è": components["–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"] > 5,
        "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ": components["–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"] > 5,
        "–¶–≤–µ—Ç–æ–≤–∞—è —Å—Ö–µ–º–∞": components["–¶–≤–µ—Ç–æ–≤–∞—è —Å—Ö–µ–º–∞"] > 5,
        "–ü–æ–∫—Ä—ã—Ç–∏–µ –∫–æ–¥–∞": code_lines >= 500
    }
    
    print(f"\nüèóÔ∏è –ü–†–û–í–ï–†–ö–ê –ö–ê–ß–ï–°–¢–í–ê:")
    for check, passed in quality_checks.items():
        status = "‚úÖ –ü–†–û–ô–î–ï–ù–û" if passed else "‚ùå –ù–ï –ü–†–û–ô–î–ï–ù–û"
        print(f"   {check}: {status}")
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –±–∞–ª–ª—ã
    total_checks = len(quality_checks)
    passed_checks = sum(quality_checks.values())
    quality_score = (passed_checks / total_checks) * 100
    
    print(f"\nüèÜ –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê: {quality_score:.1f}/100")
    
    if quality_score >= 95:
        print("‚úÖ –ö–ê–ß–ï–°–¢–í–û: A+ (–û–¢–õ–ò–ß–ù–û)")
    elif quality_score >= 90:
        print("‚úÖ –ö–ê–ß–ï–°–¢–í–û: A (–û–ß–ï–ù–¨ –•–û–†–û–®–û)")
    elif quality_score >= 80:
        print("‚ö†Ô∏è –ö–ê–ß–ï–°–¢–í–û: B (–•–û–†–û–®–û)")
    else:
        print("‚ùå –ö–ê–ß–ï–°–¢–í–û: C (–¢–†–ï–ë–£–ï–¢ –£–õ–£–ß–®–ï–ù–ò–Ø)")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    os.makedirs("data/quality_reports", exist_ok=True)
    report = {
        "component": "NaturalLanguageProcessor",
        "version": "1.0.0",
        "quality_score": quality_score,
        "quality_grade": "A+" if quality_score >= 95 else "A" if quality_score >= 90 else "B",
        "code_statistics": {
            "total_lines": total_lines,
            "code_lines": code_lines,
            "comment_lines": total_lines - code_lines,
            "code_density": code_lines/total_lines*100
        },
        "components": components,
        "quality_checks": quality_checks,
        "generated_at": datetime.now().isoformat()
    }
    
    report_file = f"data/quality_reports/natural_language_quality_test_{int(datetime.now().timestamp())}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
    
    if quality_score >= 95:
        print("üéâ NATURALLANGUAGEPROCESSOR –°–û–û–¢–í–ï–¢–°–¢–í–£–ï–¢ –°–¢–ê–ù–î–ê–†–¢–ê–ú A+ –ö–ê–ß–ï–°–¢–í–ê!")
        return True
    else:
        print(f"‚ö†Ô∏è –¢–†–ï–ë–£–ï–¢–°–Ø –î–û–†–ê–ë–û–¢–ö–ê –î–û A+ –ö–ê–ß–ï–°–¢–í–ê (–Ω–µ–¥–æ—Å—Ç–∞–µ—Ç {100-quality_score:.1f} –±–∞–ª–ª–æ–≤)")
        return False

if __name__ == "__main__":
    success = test_natural_language_quality()
    if success:
        print("\n‚úÖ –¢–ï–°–¢ –ö–ê–ß–ï–°–¢–í–ê –ü–†–û–ô–î–ï–ù –£–°–ü–ï–®–ù–û!")
    else:
        print("\n‚ùå –¢–ï–°–¢ –ö–ê–ß–ï–°–¢–í–ê –ù–ï –ü–†–û–ô–î–ï–ù!")