#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ALADDIN Detailed SFM Duplicate Analyzer
–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö 404 —Ñ—É–Ω–∫—Ü–∏–π –≤ SFM –∏ –∏—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
"""

import os
import json
import hashlib
from datetime import datetime
from pathlib import Path
from collections import defaultdict
import re

class DetailedSFMDuplicateAnalyzer:
    def __init__(self):
        self.project_root = Path("/Users/sergejhlystov/ALADDIN_NEW")
        self.sfm_registry = self.project_root / "data" / "sfm" / "function_registry.json"
        self.security_dir = self.project_root / "security"
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        self.sfm_functions = {}
        self.file_analysis = {}
        self.duplicate_groups = defaultdict(list)
        self.deletion_candidates = []
        self.keep_candidates = []
        
    def load_sfm_functions(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ SFM registry"""
        try:
            with open(self.sfm_registry, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.sfm_functions = data
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} —Ñ—É–Ω–∫—Ü–∏–π –∏–∑ SFM registry")
                return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ SFM registry: {e}")
            return False
            
    def analyze_file_content(self, file_path):
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            lines = len(content.splitlines())
            size = len(content.encode('utf-8'))
            
            # –ê–Ω–∞–ª–∏–∑ —Ñ—É–Ω–∫—Ü–∏–π
            functions = re.findall(r'def\s+(\w+)\s*\(', content)
            classes = re.findall(r'class\s+(\w+)', content)
            
            # –ê–Ω–∞–ª–∏–∑ –∏–º–ø–æ—Ä—Ç–æ–≤
            imports = re.findall(r'^(?:from\s+\S+\s+)?import\s+([^\n]+)', content, re.MULTILINE)
            
            # –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            dependencies = set()
            for imp in imports:
                if 'from' in imp:
                    deps = re.findall(r'from\s+(\S+)', imp)
                    dependencies.update(deps)
                else:
                    deps = re.findall(r'import\s+(\S+)', imp)
                    dependencies.update(deps)
            
            # –•–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            content_hash = hashlib.md5(content.encode()).hexdigest()
            
            # –ê–Ω–∞–ª–∏–∑ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            functionality_score = self.calculate_functionality_score(content, functions, classes)
            
            return {
                "path": str(file_path),
                "name": file_path.name,
                "lines": lines,
                "size_bytes": size,
                "functions": functions,
                "classes": classes,
                "imports": list(imports),
                "dependencies": list(dependencies),
                "content_hash": content_hash,
                "functionality_score": functionality_score,
                "is_backup": any(suffix in file_path.name.lower() for suffix in ['_backup', '_original', '_old']),
                "is_main": 'main' in file_path.name.lower(),
                "is_extra": 'extra' in file_path.name.lower(),
                "is_v2": 'v2' in file_path.name.lower(),
                "is_a_plus": 'a_plus' in file_path.name.lower(),
                "modified_time": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
            }
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return None
            
    def calculate_functionality_score(self, content, functions, classes):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ñ–∞–π–ª–∞"""
        score = 0
        
        # –ë–∞–∑–æ–≤—ã–µ –æ—á–∫–∏
        score += len(functions) * 10  # 10 –æ—á–∫–æ–≤ –∑–∞ —Ñ—É–Ω–∫—Ü–∏—é
        score += len(classes) * 20    # 20 –æ—á–∫–æ–≤ –∑–∞ –∫–ª–∞—Å—Å
        
        # –û—á–∫–∏ –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        functionality_keywords = [
            'async', 'await', 'ml', 'ai', 'machine_learning', 'neural_network',
            'database', 'redis', 'sql', 'api', 'http', 'websocket',
            'security', 'encryption', 'authentication', 'authorization',
            'monitoring', 'logging', 'metrics', 'analytics', 'detection',
            'prevention', 'response', 'analysis', 'prediction', 'learning'
        ]
        
        for keyword in functionality_keywords:
            if keyword in content.lower():
                score += 5
                
        # –û—á–∫–∏ –∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        if 'try:' in content and 'except:' in content:
            score += 10  # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
        if 'threading' in content or 'asyncio' in content:
            score += 15  # –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å
        if 'json' in content and 'load' in content:
            score += 5   # –†–∞–±–æ—Ç–∞ —Å –¥–∞–Ω–Ω—ã–º–∏
        if 'logging' in content:
            score += 5   # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            
        return score
        
    def find_duplicate_groups(self):
        """–ù–∞—Ö–æ–¥–∏—Ç –≥—Ä—É–ø–ø—ã –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Ñ–∞–π–ª–æ–≤"""
        print("üîç –ü–æ–∏—Å–∫ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
        
        # –°–∫–∞–Ω–∏—Ä—É–µ–º –≤—Å–µ Python —Ñ–∞–π–ª—ã
        for py_file in self.security_dir.rglob("*.py"):
            file_info = self.analyze_file_content(py_file)
            if file_info:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤–æ–µ –∏–º—è
                base_name = self.extract_base_name(py_file.name)
                self.duplicate_groups[base_name].append(file_info)
                
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –≥—Ä—É–ø–ø—ã —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏
        duplicate_groups = {k: v for k, v in self.duplicate_groups.items() if len(v) > 1}
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(duplicate_groups)} –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        return duplicate_groups
        
    def extract_base_name(self, filename):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –±–∞–∑–æ–≤–æ–µ –∏–º—è —Ñ–∞–π–ª–∞"""
        name = filename.replace('.py', '')
        
        # –£–±–∏—Ä–∞–µ–º —Å—É—Ñ—Ñ–∏–∫—Å—ã
        suffixes = [
            r'_backup_\d{8}_\d{6}',
            r'_backup',
            r'_original_backup_\d{8}_\d{6}',
            r'_original',
            r'_extra',
            r'_main',
            r'_v2',
            r'_a_plus',
            r'_enhanced',
            r'_old',
            r'_new'
        ]
        
        for suffix in suffixes:
            name = re.sub(suffix, '', name)
            
        return name
        
    def analyze_duplicate_group(self, group_name, files):
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≥—Ä—É–ø–ø—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (—É–±—ã–≤–∞–Ω–∏–µ)
        files.sort(key=lambda x: x['functionality_score'], reverse=True)
        
        analysis = {
            "group_name": group_name,
            "total_files": len(files),
            "files": files,
            "main_file": files[0],  # –°–∞–º—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π
            "deletion_candidates": [],
            "keep_candidates": [],
            "reasoning": []
        }
        
        main_file = files[0]
        main_functions = set(main_file['functions'])
        main_classes = set(main_file['classes'])
        
        for file_info in files[1:]:
            file_functions = set(file_info['functions'])
            file_classes = set(file_info['classes'])
            
            # –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
            function_overlap = len(file_functions.intersection(main_functions)) / len(main_functions) if main_functions else 0
            class_overlap = len(file_classes.intersection(main_classes)) / len(main_classes) if main_classes else 0
            
            # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
            should_delete = False
            reason = ""
            
            # 1. –ò–¥–µ–Ω—Ç–∏—á–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            if file_info['content_hash'] == main_file['content_hash']:
                should_delete = True
                reason = f"100% –∏–¥–µ–Ω—Ç–∏—á–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å {main_file['name']}"
                
            # 2. –ë—ç–∫–∞–ø —Ñ–∞–π–ª
            elif file_info['is_backup']:
                should_delete = True
                reason = f"–ë—ç–∫–∞–ø —Ñ–∞–π–ª: {file_info['name']}"
                
            # 3. –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            elif file_info['functionality_score'] < main_file['functionality_score'] * 0.5:
                should_delete = True
                ratio = main_file['functionality_score'] / max(file_info['functionality_score'], 1)
                reason = f"–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –≤ {ratio:.1f} —Ä–∞–∑ –º–µ–Ω—å—à–µ ({file_info['functionality_score']} vs {main_file['functionality_score']})"
                
            # 4. –í—ã—Å–æ–∫–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –±–µ–∑ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö
            elif function_overlap > 0.8 and not (file_functions - main_functions):
                should_delete = True
                reason = f"–í—ã—Å–æ–∫–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —Ñ—É–Ω–∫—Ü–∏–π ({function_overlap:.1%}) –±–µ–∑ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö"
                
            # 5. –°—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è (main vs –ø–æ–ª–Ω–∞—è)
            elif file_info['is_main'] and not main_file['is_main'] and file_info['lines'] < main_file['lines'] * 0.7:
                should_delete = True
                reason = f"–ë–∞–∑–æ–≤–∞—è –≤–µ—Ä—Å–∏—è –∑–∞–º–µ–Ω–µ–Ω–∞ –ø–æ–ª–Ω–æ–π ({file_info['lines']} vs {main_file['lines']} —Å—Ç—Ä–æ–∫)"
                
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
                unique_functions = file_functions - main_functions
                unique_classes = file_classes - main_classes
                
                if unique_functions or unique_classes:
                    analysis["keep_candidates"].append({
                        "file": file_info,
                        "unique_functions": list(unique_functions),
                        "unique_classes": list(unique_classes),
                        "reason": f"–°–æ–¥–µ—Ä–∂–∏—Ç {len(unique_functions)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –∏ {len(unique_classes)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤"
                    })
                else:
                    should_delete = True
                    reason = f"–ù–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"
            
            if should_delete:
                analysis["deletion_candidates"].append({
                    "file": file_info,
                    "reason": reason,
                    "functionality_score": file_info['functionality_score'],
                    "lines": file_info['lines'],
                    "function_overlap": function_overlap,
                    "class_overlap": class_overlap
                })
            else:
                analysis["keep_candidates"].append({
                    "file": file_info,
                    "reason": "–£–Ω–∏–∫–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å",
                    "functionality_score": file_info['functionality_score'],
                    "lines": file_info['lines']
                })
        
        return analysis
        
    def generate_deletion_report(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ —Ñ–∞–π–ª–∞—Ö –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è"""
        duplicate_groups = self.find_duplicate_groups()
        
        total_deletable = 0
        total_keepable = 0
        total_space_savings = 0
        
        print("\n" + "="*100)
        print("üìã –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –û –§–ê–ô–õ–ê–• –î–õ–Ø –£–î–ê–õ–ï–ù–ò–Ø")
        print("="*100)
        
        for group_name, files in duplicate_groups.items():
            if len(files) <= 1:
                continue
                
            analysis = self.analyze_duplicate_group(group_name, files)
            
            print(f"\nüîç –ì–†–£–ü–ü–ê: {group_name.upper()}")
            print(f"üìä –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(files)}")
            print(f"üìå –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª: {analysis['main_file']['name']} ({analysis['main_file']['lines']} —Å—Ç—Ä–æ–∫, {analysis['main_file']['functionality_score']} –æ—á–∫–æ–≤)")
            
            if analysis['deletion_candidates']:
                print(f"\nüóëÔ∏è –§–ê–ô–õ–´ –î–õ–Ø –£–î–ê–õ–ï–ù–ò–Ø ({len(analysis['deletion_candidates'])}):")
                for candidate in analysis['deletion_candidates']:
                    file_info = candidate['file']
                    print(f"  ‚ùå {file_info['name']}")
                    print(f"     üìä –°—Ç—Ä–æ–∫: {file_info['lines']}, –û—á–∫–æ–≤: {file_info['functionality_score']}")
                    print(f"     üìù –ü—Ä–∏—á–∏–Ω–∞: {candidate['reason']}")
                    print(f"     üìà –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —Ñ—É–Ω–∫—Ü–∏–π: {candidate.get('function_overlap', 0):.1%}")
                    print()
                    total_deletable += 1
                    total_space_savings += file_info['lines']
            
            if analysis['keep_candidates']:
                print(f"\n‚úÖ –§–ê–ô–õ–´ –î–õ–Ø –°–û–•–†–ê–ù–ï–ù–ò–Ø ({len(analysis['keep_candidates'])}):")
                for candidate in analysis['keep_candidates']:
                    file_info = candidate['file']
                    print(f"  ‚úÖ {file_info['name']}")
                    print(f"     üìä –°—Ç—Ä–æ–∫: {file_info['lines']}, –û—á–∫–æ–≤: {file_info['functionality_score']}")
                    print(f"     üìù –ü—Ä–∏—á–∏–Ω–∞: {candidate['reason']}")
                    if 'unique_functions' in candidate and candidate['unique_functions']:
                        print(f"     üîß –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏: {', '.join(candidate['unique_functions'][:5])}{'...' if len(candidate['unique_functions']) > 5 else ''}")
                    print()
                    total_keepable += 1
            
            print("-" * 80)
        
        print(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"üóëÔ∏è –§–∞–π–ª–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è: {total_deletable}")
        print(f"‚úÖ –§–∞–π–ª–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {total_keepable}")
        print(f"üíæ –≠–∫–æ–Ω–æ–º–∏—è –º–µ—Å—Ç–∞: {total_space_savings:,} —Å—Ç—Ä–æ–∫")
        print(f"üì¶ –≠–∫–æ–Ω–æ–º–∏—è –≤ MB: {total_space_savings * 50 / 1024 / 1024:.2f} MB")
        
        return {
            "total_deletable": total_deletable,
            "total_keepable": total_keepable,
            "space_savings": total_space_savings,
            "duplicate_groups": duplicate_groups
        }
        
    def run_analysis(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑"""
        print("üöÄ –ó–∞–ø—É—Å–∫ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ SFM –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
        print("="*100)
        
        if not self.load_sfm_functions():
            return None
            
        return self.generate_deletion_report()

if __name__ == "__main__":
    analyzer = DetailedSFMDuplicateAnalyzer()
    analyzer.run_analysis()