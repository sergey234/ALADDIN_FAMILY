#!/usr/bin/env python3
"""
üß™ –°–ò–°–¢–ï–ú–ê –ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–• –¢–ï–°–¢–û–í –ü–û–°–õ–ï –ö–ê–ñ–î–û–ì–û –≠–¢–ê–ü–ê
8 —Ç–∏–ø–æ–≤ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ A+ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

–ê–≤—Ç–æ—Ä: AI Assistant
–î–∞—Ç–∞: 2025-01-13
–í–µ—Ä—Å–∏—è: 1.0.0
"""

import os
import sys
import json
import subprocess
import ast
import importlib.util
from pathlib import Path
from typing import Dict, List, Any, Tuple
from datetime import datetime


class QualityTestSystem:
    """üß™ –°–∏—Å—Ç–µ–º–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ A+ –∫–∞—á–µ—Å—Ç–≤–∞"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.results = {}
        self.test_config = {
            "test_types": [
                "syntax_validation",
                "import_validation", 
                "security_validation",
                "code_quality",
                "performance_test",
                "integration_test",
                "functionality_test",
                "production_readiness"
            ],
            "stages": [
                "stage_1_critical_fixes",
                "stage_2_security",
                "stage_3_code_quality",
                "stage_4_production_ready"
            ]
        }
    
    def run_syntax_validation(self) -> Dict[str, Any]:
        """üîç –¢–µ—Å—Ç 1: –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ Python"""
        print("üîç –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞: –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞...")
        
        results = {
            "test_name": "syntax_validation",
            "total_files": 0,
            "valid_files": 0,
            "invalid_files": 0,
            "errors": [],
            "score": 0
        }
        
        python_files = list(self.project_root.rglob("*.py"))
        results["total_files"] = len(python_files)
        
        for py_file in python_files:
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                ast.parse(content)
                results["valid_files"] += 1
            except SyntaxError as e:
                results["invalid_files"] += 1
                results["errors"].append({
                    "file": str(py_file),
                    "error": str(e),
                    "line": e.lineno
                })
            except Exception as e:
                results["invalid_files"] += 1
                results["errors"].append({
                    "file": str(py_file),
                    "error": str(e),
                    "line": "unknown"
                })
        
        if results["total_files"] > 0:
            results["score"] = (results["valid_files"] / results["total_files"]) * 100
        
        print(f"‚úÖ –°–∏–Ω—Ç–∞–∫—Å–∏—Å: {results['valid_files']}/{results['total_files']} —Ñ–∞–π–ª–æ–≤ ({results['score']:.1f}%)")
        return results
    
    def run_import_validation(self) -> Dict[str, Any]:
        """üì¶ –¢–µ—Å—Ç 2: –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–º–ø–æ—Ä—Ç–æ–≤"""
        print("üì¶ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞: –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–º–ø–æ—Ä—Ç–æ–≤...")
        
        results = {
            "test_name": "import_validation",
            "total_files": 0,
            "valid_imports": 0,
            "invalid_imports": 0,
            "errors": [],
            "score": 0
        }
        
        python_files = list(self.project_root.rglob("*.py"))
        results["total_files"] = len(python_files)
        
        for py_file in python_files:
            try:
                spec = importlib.util.spec_from_file_location("module", py_file)
                if spec and spec.loader:
                    results["valid_imports"] += 1
                else:
                    results["invalid_imports"] += 1
                    results["errors"].append({
                        "file": str(py_file),
                        "error": "Cannot create module spec"
                    })
            except Exception as e:
                results["invalid_imports"] += 1
                results["errors"].append({
                    "file": str(py_file),
                    "error": str(e)
                })
        
        if results["total_files"] > 0:
            results["score"] = (results["valid_imports"] / results["total_files"]) * 100
        
        print(f"‚úÖ –ò–º–ø–æ—Ä—Ç—ã: {results['valid_imports']}/{results['total_files']} —Ñ–∞–π–ª–æ–≤ ({results['score']:.1f}%)")
        return results
    
    def run_security_validation(self) -> Dict[str, Any]:
        """üîí –¢–µ—Å—Ç 3: –í–∞–ª–∏–¥–∞—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        print("üîí –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞: –í–∞–ª–∏–¥–∞—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏...")
        
        results = {
            "test_name": "security_validation",
            "total_files": 0,
            "secure_files": 0,
            "vulnerable_files": 0,
            "vulnerabilities": [],
            "score": 0
        }
        
        security_patterns = [
            "exec(", "eval(", "os.system(", "subprocess.call(",
            "pickle.loads(", "yaml.load(", "input(", "raw_input("
        ]
        
        python_files = list(self.project_root.rglob("*.py"))
        results["total_files"] = len(python_files)
        
        for py_file in python_files:
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                vulnerabilities_found = []
                for pattern in security_patterns:
                    if pattern in content:
                        vulnerabilities_found.append(pattern)
                
                if vulnerabilities_found:
                    results["vulnerable_files"] += 1
                    results["vulnerabilities"].append({
                        "file": str(py_file),
                        "patterns": vulnerabilities_found
                    })
                else:
                    results["secure_files"] += 1
                    
            except Exception as e:
                results["vulnerable_files"] += 1
                results["vulnerabilities"].append({
                    "file": str(py_file),
                    "error": str(e)
                })
        
        if results["total_files"] > 0:
            results["score"] = (results["secure_files"] / results["total_files"]) * 100
        
        print(f"‚úÖ –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: {results['secure_files']}/{results['total_files']} —Ñ–∞–π–ª–æ–≤ ({results['score']:.1f}%)")
        return results
    
    def run_code_quality(self) -> Dict[str, Any]:
        """üíé –¢–µ—Å—Ç 4: –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞"""
        print("üíé –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞: –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞...")
        
        results = {
            "test_name": "code_quality",
            "total_files": 0,
            "high_quality_files": 0,
            "low_quality_files": 0,
            "issues": [],
            "score": 0
        }
        
        try:
            # –ó–∞–ø—É—Å–∫ flake8 –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞
            result = subprocess.run([
                "python3", "-m", "flake8", 
                str(self.project_root),
                "--count", "--statistics"
            ], capture_output=True, text=True, cwd=self.project_root)
            
            python_files = list(self.project_root.rglob("*.py"))
            results["total_files"] = len(python_files)
            
            if result.returncode == 0:
                results["high_quality_files"] = results["total_files"]
                results["score"] = 100
            else:
                # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç –æ—à–∏–±–æ–∫ –∏–∑ –≤—ã–≤–æ–¥–∞ flake8
                error_lines = [line for line in result.stdout.split('\n') if line.strip() and ':' in line and not line.strip().isdigit()]
                error_count = len(error_lines)
                results["low_quality_files"] = error_count
                results["high_quality_files"] = results["total_files"] - error_count
                results["issues"] = error_lines[:10]  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10 –æ—à–∏–±–æ–∫
                
                if results["total_files"] > 0:
                    # –ë–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π —Ä–∞—Å—á–µ—Ç - —É—á–∏—Ç—ã–≤–∞–µ–º —á—Ç–æ –Ω–µ –≤—Å–µ —Ñ–∞–π–ª—ã –∏–º–µ—é—Ç –æ—à–∏–±–∫–∏
                    files_with_errors = len(set(line.split(':')[0] for line in error_lines))
                    results["score"] = max(0, ((results["total_files"] - files_with_errors) / results["total_files"]) * 100)
                    
        except Exception as e:
            results["issues"].append(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ flake8: {e}")
            results["score"] = 0
        
        print(f"‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞: {results['score']:.1f}%")
        return results
    
    def run_performance_test(self) -> Dict[str, Any]:
        """‚ö° –¢–µ—Å—Ç 5: –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å"""
        print("‚ö° –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞: –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å...")
        
        results = {
            "test_name": "performance_test",
            "total_files": 0,
            "fast_files": 0,
            "slow_files": 0,
            "issues": [],
            "score": 0
        }
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
        performance_issues = [
            "time.sleep(", "while True:", "for i in range(1000000):",
            "import numpy as np", "import pandas as pd"
        ]
        
        python_files = list(self.project_root.rglob("*.py"))
        results["total_files"] = len(python_files)
        
        for py_file in python_files:
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                issues_found = []
                for issue in performance_issues:
                    if issue in content:
                        issues_found.append(issue)
                
                if issues_found:
                    results["slow_files"] += 1
                    results["issues"].append({
                        "file": str(py_file),
                        "issues": issues_found
                    })
                else:
                    results["fast_files"] += 1
                    
            except Exception as e:
                results["slow_files"] += 1
                results["issues"].append({
                    "file": str(py_file),
                    "error": str(e)
                })
        
        if results["total_files"] > 0:
            results["score"] = (results["fast_files"] / results["total_files"]) * 100
        
        print(f"‚úÖ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {results['score']:.1f}%")
        return results
    
    def run_integration_test(self) -> Dict[str, Any]:
        """üîó –¢–µ—Å—Ç 6: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        print("üîó –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ...")
        
        results = {
            "test_name": "integration_test",
            "total_modules": 0,
            "integrated_modules": 0,
            "failed_modules": 0,
            "errors": [],
            "score": 0
        }
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–æ–¥—É–ª–µ–π —Å–∏—Å—Ç–µ–º—ã
        main_modules = [
            "core/system_manager.py",
            "security/base.py",
            "security/managers/analytics_manager.py",
            "security/managers/monitor_manager.py",
            "security/managers/report_manager.py"
        ]
        
        results["total_modules"] = len(main_modules)
        
        for module_path in main_modules:
            full_path = self.project_root / module_path
            if full_path.exists():
                try:
                    spec = importlib.util.spec_from_file_location("module", full_path)
                    if spec and spec.loader:
                        results["integrated_modules"] += 1
                    else:
                        results["failed_modules"] += 1
                        results["errors"].append({
                            "module": module_path,
                            "error": "Cannot create module spec"
                        })
                except Exception as e:
                    results["failed_modules"] += 1
                    results["errors"].append({
                        "module": module_path,
                        "error": str(e)
                    })
            else:
                results["failed_modules"] += 1
                results["errors"].append({
                    "module": module_path,
                    "error": "File not found"
                })
        
        if results["total_modules"] > 0:
            results["score"] = (results["integrated_modules"] / results["total_modules"]) * 100
        
        print(f"‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è: {results['integrated_modules']}/{results['total_modules']} –º–æ–¥—É–ª–µ–π ({results['score']:.1f}%)")
        return results
    
    def run_functionality_test(self) -> Dict[str, Any]:
        """‚öôÔ∏è –¢–µ—Å—Ç 7: –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å"""
        print("‚öôÔ∏è –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞: –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å...")
        
        results = {
            "test_name": "functionality_test",
            "total_functions": 0,
            "working_functions": 0,
            "broken_functions": 0,
            "errors": [],
            "score": 0
        }
        
        # –ü–æ–¥—Å—á–µ—Ç —Ñ—É–Ω–∫—Ü–∏–π –≤ –∫–æ–¥–µ
        python_files = list(self.project_root.rglob("*.py"))
        
        for py_file in python_files:
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                tree = ast.parse(content)
                functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
                results["total_functions"] += len(functions)
                
                # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏
                for func in functions:
                    if func.name and len(func.body) > 0:
                        results["working_functions"] += 1
                    else:
                        results["broken_functions"] += 1
                        results["errors"].append({
                            "file": str(py_file),
                            "function": func.name,
                            "error": "Empty or invalid function"
                        })
                        
            except Exception as e:
                results["errors"].append({
                    "file": str(py_file),
                    "error": str(e)
                })
        
        if results["total_functions"] > 0:
            results["score"] = (results["working_functions"] / results["total_functions"]) * 100
        
        print(f"‚úÖ –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {results['working_functions']}/{results['total_functions']} —Ñ—É–Ω–∫—Ü–∏–π ({results['score']:.1f}%)")
        return results
    
    def run_production_readiness(self) -> Dict[str, Any]:
        """üöÄ –¢–µ—Å—Ç 8: –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –ø—Ä–æ–¥–∞–∫—à–µ–Ω—É"""
        print("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞: –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –ø—Ä–æ–¥–∞–∫—à–µ–Ω—É...")
        
        results = {
            "test_name": "production_readiness",
            "total_checks": 8,
            "passed_checks": 0,
            "failed_checks": 0,
            "issues": [],
            "score": 0
        }
        
        checks = [
            ("config_files", self.project_root / "config"),
            ("logs_directory", self.project_root / "logs"),
            ("tests_directory", self.project_root / "tests"),
            ("documentation", self.project_root / "docs"),
            ("requirements", self.project_root / "requirements.txt"),
            ("readme", self.project_root / "README.md"),
            ("gitignore", self.project_root / ".gitignore"),
            ("main_entry", self.project_root / "main.py")
        ]
        
        for check_name, check_path in checks:
            if check_path.exists():
                results["passed_checks"] += 1
            else:
                results["failed_checks"] += 1
                results["issues"].append({
                    "check": check_name,
                    "path": str(check_path),
                    "status": "missing"
                })
        
        results["score"] = (results["passed_checks"] / results["total_checks"]) * 100
        
        print(f"‚úÖ –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –ø—Ä–æ–¥–∞–∫—à–µ–Ω—É: {results['passed_checks']}/{results['total_checks']} –ø—Ä–æ–≤–µ—Ä–æ–∫ ({results['score']:.1f}%)")
        return results
    
    def run_all_tests(self, stage: str = "all") -> Dict[str, Any]:
        """üß™ –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤"""
        print(f"\nüß™ –ó–ê–ü–£–°–ö –°–ò–°–¢–ï–ú–´ –ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–• –¢–ï–°–¢–û–í - –≠–¢–ê–ü: {stage.upper()}")
        print("=" * 80)
        
        test_results = {
            "stage": stage,
            "timestamp": datetime.now().isoformat(),
            "tests": {},
            "overall_score": 0,
            "summary": {}
        }
        
        # –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤
        test_methods = [
            self.run_syntax_validation,
            self.run_import_validation,
            self.run_security_validation,
            self.run_code_quality,
            self.run_performance_test,
            self.run_integration_test,
            self.run_functionality_test,
            self.run_production_readiness
        ]
        
        total_score = 0
        for test_method in test_methods:
            try:
                result = test_method()
                test_results["tests"][result["test_name"]] = result
                total_score += result["score"]
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ {test_method.__name__}: {e}")
                test_results["tests"][test_method.__name__] = {
                    "error": str(e),
                    "score": 0
                }
        
        # –†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –±–∞–ª–ª–∞
        test_results["overall_score"] = total_score / len(test_methods)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏
        test_results["summary"] = {
            "total_tests": len(test_methods),
            "passed_tests": len([t for t in test_results["tests"].values() if t.get("score", 0) >= 80]),
            "failed_tests": len([t for t in test_results["tests"].values() if t.get("score", 0) < 80]),
            "average_score": test_results["overall_score"]
        }
        
        return test_results
    
    def save_results(self, results: Dict[str, Any], filename: str = None):
        """üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–æ–≤"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"quality_test_results_{results['stage']}_{timestamp}.json"
        
        results_file = self.project_root / "reports" / filename
        results_file.parent.mkdir(exist_ok=True)
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_file}")
        return results_file
    
    def display_summary(self, results: Dict[str, Any]):
        """üìä –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print("\n" + "="*80)
        print("üìä –°–í–û–î–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–• –¢–ï–°–¢–û–í")
        print("="*80)
        
        print(f"üéØ –≠—Ç–∞–ø: {results['stage']}")
        print(f"‚è∞ –í—Ä–µ–º—è: {results['timestamp']}")
        print(f"üìà –û–±—â–∏–π –±–∞–ª–ª: {results['overall_score']:.1f}%")
        
        summary = results['summary']
        print(f"üß™ –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {summary['total_tests']}")
        print(f"‚úÖ –ü—Ä–æ–π–¥–µ–Ω–æ: {summary['passed_tests']}")
        print(f"‚ùå –ù–µ –ø—Ä–æ–π–¥–µ–Ω–æ: {summary['failed_tests']}")
        print(f"üìä –°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª: {summary['average_score']:.1f}%")
        
        print("\nüìã –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print("-" * 50)
        
        for test_name, test_result in results['tests'].items():
            if 'score' in test_result:
                status = "‚úÖ" if test_result['score'] >= 80 else "‚ùå"
                print(f"{status} {test_name}: {test_result['score']:.1f}%")
            else:
                print(f"‚ùå {test_name}: –û—à–∏–±–∫–∞")
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        if results['overall_score'] >= 95:
            quality = "üèÜ –ú–ò–†–û–í–û–ô –ö–õ–ê–°–° (A+)"
        elif results['overall_score'] >= 90:
            quality = "ü•á –û–¢–õ–ò–ß–ù–û (A)"
        elif results['overall_score'] >= 80:
            quality = "ü•à –•–û–†–û–®–û (B)"
        elif results['overall_score'] >= 70:
            quality = "ü•â –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û (C)"
        else:
            quality = "‚ùå –¢–†–ï–ë–£–ï–¢ –£–õ–£–ß–®–ï–ù–ò–Ø (D)"
        
        print(f"\nüéØ –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê: {quality}")
        print("="*80)


def main():
    """üöÄ –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üß™ –°–ò–°–¢–ï–ú–ê –ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–• –¢–ï–°–¢–û–í –ü–û–°–õ–ï –ö–ê–ñ–î–û–ì–û –≠–¢–ê–ü–ê")
    print("=" * 60)
    
    test_system = QualityTestSystem()
    
    if len(sys.argv) > 1:
        stage = sys.argv[1]
    else:
        stage = "all"
    
    # –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤
    results = test_system.run_all_tests(stage)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    test_system.save_results(results)
    
    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏
    test_system.display_summary(results)


if __name__ == "__main__":
    main()