#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Response Time Optimization Tests –¥–ª—è ALADDIN Dashboard
–¢–µ—Å—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–∫–ª–∏–∫–∞ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

–ê–≤—Ç–æ—Ä: ALADDIN Security Team
–í–µ—Ä—Å–∏—è: 1.0.0
–î–∞—Ç–∞: 2025-01-27
–ö–∞—á–µ—Å—Ç–≤–æ: A+
"""

import asyncio
import time
import statistics
import psutil
import pytest
import httpx
import json
import os
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
import threading

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

try:
    from core.base import ComponentStatus, SecurityLevel
    from core.logging_module import LoggingManager
    ALADDIN_AVAILABLE = True
except ImportError:
    ALADDIN_AVAILABLE = False


@dataclass
class ResponseTimeMetric:
    """–ú–µ—Ç—Ä–∏–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–∫–ª–∏–∫–∞"""
    endpoint: str
    method: str
    response_time: float
    status_code: int
    timestamp: datetime
    success: bool
    error_message: Optional[str] = None
    response_size: int = 0
    cpu_usage: float = 0.0
    memory_usage: float = 0.0


@dataclass
class PerformanceBenchmark:
    """–ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    test_name: str
    endpoint: str
    iterations: int
    min_response_time: float
    max_response_time: float
    avg_response_time: float
    median_response_time: float
    p95_response_time: float
    p99_response_time: float
    success_rate: float
    throughput_rps: float  # Requests per second
    cpu_usage_avg: float
    memory_usage_avg: float
    optimization_score: float


@dataclass
class CachePerformanceTest:
    """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫—ç—à–∞"""
    cache_type: str
    hit_rate: float
    miss_rate: float
    response_time_with_cache: float
    response_time_without_cache: float
    cache_efficiency: float
    memory_overhead: float


class ResponseTimeOptimizer:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–∫–ª–∏–∫–∞"""
    
    def __init__(self, base_url: str = "http://localhost:8080"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        
        Args:
            base_url: –ë–∞–∑–æ–≤—ã–π URL –¥–∞—à–±–æ—Ä–¥–∞
        """
        self.base_url = base_url
        self.logger = LoggingManager(name="ResponseTimeOptimizer") if ALADDIN_AVAILABLE else None
        self.response_metrics: List[ResponseTimeMetric] = []
        self.benchmarks: List[PerformanceBenchmark] = []
        
    async def measure_response_time(
        self, 
        endpoint: str, 
        method: str = "GET",
        json_data: Optional[Dict] = None,
        headers: Optional[Dict] = None
    ) -> ResponseTimeMetric:
        """
        –ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–∫–ª–∏–∫–∞ endpoint
        
        Args:
            endpoint: Endpoint –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è
            method: HTTP –º–µ—Ç–æ–¥
            json_data: JSON –¥–∞–Ω–Ω—ã–µ –¥–ª—è POST –∑–∞–ø—Ä–æ—Å–æ–≤
            headers: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            
        Returns:
            –ú–µ—Ç—Ä–∏–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–∫–ª–∏–∫–∞
        """
        start_time = time.time()
        cpu_before = psutil.cpu_percent()
        memory_before = psutil.virtual_memory().percent
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                default_headers = {"Authorization": "Bearer demo_token"}
                if headers:
                    default_headers.update(headers)
                
                if method.upper() == "GET":
                    response = await client.get(f"{self.base_url}{endpoint}", headers=default_headers)
                elif method.upper() == "POST":
                    response = await client.post(
                        f"{self.base_url}{endpoint}", 
                        json=json_data, 
                        headers=default_headers
                    )
                else:
                    raise ValueError(f"Unsupported method: {method}")
                
                response_time = time.time() - start_time
                cpu_after = psutil.cpu_percent()
                memory_after = psutil.virtual_memory().percent
                
                metric = ResponseTimeMetric(
                    endpoint=endpoint,
                    method=method,
                    response_time=response_time,
                    status_code=response.status_code,
                    timestamp=datetime.now(),
                    success=200 <= response.status_code < 300,
                    response_size=len(response.content),
                    cpu_usage=(cpu_after + cpu_before) / 2,
                    memory_usage=(memory_after + memory_before) / 2
                )
                
                self.response_metrics.append(metric)
                return metric
                
        except Exception as e:
            response_time = time.time() - start_time
            
            metric = ResponseTimeMetric(
                endpoint=endpoint,
                method=method,
                response_time=response_time,
                status_code=0,
                timestamp=datetime.now(),
                success=False,
                error_message=str(e)
            )
            
            self.response_metrics.append(metric)
            return metric
    
    async def run_performance_benchmark(
        self, 
        test_name: str,
        endpoint: str, 
        method: str = "GET",
        iterations: int = 100,
        json_data: Optional[Dict] = None
    ) -> PerformanceBenchmark:
        """
        –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        
        Args:
            test_name: –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–∞
            endpoint: Endpoint –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            method: HTTP –º–µ—Ç–æ–¥
            iterations: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π
            json_data: JSON –¥–∞–Ω–Ω—ã–µ
            
        Returns:
            –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        """
        print(f"üìä –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞: {test_name}")
        print(f"   Endpoint: {method} {endpoint}")
        print(f"   –ò—Ç–µ—Ä–∞—Ü–∏–π: {iterations}")
        
        start_time = time.time()
        metrics = []
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∏—Ç–µ—Ä–∞—Ü–∏–∏
        for i in range(iterations):
            if i % 20 == 0:
                print(f"   –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{iterations}")
            
            metric = await self.measure_response_time(endpoint, method, json_data)
            metrics.append(metric)
            
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(0.01)
        
        total_time = time.time() - start_time
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        successful_metrics = [m for m in metrics if m.success]
        response_times = [m.response_time for m in successful_metrics]
        
        if response_times:
            min_time = min(response_times)
            max_time = max(response_times)
            avg_time = statistics.mean(response_times)
            median_time = statistics.median(response_times)
            p95_time = self._calculate_percentile(response_times, 95)
            p99_time = self._calculate_percentile(response_times, 99)
            success_rate = len(successful_metrics) / len(metrics) * 100
            throughput = len(metrics) / total_time
            
            cpu_usage = statistics.mean([m.cpu_usage for m in successful_metrics])
            memory_usage = statistics.mean([m.memory_usage for m in successful_metrics])
            
            # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ü–µ–Ω–∫—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (0-1, –≥–¥–µ 1 = –æ—Ç–ª–∏—á–Ω–æ)
            optimization_score = self._calculate_optimization_score(
                avg_time, success_rate, throughput
            )
            
            benchmark = PerformanceBenchmark(
                test_name=test_name,
                endpoint=endpoint,
                iterations=iterations,
                min_response_time=min_time,
                max_response_time=max_time,
                avg_response_time=avg_time,
                median_response_time=median_time,
                p95_response_time=p95_time,
                p99_response_time=p99_time,
                success_rate=success_rate,
                throughput_rps=throughput,
                cpu_usage_avg=cpu_usage,
                memory_usage_avg=memory_usage,
                optimization_score=optimization_score
            )
            
            self.benchmarks.append(benchmark)
            
            print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
            print(f"     –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.3f}s")
            print(f"     –ú–µ–¥–∏–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è: {median_time:.3f}s")
            print(f"     P95: {p95_time:.3f}s")
            print(f"     –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1f}%")
            print(f"     –ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å: {throughput:.2f} RPS")
            print(f"     –û—Ü–µ–Ω–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {optimization_score:.2f}")
            
            return benchmark
        else:
            raise Exception("–í—Å–µ –∑–∞–ø—Ä–æ—Å—ã –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å –Ω–µ—É–¥–∞—á–Ω–æ")
    
    def _calculate_percentile(self, values: List[float], percentile: int) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—è"""
        sorted_values = sorted(values)
        index = int((percentile / 100) * len(sorted_values))
        return sorted_values[min(index, len(sorted_values) - 1)]
    
    def _calculate_optimization_score(
        self, 
        avg_response_time: float, 
        success_rate: float, 
        throughput: float
    ) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ (0-1)
        time_score = max(0, 1 - (avg_response_time / 5.0))  # –•–æ—Ä–æ—à–æ –µ—Å–ª–∏ < 5s
        success_score = success_rate / 100.0
        throughput_score = min(1.0, throughput / 100.0)  # –•–æ—Ä–æ—à–æ –µ—Å–ª–∏ > 100 RPS
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        optimization_score = (
            time_score * 0.4 +      # 40% - –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞
            success_score * 0.4 +   # 40% - —É—Å–ø–µ—à–Ω–æ—Å—Ç—å
            throughput_score * 0.2  # 20% - –ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
        )
        
        return min(1.0, max(0.0, optimization_score))
    
    async def test_connection_pooling_optimization(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        print("üìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π...")
        
        results = {}
        
        # 1. –¢–µ—Å—Ç –±–µ–∑ –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        print("  1. –ë–µ–∑ –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π...")
        start_time = time.time()
        
        for _ in range(50):
            async with httpx.AsyncClient() as client:
                headers = {"Authorization": "Bearer demo_token"}
                try:
                    await client.get(f"{self.base_url}/api/services", headers=headers)
                except Exception:
                    pass
        
        no_pool_time = time.time() - start_time
        results["no_pool"] = {"time": no_pool_time, "avg_per_request": no_pool_time / 50}
        
        # 2. –¢–µ—Å—Ç —Å –ø—É–ª–æ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        print("  2. –° –ø—É–ª–æ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π...")
        start_time = time.time()
        
        async with httpx.AsyncClient(
            limits=httpx.Limits(max_keepalive_connections=10, max_connections=20)
        ) as client:
            headers = {"Authorization": "Bearer demo_token"}
            for _ in range(50):
                try:
                    await client.get(f"{self.base_url}/api/services", headers=headers)
                except Exception:
                    pass
        
        with_pool_time = time.time() - start_time
        results["with_pool"] = {"time": with_pool_time, "avg_per_request": with_pool_time / 50}
        
        # 3. –í—ã—á–∏—Å–ª—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ
        improvement = ((no_pool_time - with_pool_time) / no_pool_time) * 100
        results["improvement_percent"] = improvement
        
        print(f"   –£–ª—É—á—à–µ–Ω–∏–µ: {improvement:.1f}%")
        
        return results
    
    async def test_compression_optimization(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–∂–∞—Ç–∏—è"""
        print("üìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–∂–∞—Ç–∏—è...")
        
        results = {}
        
        # 1. –ë–µ–∑ —Å–∂–∞—Ç–∏—è
        print("  1. –ë–µ–∑ —Å–∂–∞—Ç–∏—è...")
        headers_no_compression = {
            "Authorization": "Bearer demo_token",
            "Accept-Encoding": "identity"
        }
        
        start_time = time.time()
        for _ in range(20):
            metric = await self.measure_response_time("/api/endpoints", "GET", headers=headers_no_compression)
        
        no_compression_time = time.time() - start_time
        no_compression_size = sum(m.response_size for m in self.response_metrics[-20:])
        
        results["no_compression"] = {
            "time": no_compression_time,
            "total_size": no_compression_size,
            "avg_size": no_compression_size / 20
        }
        
        # 2. –°–æ —Å–∂–∞—Ç–∏–µ–º
        print("  2. –°–æ —Å–∂–∞—Ç–∏–µ–º...")
        headers_with_compression = {
            "Authorization": "Bearer demo_token",
            "Accept-Encoding": "gzip, deflate"
        }
        
        start_time = time.time()
        for _ in range(20):
            metric = await self.measure_response_time("/api/endpoints", "GET", headers=headers_with_compression)
        
        with_compression_time = time.time() - start_time
        with_compression_size = sum(m.response_size for m in self.response_metrics[-20:])
        
        results["with_compression"] = {
            "time": with_compression_time,
            "total_size": with_compression_size,
            "avg_size": with_compression_size / 20
        }
        
        # 3. –í—ã—á–∏—Å–ª—è–µ–º —É–ª—É—á—à–µ–Ω–∏—è
        time_improvement = ((no_compression_time - with_compression_time) / no_compression_time) * 100
        size_improvement = ((no_compression_size - with_compression_size) / no_compression_size) * 100
        
        results["time_improvement_percent"] = time_improvement
        results["size_improvement_percent"] = size_improvement
        
        print(f"   –£–ª—É—á—à–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏: {time_improvement:.1f}%")
        print(f"   –£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞: {size_improvement:.1f}%")
        
        return results
    
    async def test_concurrent_optimization(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏"""
        print("üìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏...")
        
        results = {}
        
        # 1. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        print("  1. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã...")
        start_time = time.time()
        
        for _ in range(30):
            await self.measure_response_time("/api/services", "GET")
        
        sequential_time = time.time() - start_time
        results["sequential"] = {"time": sequential_time, "rps": 30 / sequential_time}
        
        # 2. –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        print("  2. –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã...")
        start_time = time.time()
        
        tasks = [self.measure_response_time("/api/services", "GET") for _ in range(30)]
        await asyncio.gather(*tasks, return_exceptions=True)
        
        concurrent_time = time.time() - start_time
        results["concurrent"] = {"time": concurrent_time, "rps": 30 / concurrent_time}
        
        # 3. –í—ã—á–∏—Å–ª—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ
        improvement = ((sequential_time - concurrent_time) / sequential_time) * 100
        results["improvement_percent"] = improvement
        
        print(f"   –£–ª—É—á—à–µ–Ω–∏–µ: {improvement:.1f}%")
        print(f"   RPS –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ: {results['sequential']['rps']:.2f}")
        print(f"   RPS –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ: {results['concurrent']['rps']:.2f}")
        
        return results
    
    def generate_optimization_report(self) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ–± –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        print("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ–± –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–∫–ª–∏–∫–∞...")
        
        if not self.benchmarks:
            return {"error": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤"}
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –±–µ–Ω—á–º–∞—Ä–∫–∏
        total_benchmarks = len(self.benchmarks)
        avg_optimization_score = statistics.mean([b.optimization_score for b in self.benchmarks])
        avg_response_time = statistics.mean([b.avg_response_time for b in self.benchmarks])
        avg_success_rate = statistics.mean([b.success_rate for b in self.benchmarks])
        avg_throughput = statistics.mean([b.throughput_rps for b in self.benchmarks])
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É
        if avg_optimization_score >= 0.9:
            overall_grade = "A+"
        elif avg_optimization_score >= 0.8:
            overall_grade = "A"
        elif avg_optimization_score >= 0.7:
            overall_grade = "B+"
        elif avg_optimization_score >= 0.6:
            overall_grade = "B"
        else:
            overall_grade = "C"
        
        report = {
            "report_date": datetime.now().isoformat(),
            "total_benchmarks": total_benchmarks,
            "avg_optimization_score": avg_optimization_score,
            "avg_response_time": avg_response_time,
            "avg_success_rate": avg_success_rate,
            "avg_throughput": avg_throughput,
            "overall_grade": overall_grade,
            "benchmarks": [
                {
                    "test_name": b.test_name,
                    "endpoint": b.endpoint,
                    "avg_response_time": b.avg_response_time,
                    "p95_response_time": b.p95_response_time,
                    "success_rate": b.success_rate,
                    "throughput_rps": b.throughput_rps,
                    "optimization_score": b.optimization_score
                }
                for b in self.benchmarks
            ],
            "recommendations": self._generate_optimization_recommendations()
        }
        
        return report
    
    def _generate_optimization_recommendations(self) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        recommendations = []
        
        for benchmark in self.benchmarks:
            if benchmark.avg_response_time > 3.0:
                recommendations.append(
                    f"–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞ –¥–ª—è {benchmark.test_name}: {benchmark.avg_response_time:.3f}s"
                )
            
            if benchmark.success_rate < 95:
                recommendations.append(
                    f"–£–ª—É—á—à–∏—Ç—å –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –¥–ª—è {benchmark.test_name}: {benchmark.success_rate:.1f}%"
                )
            
            if benchmark.throughput_rps < 50:
                recommendations.append(
                    f"–£–≤–µ–ª–∏—á–∏—Ç—å –ø—Ä–æ–ø—É—Å–∫–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –¥–ª—è {benchmark.test_name}: {benchmark.throughput_rps:.2f} RPS"
                )
            
            if benchmark.optimization_score < 0.7:
                recommendations.append(
                    f"–û–±—â–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è {benchmark.test_name}: {benchmark.optimization_score:.2f}"
                )
        
        if not recommendations:
            recommendations.append("–í—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ö–æ—Ä–æ—à–æ")
        
        return recommendations


class TestResponseTimeOptimization:
    """–¢–µ—Å—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–∫–ª–∏–∫–∞"""
    
    @pytest.fixture(autouse=True)
    def setup(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–µ—Å—Ç–æ–≤"""
        self.optimizer = ResponseTimeOptimizer()
    
    @pytest.mark.asyncio
    async def test_main_page_benchmark(self):
        """–ë–µ–Ω—á–º–∞—Ä–∫ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        print("\nüß™ –ë–µ–Ω—á–º–∞—Ä–∫ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
        
        benchmark = await self.optimizer.run_performance_benchmark(
            "main_page",
            "/",
            "GET",
            50
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∏
        assert benchmark.avg_response_time < 2.0, f"–°–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω–∞—è –≥–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {benchmark.avg_response_time:.3f}s"
        assert benchmark.success_rate >= 95, f"–°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {benchmark.success_rate:.1f}%"
        assert benchmark.optimization_score >= 0.7, f"–ù–∏–∑–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {benchmark.optimization_score:.2f}"
        
        print(f"‚úÖ –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {benchmark.avg_response_time:.3f}s")
    
    @pytest.mark.asyncio
    async def test_api_endpoints_benchmark(self):
        """–ë–µ–Ω—á–º–∞—Ä–∫ API endpoints"""
        print("\nüß™ –ë–µ–Ω—á–º–∞—Ä–∫ API endpoints...")
        
        endpoints = [
            ("api_services", "/api/services", "GET"),
            ("api_endpoints", "/api/endpoints", "GET"),
            ("api_test_history", "/api/test-history", "GET")
        ]
        
        for test_name, endpoint, method in endpoints:
            benchmark = await self.optimizer.run_performance_benchmark(
                test_name,
                endpoint,
                method,
                30
            )
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∏
            assert benchmark.avg_response_time < 3.0, f"–°–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–π {endpoint}: {benchmark.avg_response_time:.3f}s"
            assert benchmark.success_rate >= 90, f"–°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –¥–ª—è {endpoint}: {benchmark.success_rate:.1f}%"
            
            print(f"‚úÖ {endpoint} –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω: {benchmark.avg_response_time:.3f}s")
    
    @pytest.mark.asyncio
    async def test_connection_pooling_optimization(self):
        """–¢–µ—Å—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π...")
        
        results = await self.optimizer.test_connection_pooling_optimization()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∏
        assert results["improvement_percent"] >= 0, "–ü—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –¥–æ–ª–∂–µ–Ω —É–ª—É—á—à–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å"
        
        print(f"‚úÖ –ü—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ {results['improvement_percent']:.1f}%")
    
    @pytest.mark.asyncio
    async def test_compression_optimization(self):
        """–¢–µ—Å—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–∂–∞—Ç–∏—è"""
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–∂–∞—Ç–∏—è...")
        
        results = await self.optimizer.test_compression_optimization()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∏
        assert results["size_improvement_percent"] >= 0, "–°–∂–∞—Ç–∏–µ –¥–æ–ª–∂–Ω–æ —É–º–µ–Ω—å—à–∞—Ç—å —Ä–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–æ–≤"
        
        print(f"‚úÖ –°–∂–∞—Ç–∏–µ —É–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –Ω–∞ {results['size_improvement_percent']:.1f}%")
    
    @pytest.mark.asyncio
    async def test_concurrent_optimization(self):
        """–¢–µ—Å—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏"""
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏...")
        
        results = await self.optimizer.test_concurrent_optimization()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∏
        assert results["improvement_percent"] >= 0, "–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å –¥–æ–ª–∂–Ω–∞ —É–ª—É—á—à–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å"
        assert results["concurrent"]["rps"] >= results["sequential"]["rps"], "–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –±—ã—Å—Ç—Ä–µ–µ"
        
        print(f"‚úÖ –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ {results['improvement_percent']:.1f}%")
    
    @pytest.mark.asyncio
    async def test_stress_performance(self):
        """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–¥ —Å—Ç—Ä–µ—Å—Å–æ–º"""
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–¥ —Å—Ç—Ä–µ—Å—Å–æ–º...")
        
        benchmark = await self.optimizer.run_performance_benchmark(
            "stress_test",
            "/api/services",
            "GET",
            200  # –ë–æ–ª—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç–∞
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–¥ —Å—Ç—Ä–µ—Å—Å–æ–º –±–æ–ª–µ–µ –º—è–≥–∫–∏–µ
        assert benchmark.avg_response_time < 5.0, f"–°–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω–æ –ø–æ–¥ —Å—Ç—Ä–µ—Å—Å–æ–º: {benchmark.avg_response_time:.3f}s"
        assert benchmark.success_rate >= 80, f"–°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –ø–æ–¥ —Å—Ç—Ä–µ—Å—Å–æ–º: {benchmark.success_rate:.1f}%"
        assert benchmark.throughput_rps >= 20, f"–°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è –ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å: {benchmark.throughput_rps:.2f} RPS"
        
        print(f"‚úÖ –°–∏—Å—Ç–µ–º–∞ –≤—ã–¥–µ—Ä–∂–∞–ª–∞ —Å—Ç—Ä–µ—Å—Å: {benchmark.avg_response_time:.3f}s, {benchmark.throughput_rps:.2f} RPS")
    
    def test_generate_optimization_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ–± –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        print("\nüìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ–± –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–∫–ª–∏–∫–∞...")
        
        report = self.optimizer.generate_optimization_report()
        
        if "error" in report:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {report['error']}")
            return
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report_file = f"response_time_optimization_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"‚úÖ –û—Ç—á–µ—Ç –æ–± –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
        
        # –í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        print(f"\nüìà –ö–†–ê–¢–ö–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"  –í—Å–µ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤: {report['total_benchmarks']}")
        print(f"  –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {report['avg_optimization_score']:.2f}")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞: {report['avg_response_time']:.3f}s")
        print(f"  –°—Ä–µ–¥–Ω—è—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {report['avg_success_rate']:.1f}%")
        print(f"  –°—Ä–µ–¥–Ω—è—è –ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å: {report['avg_throughput']:.2f} RPS")
        print(f"  –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {report['overall_grade']}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∏ –æ—Ç—á–µ—Ç–∞
        assert report['total_benchmarks'] > 0, "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤"
        assert report['avg_optimization_score'] >= 0.5, f"–°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è –æ–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {report['avg_optimization_score']:.2f}"


if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–∫–ª–∏–∫–∞ ALADDIN Dashboard...")
    print("‚ö° –ë–µ–Ω—á–º–∞—Ä–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
    print("üîß –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π...")
    print("üìä –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–∫–ª–∏–∫–∞...")
    
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ pytest
    pytest.main([__file__, "-v", "--tb=short"])