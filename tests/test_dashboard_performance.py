#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Dashboard Performance Tests –¥–ª—è ALADDIN
–¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–µ–±-–¥–∞—à–±–æ—Ä–¥–∞ —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

–ê–≤—Ç–æ—Ä: ALADDIN Security Team
–í–µ—Ä—Å–∏—è: 1.0.0
–î–∞—Ç–∞: 2025-01-27
–ö–∞—á–µ—Å—Ç–≤–æ: A+
"""

import asyncio
import time
import psutil
import pytest
import httpx
import json
import os
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from statistics import mean, median, stdev

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

try:
    from core.base import ComponentStatus, SecurityLevel
    from core.logging_module import LoggingManager
    ALADDIN_AVAILABLE = True
except ImportError:
    ALADDIN_AVAILABLE = False


@dataclass
class PerformanceMetric:
    """–ú–µ—Ç—Ä–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    name: str
    value: float
    unit: str
    timestamp: datetime
    success: bool = True
    error_message: Optional[str] = None


@dataclass
class ResponseTimeMetric:
    """–ú–µ—Ç—Ä–∏–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–∫–ª–∏–∫–∞"""
    endpoint: str
    method: str
    response_time: float
    status_code: int
    success: bool
    timestamp: datetime
    error_message: Optional[str] = None


class DashboardPerformanceMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–∞—à–±–æ—Ä–¥–∞"""
    
    def __init__(self, base_url: str = "http://localhost:8080"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        
        Args:
            base_url: –ë–∞–∑–æ–≤—ã–π URL –¥–∞—à–±–æ—Ä–¥–∞
        """
        self.base_url = base_url
        self.logger = LoggingManager(name="DashboardPerformanceMonitor") if ALADDIN_AVAILABLE else None
        self.metrics: List[PerformanceMetric] = []
        self.response_times: List[ResponseTimeMetric] = []
        self.start_time: Optional[datetime] = None
        self.end_time: Optional[datetime] = None
        
    async def measure_endpoint_performance(
        self, 
        endpoint: str, 
        method: str = "GET",
        iterations: int = 10,
        json_data: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        –ò–∑–º–µ—Ä–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ endpoint
        
        Args:
            endpoint: Endpoint –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            method: HTTP –º–µ—Ç–æ–¥
            iterations: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π
            json_data: JSON –¥–∞–Ω–Ω—ã–µ –¥–ª—è POST –∑–∞–ø—Ä–æ—Å–æ–≤
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        """
        print(f"üìä –ò–∑–º–µ—Ä–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ {method} {endpoint}...")
        
        response_times = []
        status_codes = []
        errors = []
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            headers = {"Authorization": "Bearer demo_token"}
            
            for i in range(iterations):
                try:
                    start_time = time.time()
                    
                    if method.upper() == "GET":
                        response = await client.get(f"{self.base_url}{endpoint}", headers=headers)
                    elif method.upper() == "POST":
                        response = await client.post(
                            f"{self.base_url}{endpoint}", 
                            json=json_data, 
                            headers=headers
                        )
                    else:
                        raise ValueError(f"Unsupported method: {method}")
                    
                    duration = time.time() - start_time
                    response_times.append(duration)
                    status_codes.append(response.status_code)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫—É
                    metric = ResponseTimeMetric(
                        endpoint=endpoint,
                        method=method,
                        response_time=duration,
                        status_code=response.status_code,
                        success=200 <= response.status_code < 300,
                        timestamp=datetime.now()
                    )
                    self.response_times.append(metric)
                    
                except Exception as e:
                    errors.append(str(e))
                    metric = ResponseTimeMetric(
                        endpoint=endpoint,
                        method=method,
                        response_time=0,
                        status_code=0,
                        success=False,
                        timestamp=datetime.now(),
                        error_message=str(e)
                    )
                    self.response_times.append(metric)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        if response_times:
            stats = {
                "endpoint": endpoint,
                "method": method,
                "iterations": iterations,
                "successful_requests": len(response_times),
                "failed_requests": len(errors),
                "success_rate": len(response_times) / iterations * 100,
                "min_response_time": min(response_times),
                "max_response_time": max(response_times),
                "avg_response_time": mean(response_times),
                "median_response_time": median(response_times),
                "std_deviation": stdev(response_times) if len(response_times) > 1 else 0,
                "status_codes": list(set(status_codes)),
                "errors": errors
            }
        else:
            stats = {
                "endpoint": endpoint,
                "method": method,
                "iterations": iterations,
                "successful_requests": 0,
                "failed_requests": len(errors),
                "success_rate": 0,
                "errors": errors
            }
        
        return stats
    
    async def measure_system_metrics(self) -> Dict[str, Any]:
        """
        –ò–∑–º–µ—Ä–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        
        Returns:
            –°–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        """
        print("üìä –ò–∑–º–µ—Ä–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫...")
        
        # CPU –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
        cpu_percent = psutil.cpu_percent(interval=1)
        cpu_metric = PerformanceMetric(
            name="cpu_usage",
            value=cpu_percent,
            unit="percent",
            timestamp=datetime.now()
        )
        self.metrics.append(cpu_metric)
        
        # –ü–∞–º—è—Ç—å
        memory = psutil.virtual_memory()
        memory_metric = PerformanceMetric(
            name="memory_usage",
            value=memory.percent,
            unit="percent",
            timestamp=datetime.now()
        )
        self.metrics.append(memory_metric)
        
        # –î–∏—Å–∫
        disk = psutil.disk_usage('/')
        disk_metric = PerformanceMetric(
            name="disk_usage",
            value=disk.percent,
            unit="percent",
            timestamp=datetime.now()
        )
        self.metrics.append(disk_metric)
        
        # –ü—Ä–æ—Ü–µ—Å—Å—ã
        process_count = len(psutil.pids())
        process_metric = PerformanceMetric(
            name="process_count",
            value=process_count,
            unit="count",
            timestamp=datetime.now()
        )
        self.metrics.append(process_metric)
        
        return {
            "cpu_usage_percent": cpu_percent,
            "memory_usage_percent": memory.percent,
            "disk_usage_percent": disk.percent,
            "process_count": process_count,
            "timestamp": datetime.now().isoformat()
        }
    
    async def measure_network_metrics(self) -> Dict[str, Any]:
        """
        –ò–∑–º–µ—Ä–µ–Ω–∏–µ —Å–µ—Ç–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
        
        Returns:
            –°–µ—Ç–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        """
        print("üìä –ò–∑–º–µ—Ä–µ–Ω–∏–µ —Å–µ—Ç–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫...")
        
        # –°–µ—Ç–µ–≤—ã–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã
        net_io = psutil.net_io_counters()
        
        network_metric = PerformanceMetric(
            name="network_io",
            value=net_io.bytes_sent + net_io.bytes_recv,
            unit="bytes",
            timestamp=datetime.now()
        )
        self.metrics.append(network_metric)
        
        return {
            "bytes_sent": net_io.bytes_sent,
            "bytes_recv": net_io.bytes_recv,
            "packets_sent": net_io.packets_sent,
            "packets_recv": net_io.packets_recv,
            "timestamp": datetime.now().isoformat()
        }
    
    def generate_performance_report(self) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        
        Returns:
            –û—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        """
        print("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
        
        if not self.response_times:
            return {"error": "No performance data available"}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ endpoint
        endpoint_stats = {}
        for metric in self.response_times:
            key = f"{metric.method} {metric.endpoint}"
            if key not in endpoint_stats:
                endpoint_stats[key] = []
            endpoint_stats[key].append(metric)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ endpoint
        endpoint_performance = {}
        for endpoint, metrics in endpoint_stats.items():
            successful_metrics = [m for m in metrics if m.success]
            
            if successful_metrics:
                response_times = [m.response_time for m in successful_metrics]
                endpoint_performance[endpoint] = {
                    "total_requests": len(metrics),
                    "successful_requests": len(successful_metrics),
                    "success_rate": len(successful_metrics) / len(metrics) * 100,
                    "avg_response_time": mean(response_times),
                    "min_response_time": min(response_times),
                    "max_response_time": max(response_times),
                    "median_response_time": median(response_times),
                    "std_deviation": stdev(response_times) if len(response_times) > 1 else 0
                }
            else:
                endpoint_performance[endpoint] = {
                    "total_requests": len(metrics),
                    "successful_requests": 0,
                    "success_rate": 0,
                    "error": "All requests failed"
                }
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        all_response_times = [m.response_time for m in self.response_times if m.success]
        total_requests = len(self.response_times)
        successful_requests = len(all_response_times)
        
        report = {
            "report_date": datetime.now().isoformat(),
            "test_duration": (self.end_time - self.start_time).total_seconds() if self.start_time and self.end_time else 0,
            "total_requests": total_requests,
            "successful_requests": successful_requests,
            "overall_success_rate": successful_requests / total_requests * 100 if total_requests > 0 else 0,
            "endpoint_performance": endpoint_performance,
            "system_metrics": [m.__dict__ for m in self.metrics],
            "summary": {
                "avg_response_time": mean(all_response_times) if all_response_times else 0,
                "min_response_time": min(all_response_times) if all_response_times else 0,
                "max_response_time": max(all_response_times) if all_response_times else 0,
                "median_response_time": median(all_response_times) if all_response_times else 0
            }
        }
        
        return report


class TestDashboardPerformance:
    """–¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–∞—à–±–æ—Ä–¥–∞"""
    
    @pytest.fixture(autouse=True)
    def setup(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–µ—Å—Ç–æ–≤"""
        self.monitor = DashboardPerformanceMonitor()
        self.monitor.start_time = datetime.now()
        
    def teardown_method(self):
        """–û—á–∏—Å—Ç–∫–∞ –ø–æ—Å–ª–µ —Ç–µ—Å—Ç–æ–≤"""
        if self.monitor:
            self.monitor.end_time = datetime.now()
    
    @pytest.mark.asyncio
    async def test_main_page_performance(self):
        """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
        
        stats = await self.monitor.measure_endpoint_performance("/", "GET", 20)
        
        print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['successful_requests']}/20")
        print(f"üìä –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞: {stats.get('avg_response_time', 0):.3f}s")
        print(f"üìä –ú–µ–¥–∏–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞: {stats.get('median_response_time', 0):.3f}s")
        print(f"üìä –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {stats.get('std_deviation', 0):.3f}s")
        
        assert stats['success_rate'] >= 95, f"–°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {stats['success_rate']:.1f}%"
        assert stats.get('avg_response_time', 0) < 2.0, f"–°–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–π –æ—Ç–∫–ª–∏–∫: {stats.get('avg_response_time', 0):.3f}s"
    
    @pytest.mark.asyncio
    async def test_api_endpoints_performance(self):
        """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ API endpoints"""
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ API endpoints...")
        
        endpoints = [
            ("/api/endpoints", "GET"),
            ("/api/services", "GET"),
            ("/api/test-history", "GET"),
            ("/api/autocomplete?query=test", "GET")
        ]
        
        all_stats = []
        
        for endpoint, method in endpoints:
            stats = await self.monitor.measure_endpoint_performance(endpoint, method, 15)
            all_stats.append(stats)
            
            print(f"  {method} {endpoint}:")
            print(f"    –£—Å–ø–µ—Ö: {stats['success_rate']:.1f}%")
            print(f"    –í—Ä–µ–º—è: {stats.get('avg_response_time', 0):.3f}s")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        avg_response_times = [s.get('avg_response_time', 0) for s in all_stats if s.get('avg_response_time')]
        success_rates = [s['success_rate'] for s in all_stats]
        
        if avg_response_times:
            overall_avg_time = mean(avg_response_times)
            overall_success_rate = mean(success_rates)
            
            print(f"\nüìä –û–±—â–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:")
            print(f"  –°—Ä–µ–¥–Ω–∏–π —É—Å–ø–µ—Ö: {overall_success_rate:.1f}%")
            print(f"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {overall_avg_time:.3f}s")
            
            assert overall_success_rate >= 90, f"–°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π –æ–±—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {overall_success_rate:.1f}%"
            assert overall_avg_time < 3.0, f"–°–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–π –æ–±—â–∏–π –æ—Ç–∫–ª–∏–∫: {overall_avg_time:.3f}s"
    
    @pytest.mark.asyncio
    async def test_api_test_endpoint_performance(self):
        """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ endpoint –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è API"""
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ API test endpoint...")
        
        test_data = {
            "endpoint": "/health",
            "method": "GET"
        }
        
        stats = await self.monitor.measure_endpoint_performance(
            "/api/test", 
            "POST", 
            10, 
            test_data
        )
        
        print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['successful_requests']}/10")
        print(f"üìä –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞: {stats.get('avg_response_time', 0):.3f}s")
        print(f"üìä –ú–µ–¥–∏–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞: {stats.get('median_response_time', 0):.3f}s")
        
        assert stats['success_rate'] >= 80, f"–°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {stats['success_rate']:.1f}%"
        assert stats.get('avg_response_time', 0) < 10.0, f"–°–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–π –æ—Ç–∫–ª–∏–∫: {stats.get('avg_response_time', 0):.3f}s"
    
    @pytest.mark.asyncio
    async def test_system_metrics(self):
        """–¢–µ—Å—Ç —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫...")
        
        metrics = await self.monitor.measure_system_metrics()
        
        print(f"üìä CPU –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: {metrics['cpu_usage_percent']:.1f}%")
        print(f"üìä –ü–∞–º—è—Ç—å: {metrics['memory_usage_percent']:.1f}%")
        print(f"üìä –î–∏—Å–∫: {metrics['disk_usage_percent']:.1f}%")
        print(f"üìä –ü—Ä–æ—Ü–µ—Å—Å—ã: {metrics['process_count']}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∏ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        assert metrics['cpu_usage_percent'] < 90, f"–°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU: {metrics['cpu_usage_percent']:.1f}%"
        assert metrics['memory_usage_percent'] < 90, f"–°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {metrics['memory_usage_percent']:.1f}%"
        assert metrics['disk_usage_percent'] < 95, f"–°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–∞: {metrics['disk_usage_percent']:.1f}%"
    
    @pytest.mark.asyncio
    async def test_network_metrics(self):
        """–¢–µ—Å—Ç —Å–µ—Ç–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–µ—Ç–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫...")
        
        metrics = await self.monitor.measure_network_metrics()
        
        print(f"üìä –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –±–∞–π—Ç: {metrics['bytes_sent']:,}")
        print(f"üìä –ü–æ–ª—É—á–µ–Ω–æ –±–∞–π—Ç: {metrics['bytes_recv']:,}")
        print(f"üìä –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –ø–∞–∫–µ—Ç–æ–≤: {metrics['packets_sent']:,}")
        print(f"üìä –ü–æ–ª—É—á–µ–Ω–æ –ø–∞–∫–µ—Ç–æ–≤: {metrics['packets_recv']:,}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∏ —Å–µ—Ç–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
        assert metrics['bytes_sent'] >= 0, "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –±–∞–π—Ç"
        assert metrics['bytes_recv'] >= 0, "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –±–∞–π—Ç"
        assert metrics['packets_sent'] >= 0, "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤"
        assert metrics['packets_recv'] >= 0, "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤"
    
    @pytest.mark.asyncio
    async def test_performance_under_load(self):
        """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–¥ –Ω–∞–≥—Ä—É–∑–∫–æ–π"""
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–¥ –Ω–∞–≥—Ä—É–∑–∫–æ–π...")
        
        # –ò–∑–º–µ—Ä—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–æ –Ω–∞–≥—Ä—É–∑–∫–∏
        initial_metrics = await self.monitor.measure_system_metrics()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –Ω–∞–≥—Ä—É–∑–æ—á–Ω—ã–π —Ç–µ—Å—Ç
        load_tasks = []
        for i in range(20):
            task = self.monitor.measure_endpoint_performance("/api/services", "GET", 5)
            load_tasks.append(task)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*load_tasks, return_exceptions=True)
        
        # –ò–∑–º–µ—Ä—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ –Ω–∞–≥—Ä—É–∑–∫–∏
        final_metrics = await self.monitor.measure_system_metrics()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        successful_results = [r for r in results if isinstance(r, dict)]
        failed_results = [r for r in results if not isinstance(r, dict)]
        
        print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {len(successful_results)}/20")
        print(f"‚ùå –ù–µ—É–¥–∞—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {len(failed_results)}/20")
        
        if successful_results:
            all_response_times = []
            for result in successful_results:
                if 'avg_response_time' in result:
                    all_response_times.append(result['avg_response_time'])
            
            if all_response_times:
                avg_response_time = mean(all_response_times)
                print(f"üìä –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞ –ø–æ–¥ –Ω–∞–≥—Ä—É–∑–∫–æ–π: {avg_response_time:.3f}s")
                
                assert avg_response_time < 5.0, f"–°–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–π –æ—Ç–∫–ª–∏–∫ –ø–æ–¥ –Ω–∞–≥—Ä—É–∑–∫–æ–π: {avg_response_time:.3f}s"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        cpu_increase = final_metrics['cpu_usage_percent'] - initial_metrics['cpu_usage_percent']
        memory_increase = final_metrics['memory_usage_percent'] - initial_metrics['memory_usage_percent']
        
        print(f"üìä –£–≤–µ–ª–∏—á–µ–Ω–∏–µ CPU: {cpu_increase:.1f}%")
        print(f"üìä –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏: {memory_increase:.1f}%")
        
        assert cpu_increase < 30, f"–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ CPU: {cpu_increase:.1f}%"
        assert memory_increase < 20, f"–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏: {memory_increase:.1f}%"
    
    def test_generate_performance_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        print("\nüìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
        
        report = self.monitor.generate_performance_report()
        
        if "error" in report:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {report['error']}")
            return
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report_file = f"dashboard_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
        
        # –í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        print(f"\nüìà –ö–†–ê–¢–ö–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"  –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {report['total_requests']}")
        print(f"  –£—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {report['successful_requests']}")
        print(f"  –û–±—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {report['overall_success_rate']:.1f}%")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞: {report['summary']['avg_response_time']:.3f}s")
        print(f"  –ú–µ–¥–∏–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞: {report['summary']['median_response_time']:.3f}s")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∏ –æ—Ç—á–µ—Ç–∞
        assert report['total_requests'] > 0, "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –∑–∞–ø—Ä–æ—Å–∞—Ö"
        assert report['overall_success_rate'] >= 80, f"–°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {report['overall_success_rate']:.1f}%"
        assert report['summary']['avg_response_time'] < 5.0, f"–°–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–π –æ—Ç–∫–ª–∏–∫: {report['summary']['avg_response_time']:.3f}s"


if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ALADDIN Dashboard...")
    print("üìä –ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–∫–ª–∏–∫–∞ –∏ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫...")
    print("üõ°Ô∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
    
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ pytest
    pytest.main([__file__, "-v", "--tb=short"])