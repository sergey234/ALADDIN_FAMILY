#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Data Collector - –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ ML –º–æ–¥–µ–ª–µ–π
"""

import asyncio
import json
import logging
import os
import random
import time
from datetime import datetime
from typing import Any, Dict, List

import requests
from bs4 import BeautifulSoup

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EnhancedDataCollector:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    """

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Å–±–æ—Ä—â–∏–∫–∞"""
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/91.0.4472.124 Safari/537.36"
            }
        )

        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        self.data_sources = {
            "cbr": {
                "base_url": "https://www.cbr.ru",
                "endpoints": [
                    "/na/",
                    "/analytics/",
                    "/press/",
                    "/vfs/",
                    "/statistics/",
                    "/analytics/statistics/",
                    "/vfs/statistics/",
                ],
            },
            "news_sources": {
                "rbc": {
                    "base_url": "https://www.rbc.ru",
                    "search_url": "https://www.rbc.ru/search/?query={query}",
                    "keywords": [
                        "–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ",
                        "—Ñ–∏—à–∏–Ω–≥",
                        "–∫–∏–±–µ—Ä–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ",
                        "–±–∞–Ω–∫–æ–≤—Å–∫–æ–µ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ",
                        "—Ç–µ–ª–µ—Ñ–æ–Ω–Ω–æ–µ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ",
                        "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ",
                        "–æ–±–º–∞–Ω",
                        "—Ö–∏—â–µ–Ω–∏–µ",
                    ],
                },
                "ria": {
                    "base_url": "https://ria.ru",
                    "search_url": "https://ria.ru/search/?query={query}",
                    "keywords": [
                        "–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ",
                        "—Ñ–∏—à–∏–Ω–≥",
                        "–∫–∏–±–µ—Ä–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ",
                        "–±–∞–Ω–∫–æ–≤—Å–∫–æ–µ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ",
                    ],
                },
                "interfax": {
                    "base_url": "https://www.interfax.ru",
                    "search_url": "https://www.interfax.ru/search/?query={query}",
                    "keywords": [
                        "–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ",
                        "—Ñ–∏—à–∏–Ω–≥",
                        "–∫–∏–±–µ—Ä–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ",
                    ],
                },
                "tass": {
                    "base_url": "https://tass.ru",
                    "search_url": "https://tass.ru/search?query={query}",
                    "keywords": [
                        "–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ",
                        "—Ñ–∏—à–∏–Ω–≥",
                        "–∫–∏–±–µ—Ä–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ",
                    ],
                },
            },
            "government_sources": {
                "mvd": {
                    "base_url": "https://–º–≤–¥.—Ä—Ñ",
                    "endpoints": ["/news", "/press_releases"],
                },
                "fsb": {
                    "base_url": "https://www.fsb.ru",
                    "endpoints": ["/fsb/press_center/news"],
                },
                "roskomnadzor": {
                    "base_url": "https://rkn.gov.ru",
                    "endpoints": ["/news"],
                },
            },
        }

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        os.makedirs("data/enhanced_collection", exist_ok=True)

        logger.info("üöÄ –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    async def collect_comprehensive_data(self) -> Dict[str, Any]:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

        Returns:
            Dict[str, Any]: –°–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        logger.info("üìä –ù–∞—á–∞–ª–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö...")

        all_data = {
            "metadata": {
                "collection_timestamp": datetime.now().isoformat(),
                "total_sources": 0,
                "total_records": 0,
                "collection_duration": 0,
            },
            "cbr_data": [],
            "news_data": [],
            "government_data": [],
            "fraud_patterns": {
                "by_type": {},
                "by_region": {},
                "by_severity": {},
            },
        }

        start_time = time.time()

        try:
            # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¶–ë –†–§
            logger.info("üè¶ –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¶–ë –†–§...")
            cbr_data = await self._collect_cbr_data()
            all_data["cbr_data"] = cbr_data
            all_data["metadata"]["total_sources"] += 1

            # –°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            logger.info("üì∞ –°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            news_data = await self._collect_news_data()
            all_data["news_data"] = news_data
            all_data["metadata"]["total_sources"] += len(
                self.data_sources["news_sources"]
            )

            # –°–±–æ—Ä –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            logger.info("üèõÔ∏è –°–±–æ—Ä –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            gov_data = await self._collect_government_data()
            all_data["government_data"] = gov_data
            all_data["metadata"]["total_sources"] += len(
                self.data_sources["government_sources"]
            )

            # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            logger.info("üîç –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞...")
            all_data["fraud_patterns"] = self._analyze_fraud_patterns(all_data)

            # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π
            all_data["metadata"]["total_records"] = (
                len(all_data["cbr_data"])
                + len(all_data["news_data"])
                + len(all_data["government_data"])
            )

            # –í—Ä–µ–º—è —Å–±–æ—Ä–∞
            all_data["metadata"]["collection_duration"] = (
                time.time() - start_time
            )

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            data_file = (
                f"data/enhanced_collection/comprehensive_data_{timestamp}.json"
            )

            with open(data_file, "w", encoding="utf-8") as f:
                json.dump(all_data, f, ensure_ascii=False, indent=2)

            logger.info(f"‚úÖ –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω: {data_file}")
            logger.info(
                f"üìä –°–æ–±—Ä–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {all_data['metadata']['total_records']}"
            )

            return all_data

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return all_data

    async def _collect_cbr_data(self) -> List[Dict[str, Any]]:
        """–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ—Ç –¶–ë –†–§"""
        cbr_data = []

        try:
            cbr_config = self.data_sources["cbr"]

            for endpoint in cbr_config["endpoints"]:
                try:
                    url = cbr_config["base_url"] + endpoint
                    response = self.session.get(url, timeout=10)

                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, "html.parser")

                        # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ—Ç—á–µ—Ç—ã –∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                        links = soup.find_all("a", href=True)

                        for link in links[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                            href = link.get("href")
                            text = link.get_text(strip=True)

                            if any(
                                keyword in text.lower()
                                for keyword in [
                                    "–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ",
                                    "—Ñ–∏—à–∏–Ω–≥",
                                    "–∫–∏–±–µ—Ä",
                                    "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
                                ]
                            ):

                                report_data = {
                                    "id": f"cbr_{len(cbr_data)+1}",
                                    "title": text,
                                    "url": (
                                        href
                                        if href.startswith("http")
                                        else cbr_config["base_url"] + href
                                    ),
                                    "source": "–¶–ë –†–§",
                                    "date": datetime.now().strftime(
                                        "%Y-%m-%d"
                                    ),
                                    "fraud_type": self._classify_fraud_type(
                                        text
                                    ),
                                    "severity": self._determine_severity(text),
                                    "region": "–†–æ—Å—Å–∏–π—Å–∫–∞—è –§–µ–¥–µ—Ä–∞—Ü–∏—è",
                                    "amount_lost": self._estimate_amount(text),
                                    "description": text,
                                }

                                cbr_data.append(report_data)

                    # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    await asyncio.sleep(1)

                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å {endpoint}: {e}")
                    continue

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¶–ë –†–§: {e}")

        return cbr_data

    async def _collect_news_data(self) -> List[Dict[str, Any]]:
        """–°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        news_data = []

        try:
            for source_name, source_config in self.data_sources[
                "news_sources"
            ].items():
                try:
                    logger.info(f"üì∞ –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ {source_name}...")

                    for keyword in source_config["keywords"]:
                        try:
                            search_url = source_config["search_url"].format(
                                query=keyword
                            )
                            response = self.session.get(search_url, timeout=10)

                            if response.status_code == 200:
                                soup = BeautifulSoup(
                                    response.content, "html.parser"
                                )

                                # –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞)
                                articles = soup.find_all(
                                    ["article", "div"],
                                    class_=["news-item", "article", "item"],
                                )

                                for article in articles[
                                    :3
                                ]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                                    title_elem = article.find(
                                        ["h1", "h2", "h3", "a"]
                                    )
                                    if title_elem:
                                        title = title_elem.get_text(strip=True)

                                        if any(
                                            keyword in title.lower()
                                            for keyword in [
                                                "–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ",
                                                "—Ñ–∏—à–∏–Ω–≥",
                                                "–∫–∏–±–µ—Ä",
                                                "–æ–±–º–∞–Ω",
                                            ]
                                        ):

                                            article_data = {
                                                "id": f"news_{len(news_data)+1}",
                                                "title": title,
                                                "source": source_name.upper(),
                                                "date": datetime.now().strftime(
                                                    "%Y-%m-%d"
                                                ),
                                                "fraud_type": self._classify_fraud_type(
                                                    title
                                                ),
                                                "severity": self._determine_severity(
                                                    title
                                                ),
                                                "region": self._extract_region(
                                                    title
                                                ),
                                                "amount_lost": self._estimate_amount(
                                                    title
                                                ),
                                                "keywords": [keyword],
                                                "description": title,
                                            }

                                            news_data.append(article_data)

                            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                            await asyncio.sleep(2)

                        except Exception as e:
                            logger.warning(
                                f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É {keyword}: {e}"
                            )
                            continue

                except Exception as e:
                    logger.warning(
                        f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {source_name}: {e}"
                    )
                    continue

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")

        return news_data

    async def _collect_government_data(self) -> List[Dict[str, Any]]:
        """–°–±–æ—Ä –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        gov_data = []

        try:
            for source_name, source_config in self.data_sources[
                "government_sources"
            ].items():
                try:
                    logger.info(f"üèõÔ∏è –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ {source_name}...")

                    for endpoint in source_config["endpoints"]:
                        try:
                            url = source_config["base_url"] + endpoint
                            response = self.session.get(url, timeout=10)

                            if response.status_code == 200:
                                soup = BeautifulSoup(
                                    response.content, "html.parser"
                                )

                                # –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ –ø—Ä–µ—Å—Å-—Ä–µ–ª–∏–∑–æ–≤
                                items = soup.find_all(
                                    ["div", "article"],
                                    class_=["news", "item", "press"],
                                )

                                for item in items[
                                    :3
                                ]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                                    title_elem = item.find(
                                        ["h1", "h2", "h3", "a"]
                                    )
                                    if title_elem:
                                        title = title_elem.get_text(strip=True)

                                        if any(
                                            keyword in title.lower()
                                            for keyword in [
                                                "–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ",
                                                "—Ñ–∏—à–∏–Ω–≥",
                                                "–∫–∏–±–µ—Ä",
                                                "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
                                            ]
                                        ):

                                            gov_item = {
                                                "id": f"gov_{len(gov_data)+1}",
                                                "title": title,
                                                "source": source_name.upper(),
                                                "date": datetime.now().strftime(
                                                    "%Y-%m-%d"
                                                ),
                                                "fraud_type": self._classify_fraud_type(
                                                    title
                                                ),
                                                "severity": self._determine_severity(
                                                    title
                                                ),
                                                "region": "–†–æ—Å—Å–∏–π—Å–∫–∞—è –§–µ–¥–µ—Ä–∞—Ü–∏—è",
                                                "amount_lost": self._estimate_amount(
                                                    title
                                                ),
                                                "description": title,
                                            }

                                            gov_data.append(gov_item)

                            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                            await asyncio.sleep(1)

                        except Exception as e:
                            logger.warning(
                                f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å {endpoint}: {e}"
                            )
                            continue

                except Exception as e:
                    logger.warning(
                        f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {source_name}: {e}"
                    )
                    continue

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")

        return gov_data

    def _classify_fraud_type(self, text: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ –ø–æ —Ç–µ–∫—Å—Ç—É"""
        text_lower = text.lower()

        if any(word in text_lower for word in ["–±–∞–Ω–∫", "–±–∞–Ω–∫–æ–≤—Å–∫", "–∫–∞—Ä—Ç"]):
            return "–±–∞–Ω–∫–æ–≤—Å–∫–æ–µ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ"
        elif any(word in text_lower for word in ["–∫–∏–±–µ—Ä", "—Ö–∞–∫–µ—Ä", "–≤–∑–ª–æ–º"]):
            return "–∫–∏–±–µ—Ä–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ"
        elif any(word in text_lower for word in ["—Ç–µ–ª–µ—Ñ–æ–Ω", "–∑–≤–æ–Ω–æ–∫"]):
            return "—Ç–µ–ª–µ—Ñ–æ–Ω–Ω–æ–µ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ"
        elif any(
            word in text_lower for word in ["–∏–Ω—Ç–µ—Ä–Ω–µ—Ç", "–æ–Ω–ª–∞–π–Ω", "—Å–µ—Ç—å"]
        ):
            return "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ"
        elif any(word in text_lower for word in ["—Ñ–∏—à–∏–Ω–≥", "–ø–æ–¥–¥–µ–ª–∫–∞"]):
            return "—Ñ–∏—à–∏–Ω–≥"
        elif any(word in text_lower for word in ["–ø–∏—Ä–∞–º–∏–¥–∞", "—Å—Ö–µ–º–∞"]):
            return "—Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –ø–∏—Ä–∞–º–∏–¥–∞"
        else:
            return "–æ–±—â–µ–µ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ"

    def _determine_severity(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏ –ø–æ —Ç–µ–∫—Å—Ç—É"""
        text_lower = text.lower()

        if any(
            word in text_lower for word in ["–∫—Ä–∏—Ç–∏—á–µ—Å–∫", "–º–∞—Å—Å–æ–≤", "—ç–ø–∏–¥–µ–º–∏—è"]
        ):
            return "–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è"
        elif any(word in text_lower for word in ["–≤—ã—Å–æ–∫", "—Å–µ—Ä—å–µ–∑–Ω", "–∫—Ä—É–ø–Ω"]):
            return "–≤—ã—Å–æ–∫–∞—è"
        elif any(word in text_lower for word in ["—Å—Ä–µ–¥–Ω", "—É–º–µ—Ä–µ–Ω–Ω"]):
            return "—Å—Ä–µ–¥–Ω—è—è"
        else:
            return "–Ω–∏–∑–∫–∞—è"

    def _extract_region(self, text: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        text_lower = text.lower()

        regions = {
            "–º–æ—Å–∫–≤–∞": "–ú–æ—Å–∫–≤–∞",
            "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
            "–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥",
            "–∫–∞–∑–∞–Ω—å": "–ö–∞–∑–∞–Ω—å",
            "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫",
            "—Ä–æ—Å—Å–∏—è": "–†–æ—Å—Å–∏–π—Å–∫–∞—è –§–µ–¥–µ—Ä–∞—Ü–∏—è",
        }

        for region_key, region_name in regions.items():
            if region_key in text_lower:
                return region_name

        return "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω"

    def _estimate_amount(self, text: str) -> int:
        """–û—Ü–µ–Ω–∫–∞ —Å—É–º–º—ã –ø–æ—Ç–µ—Ä—å –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        text_lower = text.lower()

        if any(word in text_lower for word in ["–º–∏–ª–ª–∏–æ–Ω", "–º–ª–Ω"]):
            return random.randint(1000000, 5000000)
        elif any(word in text_lower for word in ["—Ç—ã—Å—è—á", "—Ç—ã—Å"]):
            return random.randint(50000, 500000)
        elif any(word in text_lower for word in ["—Å–æ—Ç–µ–Ω"]):
            return random.randint(100000, 1000000)
        else:
            return random.randint(10000, 100000)

    def _analyze_fraud_patterns(
        self, data: Dict[str, Any]
    ) -> Dict[str, Dict[str, int]]:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞"""
        patterns = {"by_type": {}, "by_region": {}, "by_severity": {}}

        # –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        all_items = (
            data["cbr_data"] + data["news_data"] + data["government_data"]
        )

        for item in all_items:
            # –ü–æ —Ç–∏–ø–∞–º
            fraud_type = item.get("fraud_type", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
            patterns["by_type"][fraud_type] = (
                patterns["by_type"].get(fraud_type, 0) + 1
            )

            # –ü–æ —Ä–µ–≥–∏–æ–Ω–∞–º
            region = item.get("region", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
            patterns["by_region"][region] = (
                patterns["by_region"].get(region, 0) + 1
            )

            # –ü–æ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
            severity = item.get("severity", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
            patterns["by_severity"][severity] = (
                patterns["by_severity"].get(severity, 0) + 1
            )

        return patterns

    def generate_enhanced_report(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "data_summary": {
                "total_records": data["metadata"]["total_records"],
                "total_sources": data["metadata"]["total_sources"],
                "collection_duration": data["metadata"]["collection_duration"],
                "cbr_records": len(data["cbr_data"]),
                "news_records": len(data["news_data"]),
                "government_records": len(data["government_data"]),
            },
            "fraud_patterns": data["fraud_patterns"],
            "recommendations": [
                "–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ API –±–∞–Ω–∫–æ–≤",
                "–î–æ–±–∞–≤–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ú–í–î –∏ –§–°–ë",
                "–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏",
                "–°–æ–∑–¥–∞—Ç—å —Å–∏—Å—Ç–µ–º—É —Ä–∞–Ω–Ω–µ–≥–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è",
                "–î–æ–±–∞–≤–∏—Ç—å –≥–µ–æ–ª–æ–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑",
            ],
        }

        return report


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print("üìä –†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–ë–û–† –î–ê–ù–ù–´–• –î–õ–Ø ML –ú–û–î–ï–õ–ï–ô")
    print("=" * 50)

    collector = EnhancedDataCollector()

    try:
        # –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
        data = await collector.collect_comprehensive_data()

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        report = collector.generate_enhanced_report(data)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = (
            f"data/enhanced_collection/enhanced_report_{timestamp}.json"
        )

        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"üìä –°–æ–±—Ä–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {data['metadata']['total_records']}")
        print(f"üìã –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    asyncio.run(main())
