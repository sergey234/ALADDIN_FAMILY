#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Russian Fraud ML Models
–ú–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞
"""

import json
import logging
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, Any, Tuple
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import joblib
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RussianFraudMLModels:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞
    """

    def __init__(self, data_path: str = "data/demo_russian_fraud_data.json"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∞

        Args:
            data_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–µ
        """
        self.data_path = data_path
        self.models = {}
        self.encoders = {}
        self.scalers = {}
        self.feature_columns = []
        self.model_metrics = {}

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        self.fraud_data = self._load_data()

    def _load_data(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–µ"""
        try:
            with open(self.data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ: {data['metadata']['total_records']} –∑–∞–ø–∏—Å–µ–π")
            return data
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return {}

    def _prepare_training_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

        Returns:
            Tuple[np.ndarray, np.ndarray]: X (–ø—Ä–∏–∑–Ω–∞–∫–∏), y (—Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ)
        """
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –∏–∑ –¥–∞–Ω–Ω—ã—Ö
        records = []

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤ –¶–ë –†–§
        for report in self.fraud_data.get('cbr_reports', []):
            records.append({
                'fraud_type': report['fraud_type'],
                'severity': report['severity'],
                'region': report['region'],
                'amount_lost': report['amount_lost'],
                'source': '–¶–ë –†–§'
            })

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
        for article in self.fraud_data.get('news_articles', []):
            records.append({
                'fraud_type': article['fraud_type'],
                'severity': article['severity'],
                'region': article['region'],
                'amount_lost': self._estimate_amount_from_description(article['description']),
                'source': article['source']
            })

        df = pd.DataFrame(records)

        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df['severity_numeric'] = df['severity'].map({
            '–Ω–∏–∑–∫–∞—è': 1,
            '—Å—Ä–µ–¥–Ω—è—è': 2,
            '–≤—ã—Å–æ–∫–∞—è': 3,
            '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è': 4
        })

        df['region_encoded'] = LabelEncoder().fit_transform(df['region'])
        df['source_encoded'] = LabelEncoder().fit_transform(df['source'])
        df['fraud_type_encoded'] = LabelEncoder().fit_transform(df['fraud_type'])

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—É–º–º—ã –ø–æ—Ç–µ—Ä—å
        df['amount_normalized'] = (
            (df['amount_lost'] - df['amount_lost'].min()) /
            (df['amount_lost'].max() - df['amount_lost'].min())
        )

        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏
        feature_columns = [
            'severity_numeric', 'region_encoded', 'source_encoded', 'amount_normalized'
        ]
        X = df[feature_columns].values
        y = df['fraud_type_encoded'].values

        self.feature_columns = feature_columns

        return X, y

    def _estimate_amount_from_description(self, description: str) -> int:
        """
        –û—Ü–µ–Ω–∫–∞ —Å—É–º–º—ã –ø–æ—Ç–µ—Ä—å –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è

        Args:
            description: –û–ø–∏—Å–∞–Ω–∏–µ —Å–ª—É—á–∞—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞

        Returns:
            int: –û—Ü–µ–Ω–æ—á–Ω–∞—è —Å—É–º–º–∞ –ø–æ—Ç–µ—Ä—å
        """
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—É–º–º—ã
        description_lower = description.lower()

        if '–º–∏–ª–ª–∏–æ–Ω' in description_lower or '–º–ª–Ω' in description_lower:
            return 1000000
        elif '—Ç—ã—Å—è—á' in description_lower or '—Ç—ã—Å' in description_lower:
            return 100000
        elif '—Å–æ—Ç–µ–Ω' in description_lower:
            return 500000
        else:
            return 50000  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    def create_fraud_classifier(self) -> Dict[str, Any]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ç–∏–ø–æ–≤ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞

        Returns:
            Dict[str, Any]: –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏
        """
        try:
            logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ç–∏–ø–æ–≤ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞...")

            X, y = self._prepare_training_data()

            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            self.scalers['fraud_classifier'] = scaler

            # –û–±—É—á–µ–Ω–∏–µ Random Forest
            rf_model = RandomForestClassifier(
                n_estimators=100,
                random_state=42,
                max_depth=10,
                min_samples_split=5
            )
            rf_model.fit(X_train_scaled, y_train)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred = rf_model.predict(X_test_scaled)

            # –ú–µ—Ç—Ä–∏–∫–∏
            accuracy = accuracy_score(y_test, y_pred)
            report = classification_report(y_test, y_pred, output_dict=True)

            metrics = {
                'model_name': 'Fraud Type Classifier',
                'algorithm': 'Random Forest',
                'accuracy': accuracy,
                'classification_report': report,
                'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),
                'feature_importance': rf_model.feature_importances_.tolist(),
                'training_samples': len(X_train),
                'test_samples': len(X_test)
            }

            self.models['fraud_classifier'] = rf_model
            self.model_metrics['fraud_classifier'] = metrics

            logger.info(f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω. –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")

            return metrics

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞: {e}")
            return {}

    def create_severity_predictor(self) -> Dict[str, Any]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞

        Returns:
            Dict[str, Any]: –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏
        """
        try:
            logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞...")

            X, y = self._prepare_training_data()

            # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
            records = []
            for report in self.fraud_data.get('cbr_reports', []):
                records.append({
                    'fraud_type_encoded': LabelEncoder().fit_transform([report['fraud_type']])[0],
                    'region_encoded': LabelEncoder().fit_transform([report['region']])[0],
                    'amount_normalized': report['amount_lost'] / 1000000,  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                    'severity': report['severity']
                })

            for article in self.fraud_data.get('news_articles', []):
                records.append({
                    'fraud_type_encoded': LabelEncoder().fit_transform([article['fraud_type']])[0],
                    'region_encoded': LabelEncoder().fit_transform([article['region']])[0],
                    'amount_normalized': self._estimate_amount_from_description(article['description']) / 1000000,
                    'severity': article['severity']
                })

            df = pd.DataFrame(records)
            X_severity = df[['fraud_type_encoded', 'region_encoded', 'amount_normalized']].values
            y_severity = df['severity'].map({
                '–Ω–∏–∑–∫–∞—è': 1,
                '—Å—Ä–µ–¥–Ω—è—è': 2,
                '–≤—ã—Å–æ–∫–∞—è': 3,
                '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è': 4
            }).values

            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            X_train, X_test, y_train, y_test = train_test_split(
                X_severity, y_severity, test_size=0.2, random_state=42, stratify=y_severity
            )

            # –û–±—É—á–µ–Ω–∏–µ Gradient Boosting
            gb_model = GradientBoostingClassifier(
                n_estimators=100,
                random_state=42,
                max_depth=6,
                learning_rate=0.1
            )
            gb_model.fit(X_train, y_train)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred = gb_model.predict(X_test)

            # –ú–µ—Ç—Ä–∏–∫–∏
            accuracy = accuracy_score(y_test, y_pred)

            metrics = {
                'model_name': 'Severity Predictor',
                'algorithm': 'Gradient Boosting',
                'accuracy': accuracy,
                'feature_importance': gb_model.feature_importances_.tolist(),
                'training_samples': len(X_train),
                'test_samples': len(X_test)
            }

            self.models['severity_predictor'] = gb_model
            self.model_metrics['severity_predictor'] = metrics

            logger.info(f"–ü—Ä–µ–¥–∏–∫—Ç–æ—Ä —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–Ω. –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")

            return metrics

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏: {e}")
            return {}

    def create_region_analyzer(self) -> Dict[str, Any]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤

        Returns:
            Dict[str, Any]: –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏
        """
        try:
            logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤...")

            # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            regional_data = []

            for region, count in self.fraud_data.get('fraud_patterns', {}).get('by_region', {}).items():
                regional_data.append({
                    'region': region,
                    'fraud_count': count,
                    'population_factor': self._get_population_factor(region),
                    'economic_factor': self._get_economic_factor(region),
                    'risk_score': min(count / 10, 10)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ 10
                })

            df = pd.DataFrame(regional_data)

            # –ü—Ä–∏–∑–Ω–∞–∫–∏
            X = df[['population_factor', 'economic_factor']].values
            y = df['risk_score'].values

            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
            from sklearn.linear_model import LinearRegression

            lr_model = LinearRegression()
            lr_model.fit(X, y)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred = lr_model.predict(X)

            # –ú–µ—Ç—Ä–∏–∫–∏
            from sklearn.metrics import mean_squared_error, r2_score
            mse = mean_squared_error(y, y_pred)
            r2 = r2_score(y, y_pred)

            metrics = {
                'model_name': 'Regional Risk Analyzer',
                'algorithm': 'Linear Regression',
                'mse': mse,
                'r2_score': r2,
                'coefficients': lr_model.coef_.tolist(),
                'training_samples': len(X)
            }

            self.models['region_analyzer'] = lr_model
            self.model_metrics['region_analyzer'] = metrics

            logger.info(f"–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ä–µ–≥–∏–æ–Ω–æ–≤ —Å–æ–∑–¥–∞–Ω. R¬≤: {r2:.3f}")

            return metrics

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ä–µ–≥–∏–æ–Ω–æ–≤: {e}")
            return {}

    def _get_population_factor(self, region: str) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ—Ä–∞ –Ω–∞—Å–µ–ª–µ–Ω–∏—è –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞"""
        population_factors = {
            '–ú–æ—Å–∫–≤–∞': 1.0,
            '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥': 0.7,
            '–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥': 0.4,
            '–ö–∞–∑–∞–Ω—å': 0.3,
            '–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫': 0.35,
            '–†–æ—Å—Å–∏–π—Å–∫–∞—è –§–µ–¥–µ—Ä–∞—Ü–∏—è': 0.8
        }
        return population_factors.get(region, 0.2)

    def _get_economic_factor(self, region: str) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–≥–æ —Ñ–∞–∫—Ç–æ—Ä–∞ –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞"""
        economic_factors = {
            '–ú–æ—Å–∫–≤–∞': 1.0,
            '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥': 0.8,
            '–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥': 0.6,
            '–ö–∞–∑–∞–Ω—å': 0.5,
            '–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫': 0.55,
            '–†–æ—Å—Å–∏–π—Å–∫–∞—è –§–µ–¥–µ—Ä–∞—Ü–∏—è': 0.7
        }
        return economic_factors.get(region, 0.3)

    def save_models(self, models_dir: str = "data/ml_models"):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

        Args:
            models_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        """
        try:
            os.makedirs(models_dir, exist_ok=True)

            for model_name, model in self.models.items():
                model_path = os.path.join(models_dir, f"{model_name}.joblib")
                joblib.dump(model, model_path)
                logger.info(f"–ú–æ–¥–µ–ª—å {model_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –∏ —Å–∫–µ–π–ª–µ—Ä–æ–≤
            scalers_path = os.path.join(models_dir, "scalers.joblib")
            joblib.dump(self.scalers, scalers_path)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            metrics_path = os.path.join(models_dir, "model_metrics.json")
            with open(metrics_path, 'w', encoding='utf-8') as f:
                json.dump(self.model_metrics, f, ensure_ascii=False, indent=2)

            logger.info(f"–í—Å–µ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {models_dir}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")

    def predict_fraud_type(self, severity: str, region: str, amount: float, source: str = "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ") -> Dict[str, Any]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–∏–ø–∞ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞

        Args:
            severity: –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å (–Ω–∏–∑–∫–∞—è, —Å—Ä–µ–¥–Ω—è—è, –≤—ã—Å–æ–∫–∞—è, –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è)
            region: –†–µ–≥–∏–æ–Ω
            amount: –°—É–º–º–∞ –ø–æ—Ç–µ—Ä—å
            source: –ò—Å—Ç–æ—á–Ω–∏–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

        Returns:
            Dict[str, Any]: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        """
        try:
            if 'fraud_classifier' not in self.models:
                return {"error": "–ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–µ –æ–±—É—á–µ–Ω–∞"}

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            severity_numeric = {
                '–Ω–∏–∑–∫–∞—è': 1, '—Å—Ä–µ–¥–Ω—è—è': 2, '–≤—ã—Å–æ–∫–∞—è': 3, '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è': 4
            }.get(severity, 2)
            region_encoded = 0  # –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            source_encoded = 0
            amount_normalized = min(amount / 1000000, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è

            X_pred = np.array([[
                severity_numeric, region_encoded, source_encoded, amount_normalized
            ]])

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            scaler = self.scalers.get('fraud_classifier')
            if scaler:
                X_pred = scaler.transform(X_pred)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            model = self.models['fraud_classifier']
            prediction = model.predict(X_pred)[0]
            probabilities = model.predict_proba(X_pred)[0]

            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–∞ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞
            fraud_types = [
                '–±–∞–Ω–∫–æ–≤—Å–∫–æ–µ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ', '–∫–∏–±–µ—Ä–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ', '—Ç–µ–ª–µ—Ñ–æ–Ω–Ω–æ–µ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ',
                '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ', '—Ñ–∏—à–∏–Ω–≥', '–∫–∞—Ä—Ç–æ—á–Ω–æ–µ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –ø–∏—Ä–∞–º–∏–¥–∞'
            ]

            predicted_type = (
                fraud_types[prediction] if prediction < len(fraud_types) else "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø"
            )
            confidence = max(probabilities)

            return {
                'predicted_type': predicted_type,
                'confidence': confidence,
                'all_probabilities': dict(zip(fraud_types, probabilities))
            }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return {"error": str(e)}

    def generate_model_report(self) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –ø–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º

        Returns:
            Dict[str, Any]: –û—Ç—á–µ—Ç –ø–æ –º–æ–¥–µ–ª—è–º
        """
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_models': len(self.models),
            'models': self.model_metrics,
            'data_summary': {
                'total_records': self.fraud_data.get('metadata', {}).get('total_records', 0),
                'fraud_types': len(self.fraud_data.get('fraud_patterns', {}).get('by_type', {})),
                'regions': len(self.fraud_data.get('fraud_patterns', {}).get('by_region', {}))
            },
            'recommendations': [
                "–£–≤–µ–ª–∏—á–∏—Ç—å –æ–±—ä–µ–º –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏",
                "–î–æ–±–∞–≤–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —É—á–µ—Ç–∞ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏",
                "–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –±–∞–Ω–∫–æ–≤",
                "–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"
            ]
        }

        return report

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–µ–π"""
    print("ü§ñ –°–û–ó–î–ê–ù–ò–ï ML –ú–û–î–ï–õ–ï–ô –î–õ–Ø –î–ï–¢–ï–ö–¶–ò–ò –†–û–°–°–ò–ô–°–ö–û–ì–û –ú–û–®–ï–ù–ù–ò–ß–ï–°–¢–í–ê")
    print("=" * 70)

    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –∫–ª–∞—Å—Å–∞
        ml_models = RussianFraudMLModels()

        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        print("üìä –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ç–∏–ø–æ–≤ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞...")
        ml_models.create_fraud_classifier()
        
        print("üìà –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏...")
        ml_models.create_severity_predictor()
        
        print("üó∫Ô∏è –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ä–µ–≥–∏–æ–Ω–æ–≤...")
        ml_models.create_region_analyzer()

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
        ml_models.save_models()

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")
        prediction = ml_models.predict_fraud_type(
            severity="–≤—ã—Å–æ–∫–∞—è",
            region="–ú–æ—Å–∫–≤–∞",
            amount=500000,
            source="–±–∞–Ω–∫"
        )

        print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —Ç–∏–ø –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {prediction.get('predicted_type', '–æ—à–∏–±–∫–∞')}")
        print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {prediction.get('confidence', 0):.3f}")

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        report = ml_models.generate_model_report()

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report_path = "data/ml_models_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"üìã –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
        print("üéâ –ú–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ!")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
