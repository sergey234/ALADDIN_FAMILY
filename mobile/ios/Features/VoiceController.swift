import Foundation
import Speech
import AVFoundation

class VoiceController {
    static let shared = VoiceController()
    
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "ru-RU"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    
    private var isListening = false
    
    private init() {
        requestAuthorization()
    }
    
    // MARK: - Authorization
    
    func requestAuthorization() {
        SFSpeechRecognizer.requestAuthorization { status in
            DispatchQueue.main.async {
                switch status {
                case .authorized:
                    print("‚úÖ Speech recognition authorized")
                case .denied:
                    print("‚ùå Speech recognition denied")
                case .restricted:
                    print("‚ùå Speech recognition restricted")
                case .notDetermined:
                    print("‚ö†Ô∏è Speech recognition not determined")
                @unknown default:
                    print("‚ö†Ô∏è Speech recognition unknown status")
                }
            }
        }
    }
    
    // MARK: - Voice Recognition
    
    func startListening(completion: @escaping (String?) -> Void) {
        guard !isListening else {
            print("‚ö†Ô∏è Already listening")
            return
        }
        
        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            print("‚ùå Speech recognizer not available")
            completion(nil)
            return
        }
        
        do {
            // Cancel previous task
            recognitionTask?.cancel()
            recognitionTask = nil
            
            // Audio session
            let audioSession = AVAudioSession.sharedInstance()
            try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            
            // Recognition request
            recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
            
            guard let recognitionRequest = recognitionRequest else {
                print("‚ùå Unable to create recognition request")
                completion(nil)
                return
            }
            
            recognitionRequest.shouldReportPartialResults = true
            
            // Input node
            let inputNode = audioEngine.inputNode
            
            // Recognition task
            recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { result, error in
                var isFinal = false
                
                if let result = result {
                    let transcription = result.bestTranscription.formattedString
                    print("üé§ Recognized: \(transcription)")
                    completion(transcription)
                    isFinal = result.isFinal
                }
                
                if error != nil || isFinal {
                    self.audioEngine.stop()
                    inputNode.removeTap(onBus: 0)
                    
                    self.recognitionRequest = nil
                    self.recognitionTask = nil
                    self.isListening = false
                }
            }
            
            // Audio format
            let recordingFormat = inputNode.outputFormat(forBus: 0)
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
                recognitionRequest.append(buffer)
            }
            
            audioEngine.prepare()
            try audioEngine.start()
            
            isListening = true
            print("‚úÖ Listening started...")
            
        } catch {
            print("‚ùå Audio engine error: \(error.localizedDescription)")
            completion(nil)
        }
    }
    
    func stopListening() {
        guard isListening else { return }
        
        audioEngine.stop()
        recognitionRequest?.endAudio()
        
        isListening = false
        print("üõë Listening stopped")
    }
    
    // MARK: - Voice Commands
    
    func processCommand(_ command: String, completion: @escaping (VoiceCommandResult) -> Void) {
        let lowercasedCommand = command.lowercased()
        
        if lowercasedCommand.contains("–≤–∫–ª—é—á–∏ –∑–∞—â–∏—Ç—É") || lowercasedCommand.contains("–≤–∫–ª—é—á–∏—Ç—å –∑–∞—â–∏—Ç—É") {
            completion(VoiceCommandResult(action: .enableProtection, message: "–ó–∞—â–∏—Ç–∞ –≤–∫–ª—é—á–µ–Ω–∞"))
        }
        else if lowercasedCommand.contains("–≤—ã–∫–ª—é—á–∏ –∑–∞—â–∏—Ç—É") || lowercasedCommand.contains("–≤—ã–∫–ª—é—á–∏—Ç—å –∑–∞—â–∏—Ç—É") {
            completion(VoiceCommandResult(action: .disableProtection, message: "–ó–∞—â–∏—Ç–∞ –≤—ã–∫–ª—é—á–µ–Ω–∞"))
        }
        else if lowercasedCommand.contains("–≤–∫–ª—é—á–∏ –≤–ø–Ω") || lowercasedCommand.contains("–≤–∫–ª—é—á–∏—Ç—å vpn") {
            completion(VoiceCommandResult(action: .enableVPN, message: "VPN –≤–∫–ª—é—á–µ–Ω"))
        }
        else if lowercasedCommand.contains("–≤—ã–∫–ª—é—á–∏ –≤–ø–Ω") || lowercasedCommand.contains("–≤—ã–∫–ª—é—á–∏—Ç—å vpn") {
            completion(VoiceCommandResult(action: .disableVPN, message: "VPN –≤—ã–∫–ª—é—á–µ–Ω"))
        }
        else if lowercasedCommand.contains("–ø–æ–∫–∞–∂–∏ –∞–Ω–∞–ª–∏—Ç–∏–∫—É") || lowercasedCommand.contains("–ø–æ–∫–∞–∑–∞—Ç—å –∞–Ω–∞–ª–∏—Ç–∏–∫—É") {
            completion(VoiceCommandResult(action: .showAnalytics, message: "–û—Ç–∫—Ä—ã–≤–∞—é –∞–Ω–∞–ª–∏—Ç–∏–∫—É"))
        }
        else if lowercasedCommand.contains("–ø–æ–∫–∞–∂–∏ —Å–µ–º—å—é") || lowercasedCommand.contains("–ø–æ–∫–∞–∑–∞—Ç—å —Å–µ–º—å—é") {
            completion(VoiceCommandResult(action: .showFamily, message: "–û—Ç–∫—Ä—ã–≤–∞—é —Å–µ–º—å—é"))
        }
        else if lowercasedCommand.contains("–ø–æ–º–æ—â—å") || lowercasedCommand.contains("–ø–æ–º–æ–≥–∏") {
            completion(VoiceCommandResult(action: .showHelp, message: "–ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?"))
        }
        else {
            completion(VoiceCommandResult(action: .unknown, message: "–ö–æ–º–∞–Ω–¥–∞ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞"))
        }
    }
    
    // MARK: - Text to Speech
    
    func speak(_ text: String) {
        let synthesizer = AVSpeechSynthesizer()
        let utterance = AVSpeechUtterance(string: text)
        utterance.voice = AVSpeechSynthesisVoice(language: "ru-RU")
        utterance.rate = 0.5
        
        synthesizer.speak(utterance)
        print("üîä Speaking: \(text)")
    }
}

// MARK: - Data Models

struct VoiceCommandResult {
    let action: VoiceAction
    let message: String
}

enum VoiceAction {
    case enableProtection
    case disableProtection
    case enableVPN
    case disableVPN
    case showAnalytics
    case showFamily
    case showHelp
    case unknown
}

