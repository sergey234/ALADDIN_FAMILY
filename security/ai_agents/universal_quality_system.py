#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ö–ê–ß–ï–°–¢–í–ê –ù–ê 100%
–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
"""

import ast
import multiprocessing
import os
import re
import subprocess
import sys
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional


class QualityLevel(Enum):
    """–£—Ä–æ–≤–Ω–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞"""

    EXCELLENT = "excellent"  # A+ (95-100%)
    GOOD = "good"  # A (85-94%)
    FAIR = "fair"  # B (70-84%)
    POOR = "poor"  # C (50-69%)
    CRITICAL = "critical"  # D (0-49%)


class SafetyLevel(Enum):
    """–£—Ä–æ–≤–Ω–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π"""

    SAFE = "safe"  # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    MANUAL = "manual"  # –¢—Ä–µ–±—É–µ—Ç —Ä—É—á–Ω–æ–≥–æ –æ–¥–æ–±—Ä–µ–Ω–∏—è
    DANGEROUS = "dangerous"  # –û–ø–∞—Å–Ω–æ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    CRITICAL = "critical"  # –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏


class ErrorSeverity(Enum):
    """–°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å –æ—à–∏–±–æ–∫"""

    LOW = "low"  # –ù–∏–∑–∫–∞—è
    MEDIUM = "medium"  # –°—Ä–µ–¥–Ω—è—è
    HIGH = "high"  # –í—ã—Å–æ–∫–∞—è
    CRITICAL = "critical"  # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è


@dataclass
class QualityIssue:
    """–ü—Ä–æ–±–ª–µ–º–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞"""

    line_number: int
    column: int
    issue_type: str
    code: str
    description: str
    safety_level: SafetyLevel
    severity: ErrorSeverity
    category: str
    suggested_fix: Optional[str] = None
    auto_fixable: bool = False
    security_impact: str = "none"
    performance_impact: str = "none"
    confidence: float = 1.0


@dataclass
class SecurityVulnerability:
    """–£—è–∑–≤–∏–º–æ—Å—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""

    line_number: int
    vulnerability_type: str
    description: str
    severity: ErrorSeverity
    cve_reference: Optional[str] = None
    mitigation: str = ""
    risk_score: float = 0.0


@dataclass
class QualityMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞"""

    total_lines: int
    code_lines: int
    comment_lines: int
    docstring_lines: int
    blank_lines: int
    functions: int
    classes: int
    complexity: float
    maintainability_index: float
    test_coverage: float
    quality_score: float


class UniversalQualitySystem:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ 100%"""

    def __init__(self):
        self.logger = self._setup_logging()
        self.security_patterns = self._load_security_patterns()
        self.quality_rules = self._load_quality_rules()
        self.performance_patterns = self._load_performance_patterns()

    def _setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        import logging

        logger = logging.getLogger("UniversalQualitySystem")
        logger.setLevel(logging.INFO)
        return logger

    def _load_security_patterns(self) -> Dict[str, List[str]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        return {
            "dangerous_functions": [
                "eval",
                "exec",
                "compile",
                "__import__",
                "getattr",
                "setattr",
                "delattr",
                "hasattr",
                "globals",
                "locals",
                "vars",
                "dir",
            ],
            "injection_patterns": [
                r"subprocess\.call\(.*shell=True",
                r"os\.system\(",
                r"os\.popen\(",
                r"commands\.getoutput\(",
            ],
            "crypto_weak": [r"md5\(", r"sha1\(", r"DES\(", r"RC4\("],
            "auth_weak": [
                r"password.*=.*['\"].*['\"]",
                r"secret.*=.*['\"].*['\"]",
                r"token.*=.*['\"].*['\"]",
            ],
        }

    def _load_quality_rules(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –∫–∞—á–µ—Å—Ç–≤–∞"""
        return {
            "max_line_length": 79,
            "max_function_length": 50,
            "max_class_length": 200,
            "max_parameters": 5,
            "max_complexity": 10,
            "min_test_coverage": 80.0,
            "min_docstring_coverage": 70.0,
        }

    def _load_performance_patterns(self) -> Dict[str, List[str]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        return {
            "inefficient_loops": [
                r"for i in range\(len\(",
                r"\.keys\(\)",
                r"\.values\(\)",
                r"\.items\(\)",
            ],
            "memory_leaks": [r"global\s+\w+", r"nonlocal\s+\w+"],
            "slow_operations": [r"\.append\(.*\)\s*for", r"list\(.*\)\s*for"],
        }

    def analyze_file_completely(self, file_path: str) -> Dict[str, Any]:
        """–ü–û–õ–ù–´–ô –ê–ù–ê–õ–ò–ó –§–ê–ô–õ–ê –ù–ê 100%"""
        try:
            self.logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ü–û–õ–ù–´–ô –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {file_path}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
            if not os.path.exists(file_path):
                return {"error": f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}"}

            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # –í—ã–ø–æ–ª–Ω—è–µ–º –í–°–ï –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            analysis_tasks = [
                ("syntax_analysis", self._analyze_syntax_completely),
                ("import_analysis", self._analyze_imports_completely),
                ("flake8_analysis", self._analyze_flake8_completely),
                ("security_analysis", self._analyze_security_completely),
                ("performance_analysis", self._analyze_performance_completely),
                ("structure_analysis", self._analyze_structure_completely),
                (
                    "documentation_analysis",
                    self._analyze_documentation_completely,
                ),
                ("metrics_analysis", self._analyze_metrics_completely),
                ("vulnerability_scan", self._scan_vulnerabilities_completely),
                ("code_smells", self._detect_code_smells_completely),
            ]

            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞
            results = {}
            with multiprocessing.Pool() as pool:
                tasks = [
                    (task[1], file_path, content) for task in analysis_tasks
                ]
                task_results = pool.starmap(self._run_analysis_task, tasks)

                for i, (task_name, _) in enumerate(analysis_tasks):
                    results[task_name] = task_results[i]

            # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç
            complete_analysis = {
                "file_path": file_path,
                "timestamp": datetime.now().isoformat(),
                "file_size": os.path.getsize(file_path),
                "line_count": len(content.splitlines()),
                "analysis_results": results,
                "quality_summary": self._generate_quality_summary(results),
                "security_summary": self._generate_security_summary(results),
                "recommendations": (
                    self._generate_comprehensive_recommendations(results)
                ),
                "fix_plan": self._create_comprehensive_fix_plan(results),
                "overall_score": self._calculate_overall_score(results),
            }

            self.logger.info(
                f"‚úÖ –ü–û–õ–ù–´–ô –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è —Ñ–∞–π–ª–∞: {file_path}"
            )
            return complete_analysis

        except Exception as e:
            self.logger.error(
                f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞ {file_path}: {e}"
            )
            return {"error": str(e)}

    def _run_analysis_task(
        self, func, file_path: str, content: str
    ) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            return func(file_path, content)
        except Exception as e:
            return {"error": str(e)}

    def _analyze_syntax_completely(
        self, file_path: str, content: str
    ) -> Dict[str, Any]:
        """–ü–û–õ–ù–´–ô –∞–Ω–∞–ª–∏–∑ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ Python
            result = subprocess.run(
                [sys.executable, "-m", "py_compile", file_path],
                capture_output=True,
                text=True,
                timeout=30,
            )

            # –ê–Ω–∞–ª–∏–∑ AST
            try:
                tree = ast.parse(content)
                ast_analysis = {
                    "valid_ast": True,
                    "nodes_count": len(list(ast.walk(tree))),
                    "depth": self._calculate_ast_depth(tree),
                }
            except SyntaxError as e:
                ast_analysis = {
                    "valid_ast": False,
                    "syntax_error": str(e),
                    "error_line": e.lineno,
                    "error_column": e.offset,
                }

            return {
                "status": "success" if result.returncode == 0 else "error",
                "error_message": (
                    result.stderr if result.returncode != 0 else None
                ),
                "ast_analysis": ast_analysis,
                "safe_to_fix": result.returncode == 0,
            }
        except Exception as e:
            return {"error": str(e), "status": "error"}

    def _analyze_imports_completely(
        self, file_path: str, content: str
    ) -> Dict[str, Any]:
        """–ü–û–õ–ù–´–ô –∞–Ω–∞–ª–∏–∑ –∏–º–ø–æ—Ä—Ç–æ–≤"""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–º–ø–æ—Ä—Ç–æ–≤
            result = subprocess.run(
                [
                    sys.executable,
                    "-c",
                    f"import {file_path.replace('/', '.').replace('.py', '')}",
                ],
                capture_output=True,
                text=True,
                timeout=30,
            )

            # –ê–Ω–∞–ª–∏–∑ –∏–º–ø–æ—Ä—Ç–æ–≤ –≤ –∫–æ–¥–µ
            import_analysis = self._analyze_imports_in_code(content)

            return {
                "status": "success" if result.returncode == 0 else "error",
                "error_message": (
                    result.stderr if result.returncode != 0 else None
                ),
                "import_analysis": import_analysis,
                "safe_to_fix": result.returncode == 0,
            }
        except Exception as e:
            return {"error": str(e), "status": "error"}

    def _analyze_flake8_completely(
        self, file_path: str, content: str
    ) -> Dict[str, Any]:
        """–ü–û–õ–ù–´–ô –∞–Ω–∞–ª–∏–∑ flake8"""
        try:
            result = subprocess.run(
                [
                    sys.executable,
                    "-m",
                    "flake8",
                    file_path,
                    "--format=%(path)s:%(row)d:%(col)d: %(code)s %(text)s",
                ],
                capture_output=True,
                text=True,
                timeout=30,
            )

            errors = []
            if result.stdout:
                for line in result.stdout.strip().split("\n"):
                    if line:
                        parts = line.split(":", 3)
                        if len(parts) >= 4:
                            error_info = self._create_quality_issue(parts)
                            errors.append(error_info)

            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–æ–∫
            error_groups = self._group_errors_by_safety(errors)

            return {
                "total_errors": len(errors),
                "errors": errors,
                "error_groups": error_groups,
                "quality_score": self._calculate_flake8_quality_score(
                    len(errors), len(content.splitlines())
                ),
            }
        except Exception as e:
            return {"error": str(e)}

    def _analyze_security_completely(
        self, file_path: str, content: str
    ) -> Dict[str, Any]:
        """–ü–û–õ–ù–´–ô –∞–Ω–∞–ª–∏–∑ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        vulnerabilities = []

        lines = content.split("\n")
        for i, line in enumerate(lines, 1):
            line_lower = line.lower()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–ø–∞—Å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
            for pattern in self.security_patterns["dangerous_functions"]:
                if pattern in line_lower:
                    vulnerabilities.append(
                        SecurityVulnerability(
                            line_number=i,
                            vulnerability_type="dangerous_function",
                            description=(
                                f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–ø–∞—Å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏: {pattern}"
                            ),
                            severity=ErrorSeverity.HIGH,
                            risk_score=0.8,
                        )
                    )

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—ä–µ–∫—Ü–∏–π
            for pattern in self.security_patterns["injection_patterns"]:
                if re.search(pattern, line):
                    vulnerabilities.append(
                        SecurityVulnerability(
                            line_number=i,
                            vulnerability_type="injection_vulnerability",
                            description="–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è —É—è–∑–≤–∏–º–æ—Å—Ç—å –∏–Ω—ä–µ–∫—Ü–∏–∏",
                            severity=ErrorSeverity.CRITICAL,
                            risk_score=0.9,
                        )
                    )

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ª–∞–±–æ–≥–æ —à–∏—Ñ—Ä–æ–≤–∞–Ω–∏—è
            for pattern in self.security_patterns["crypto_weak"]:
                if re.search(pattern, line):
                    vulnerabilities.append(
                        SecurityVulnerability(
                            line_number=i,
                            vulnerability_type="weak_crypto",
                            description=(
                                "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–ª–∞–±–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —à–∏—Ñ—Ä–æ–≤–∞–Ω–∏—è"
                            ),
                            severity=ErrorSeverity.HIGH,
                            risk_score=0.7,
                        )
                    )

        return {
            "vulnerabilities": [v.__dict__ for v in vulnerabilities],
            "total_vulnerabilities": len(vulnerabilities),
            "critical_vulnerabilities": len(
                [
                    v
                    for v in vulnerabilities
                    if v.severity == ErrorSeverity.CRITICAL
                ]
            ),
            "high_vulnerabilities": len(
                [
                    v
                    for v in vulnerabilities
                    if v.severity == ErrorSeverity.HIGH
                ]
            ),
            "security_score": self._calculate_security_score(vulnerabilities),
        }

    def _analyze_performance_completely(
        self, file_path: str, content: str
    ) -> Dict[str, Any]:
        """–ü–û–õ–ù–´–ô –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        performance_issues = []

        lines = content.split("\n")
        for i, line in enumerate(lines, 1):

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ü–∏–∫–ª–æ–≤
            for pattern in self.performance_patterns["inefficient_loops"]:
                if re.search(pattern, line):
                    performance_issues.append(
                        {
                            "line": i,
                            "type": "inefficient_loop",
                            "description": "–ù–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ü–∏–∫–ª",
                            "suggestion": (
                                "–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ —Ü–∏–∫–ª –¥–ª—è –ª—É—á—à–µ–π "
                                "–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"
                            ),
                            "severity": "medium",
                        }
                    )

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Ç–µ—á–µ–∫ –ø–∞–º—è—Ç–∏
            for pattern in self.performance_patterns["memory_leaks"]:
                if re.search(pattern, line):
                    performance_issues.append(
                        {
                            "line": i,
                            "type": "memory_leak",
                            "description": "–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è —É—Ç–µ—á–∫–∞ –ø–∞–º—è—Ç–∏",
                            "suggestion": (
                                "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ "
                                "–≤–º–µ—Å—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö"
                            ),
                            "severity": "high",
                        }
                    )

        return {
            "performance_issues": performance_issues,
            "total_issues": len(performance_issues),
            "performance_score": self._calculate_performance_score(
                performance_issues
            ),
        }

    def _analyze_structure_completely(
        self, file_path: str, content: str
    ) -> Dict[str, Any]:
        """–ü–û–õ–ù–´–ô –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–æ–¥–∞"""
        try:
            tree = ast.parse(content)

            classes = []
            functions = []
            complexity = 0

            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    class_info = {
                        "name": node.name,
                        "line": node.lineno,
                        "methods": [
                            n.name
                            for n in node.body
                            if isinstance(n, ast.FunctionDef)
                        ],
                        "complexity": self._calculate_complexity(node),
                    }
                    classes.append(class_info)
                    complexity += class_info["complexity"]

                elif isinstance(node, ast.FunctionDef):
                    func_info = {
                        "name": node.name,
                        "line": node.lineno,
                        "args": [arg.arg for arg in node.args.args],
                        "is_async": isinstance(node, ast.AsyncFunctionDef),
                        "complexity": self._calculate_complexity(node),
                    }
                    functions.append(func_info)
                    complexity += func_info["complexity"]

            return {
                "classes": classes,
                "functions": functions,
                "total_classes": len(classes),
                "total_functions": len(functions),
                "total_complexity": complexity,
                "average_complexity": complexity
                / max(len(functions) + len(classes), 1),
                "structure_score": self._calculate_structure_score(
                    classes, functions, complexity
                ),
            }
        except Exception as e:
            return {"error": str(e)}

    def _analyze_documentation_completely(
        self, file_path: str, content: str
    ) -> Dict[str, Any]:
        """–ü–û–õ–ù–´–ô –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        lines = content.split("\n")
        docstring_lines = 0
        comment_lines = 0
        total_lines = len(lines)

        in_docstring = False
        docstring_quote = None

        for line in lines:
            stripped = line.strip()

            # –ü–æ–¥—Å—á–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
            if stripped.startswith("#"):
                comment_lines += 1

            # –ü–æ–¥—Å—á–µ—Ç docstring
            if '"""' in line or "'''" in line:
                if not in_docstring:
                    in_docstring = True
                    docstring_quote = '"""' if '"""' in line else "'''"
                elif docstring_quote in line:
                    in_docstring = False
                    docstring_quote = None

            if in_docstring:
                docstring_lines += 1

        return {
            "total_lines": total_lines,
            "comment_lines": comment_lines,
            "docstring_lines": docstring_lines,
            "comment_percentage": (
                (comment_lines / total_lines * 100) if total_lines > 0 else 0
            ),
            "docstring_percentage": (
                (docstring_lines / total_lines * 100) if total_lines > 0 else 0
            ),
            "documentation_score": self._calculate_documentation_score(
                comment_lines, docstring_lines, total_lines
            ),
        }

    def _analyze_metrics_completely(
        self, file_path: str, content: str
    ) -> Dict[str, Any]:
        """–ü–û–õ–ù–´–ô –∞–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫"""
        lines = content.split("\n")
        code_lines = 0
        comment_lines = 0
        blank_lines = 0

        for line in lines:
            stripped = line.strip()
            if not stripped:
                blank_lines += 1
            elif stripped.startswith("#"):
                comment_lines += 1
            else:
                code_lines += 1

        return {
            "total_lines": len(lines),
            "code_lines": code_lines,
            "comment_lines": comment_lines,
            "blank_lines": blank_lines,
            "code_ratio": code_lines / len(lines) if lines else 0,
            "comment_ratio": comment_lines / len(lines) if lines else 0,
            "blank_ratio": blank_lines / len(lines) if lines else 0,
        }

    def _scan_vulnerabilities_completely(
        self, file_path: str, content: str
    ) -> Dict[str, Any]:
        """–ü–û–õ–ù–û–ï —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π"""
        vulnerabilities = []

        # –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏
        vulnerability_patterns = {
            "sql_injection": [
                r"SELECT.*\+.*%",
                r"INSERT.*\+.*%",
                r"UPDATE.*\+.*%",
            ],
            "xss": [r"innerHTML\s*=", r"document\.write\("],
            "path_traversal": [r"\.\./", r"\.\.\\"],
            "command_injection": [
                r"subprocess\.call\(.*shell=True",
                r"os\.system\(",
            ],
        }

        lines = content.split("\n")
        for i, line in enumerate(lines, 1):
            for vuln_type, patterns in vulnerability_patterns.items():
                for pattern in patterns:
                    if re.search(pattern, line, re.IGNORECASE):
                        vulnerabilities.append(
                            {
                                "line": i,
                                "type": vuln_type,
                                "pattern": pattern,
                                "severity": "high",
                            }
                        )

        return {
            "vulnerabilities": vulnerabilities,
            "total_vulnerabilities": len(vulnerabilities),
            "vulnerability_score": self._calculate_vulnerability_score(
                vulnerabilities
            ),
        }

    def _detect_code_smells_completely(
        self, file_path: str, content: str
    ) -> Dict[str, Any]:
        """–ü–û–õ–ù–û–ï –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∑–∞–ø–∞—Ö–æ–≤ –∫–æ–¥–∞"""
        code_smells = []

        lines = content.split("\n")
        for i, line in enumerate(lines, 1):

            # –î–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
            if len(line) > 79:
                code_smells.append(
                    {
                        "line": i,
                        "type": "long_line",
                        "description": "–°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞",
                        "severity": "low",
                    }
                )

            # –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞
            if line.strip() and line.count(line.strip()) > 1:
                code_smells.append(
                    {
                        "line": i,
                        "type": "duplicate_code",
                        "description": "–î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞",
                        "severity": "medium",
                    }
                )

            # –°–ª–æ–∂–Ω—ã–µ —É—Å–ª–æ–≤–∏—è
            if line.count("and") + line.count("or") > 3:
                code_smells.append(
                    {
                        "line": i,
                        "type": "complex_condition",
                        "description": "–°–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–æ–µ —É—Å–ª–æ–≤–∏–µ",
                        "severity": "medium",
                    }
                )

        return {
            "code_smells": code_smells,
            "total_smells": len(code_smells),
            "smell_score": self._calculate_smell_score(code_smells),
        }

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    def _create_quality_issue(self, parts: List[str]) -> QualityIssue:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ –ø—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞"""
        line = int(parts[1])
        column = int(parts[2])
        code = parts[3].split()[0]
        message = (
            parts[3].split(" ", 1)[1] if len(parts[3].split()) > 1 else ""
        )

        return QualityIssue(
            line_number=line,
            column=column,
            issue_type=code,
            code=code,
            description=message,
            safety_level=self._get_safety_level(code),
            severity=self._get_severity(code),
            category=self._get_category(code),
            auto_fixable=self._is_auto_fixable(code),
        )

    def _get_safety_level(self, code: str) -> SafetyLevel:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        safe_codes = ["E501", "W293", "W292", "E302", "E305", "E128", "E129"]
        manual_codes = ["F401", "F841", "E302", "E305"]
        dangerous_codes = ["F403", "F405", "F811", "F812"]
        critical_codes = ["F999"]

        if code in safe_codes:
            return SafetyLevel.SAFE
        elif code in manual_codes:
            return SafetyLevel.MANUAL
        elif code in dangerous_codes:
            return SafetyLevel.DANGEROUS
        elif code in critical_codes:
            return SafetyLevel.CRITICAL
        else:
            return SafetyLevel.MANUAL

    def _get_severity(self, code: str) -> ErrorSeverity:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏"""
        critical_codes = ["F999", "F403", "F405"]
        high_codes = ["F401", "F841", "E999"]
        medium_codes = ["E501", "E302", "E305"]
        low_codes = ["W293", "W292", "W291"]

        if code in critical_codes:
            return ErrorSeverity.CRITICAL
        elif code in high_codes:
            return ErrorSeverity.HIGH
        elif code in medium_codes:
            return ErrorSeverity.MEDIUM
        elif code in low_codes:
            return ErrorSeverity.LOW
        else:
            return ErrorSeverity.MEDIUM

    def _get_category(self, code: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        if code.startswith("E"):
            return "error"
        elif code.startswith("W"):
            return "warning"
        elif code.startswith("F"):
            return "fatal"
        else:
            return "unknown"

    def _is_auto_fixable(self, code: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∞–≤—Ç–æ–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è"""
        auto_fixable_codes = [
            "E501",
            "W293",
            "W292",
            "E302",
            "E305",
            "E128",
            "E129",
        ]
        return code in auto_fixable_codes

    def _group_errors_by_safety(
        self, errors: List[QualityIssue]
    ) -> Dict[str, List[QualityIssue]]:
        """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–æ–∫ –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        groups = {"safe": [], "manual": [], "dangerous": [], "critical": []}

        for error in errors:
            groups[error.safety_level.value].append(error)

        return groups

    def _calculate_overall_score(self, results: Dict[str, Any]) -> float:
        """–†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –±–∞–ª–ª–∞ –∫–∞—á–µ—Å—Ç–≤–∞"""
        scores = []

        if (
            "flake8_analysis" in results
            and "quality_score" in results["flake8_analysis"]
        ):
            scores.append(results["flake8_analysis"]["quality_score"])

        if (
            "security_analysis" in results
            and "security_score" in results["security_analysis"]
        ):
            scores.append(results["security_analysis"]["security_score"])

        if (
            "performance_analysis" in results
            and "performance_score" in results["performance_analysis"]
        ):
            scores.append(results["performance_analysis"]["performance_score"])

        if (
            "structure_analysis" in results
            and "structure_score" in results["structure_analysis"]
        ):
            scores.append(results["structure_analysis"]["structure_score"])

        if (
            "documentation_analysis" in results
            and "documentation_score" in results["documentation_analysis"]
        ):
            scores.append(
                results["documentation_analysis"]["documentation_score"]
            )

        return sum(scores) / len(scores) if scores else 0.0

    def _generate_quality_summary(
        self, results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞"""
        return {
            "overall_quality": (
                "A+"
                if self._calculate_overall_score(results) > 90
                else (
                    "A" if self._calculate_overall_score(results) > 80 else "B"
                )
            ),
            "total_issues": sum(
                len(results.get(key, {}).get("errors", []))
                for key in results
                if "errors" in str(results.get(key, {}))
            ),
            "critical_issues": 0,  # –ë—É–¥–µ—Ç —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–æ
            "recommendations_count": 0,  # –ë—É–¥–µ—Ç —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–æ
        }

    def _generate_security_summary(
        self, results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        security_analysis = results.get("security_analysis", {})
        return {
            "vulnerabilities_found": security_analysis.get(
                "total_vulnerabilities", 0
            ),
            "critical_vulnerabilities": security_analysis.get(
                "critical_vulnerabilities", 0
            ),
            "security_score": security_analysis.get("security_score", 100),
            "recommendations": [
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Å–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏",
                "–û–±–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏",
            ],
        }

    def _generate_comprehensive_recommendations(
        self, results: Dict[str, Any]
    ) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        recommendations = []

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
        if "flake8_analysis" in results:
            flake8 = results["flake8_analysis"]
            if flake8.get("total_errors", 0) > 0:
                recommendations.append(
                    f"–ò—Å–ø—Ä–∞–≤—å—Ç–µ {flake8['total_errors']} –æ—à–∏–±–æ–∫ flake8"
                )

        if "security_analysis" in results:
            security = results["security_analysis"]
            if security.get("total_vulnerabilities", 0) > 0:
                recommendations.append(
                    f"–ò—Å–ø—Ä–∞–≤—å—Ç–µ {security['total_vulnerabilities']} "
                    f"—É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"
                )

        if "documentation_analysis" in results:
            doc = results["documentation_analysis"]
            if doc.get("comment_percentage", 0) < 10:
                recommendations.append("–î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∫ –∫–æ–¥—É")

        return recommendations

    def _create_comprehensive_fix_plan(
        self, results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø–ª–∞–Ω–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π"""
        plan = {
            "total_issues": 0,
            "safe_fixes": 0,
            "manual_fixes": 0,
            "dangerous_fixes": 0,
            "critical_fixes": 0,
            "steps": [],
        }

        # –ü–æ–¥—Å—á–µ—Ç –ø—Ä–æ–±–ª–µ–º
        if "flake8_analysis" in results:
            flake8 = results["flake8_analysis"]
            error_groups = flake8.get("error_groups", {})
            plan["safe_fixes"] += len(error_groups.get("safe", []))
            plan["manual_fixes"] += len(error_groups.get("manual", []))
            plan["dangerous_fixes"] += len(error_groups.get("dangerous", []))
            plan["critical_fixes"] += len(error_groups.get("critical", []))

        plan["total_issues"] = (
            plan["safe_fixes"]
            + plan["manual_fixes"]
            + plan["dangerous_fixes"]
            + plan["critical_fixes"]
        )

        # –°–æ–∑–¥–∞–Ω–∏–µ —à–∞–≥–æ–≤
        if plan["safe_fixes"] > 0:
            plan["steps"].append(
                {
                    "step": 1,
                    "action": "–ò—Å–ø—Ä–∞–≤–∏—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –æ—à–∏–±–∫–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è",
                    "count": plan["safe_fixes"],
                    "priority": "high",
                    "estimated_time": "5-10 –º–∏–Ω—É—Ç",
                }
            )

        if plan["manual_fixes"] > 0:
            plan["steps"].append(
                {
                    "step": 2,
                    "action": "–†—É—á–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ",
                    "count": plan["manual_fixes"],
                    "priority": "medium",
                    "estimated_time": "15-30 –º–∏–Ω—É—Ç",
                }
            )

        return plan

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —Ä–∞—Å—á–µ—Ç–æ–≤
    def _calculate_ast_depth(self, tree: ast.AST) -> int:
        """–†–∞—Å—á–µ—Ç –≥–ª—É–±–∏–Ω—ã AST"""

        def depth(node):
            if isinstance(node, ast.AST):
                return 1 + max(
                    (depth(child) for child in ast.iter_child_nodes(node)),
                    default=0,
                )
            return 0

        return depth(tree)

    def _calculate_complexity(self, node: ast.AST) -> int:
        """–†–∞—Å—á–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —É–∑–ª–∞"""
        complexity = 1
        for child in ast.iter_child_nodes(node):
            if isinstance(
                child,
                (ast.If, ast.While, ast.For, ast.AsyncFor, ast.ExceptHandler),
            ):
                complexity += 1
            complexity += self._calculate_complexity(child)
        return complexity

    def _calculate_flake8_quality_score(
        self, error_count: int, line_count: int
    ) -> float:
        """–†–∞—Å—á–µ—Ç –±–∞–ª–ª–∞ –∫–∞—á–µ—Å—Ç–≤–∞ flake8"""
        if line_count == 0:
            return 100.0
        error_ratio = error_count / line_count
        return max(0, 100 - (error_ratio * 100))

    def _calculate_security_score(
        self, vulnerabilities: List[SecurityVulnerability]
    ) -> float:
        """–†–∞—Å—á–µ—Ç –±–∞–ª–ª–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        if not vulnerabilities:
            return 100.0

        total_risk = sum(v.risk_score for v in vulnerabilities)
        return max(0, 100 - (total_risk * 50))

    def _calculate_performance_score(self, issues: List[Dict]) -> float:
        """–†–∞—Å—á–µ—Ç –±–∞–ª–ª–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if not issues:
            return 100.0

        severity_penalty = sum(
            (
                0.1
                if issue["severity"] == "low"
                else 0.2 if issue["severity"] == "medium" else 0.3
            )
            for issue in issues
        )
        return max(0, 100 - (severity_penalty * 100))

    def _calculate_structure_score(
        self, classes: List[Dict], functions: List[Dict], complexity: int
    ) -> float:
        """–†–∞—Å—á–µ—Ç –±–∞–ª–ª–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        if not classes and not functions:
            return 0.0

        avg_complexity = complexity / max(len(classes) + len(functions), 1)
        if avg_complexity <= 5:
            return 100.0
        elif avg_complexity <= 10:
            return 80.0
        elif avg_complexity <= 20:
            return 60.0
        else:
            return 40.0

    def _calculate_documentation_score(
        self, comment_lines: int, docstring_lines: int, total_lines: int
    ) -> float:
        """–†–∞—Å—á–µ—Ç –±–∞–ª–ª–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        if total_lines == 0:
            return 0.0

        doc_ratio = (comment_lines + docstring_lines) / total_lines
        return min(100, doc_ratio * 200)

    def _calculate_vulnerability_score(
        self, vulnerabilities: List[Dict]
    ) -> float:
        """–†–∞—Å—á–µ—Ç –±–∞–ª–ª–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π"""
        if not vulnerabilities:
            return 100.0

        high_severity_count = len(
            [v for v in vulnerabilities if v.get("severity") == "high"]
        )
        return max(0, 100 - (high_severity_count * 20))

    def _calculate_smell_score(self, smells: List[Dict]) -> float:
        """–†–∞—Å—á–µ—Ç –±–∞–ª–ª–∞ –∑–∞–ø–∞—Ö–æ–≤ –∫–æ–¥–∞"""
        if not smells:
            return 100.0

        return max(0, 100 - (len(smells) * 5))

    def _analyze_imports_in_code(self, content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∏–º–ø–æ—Ä—Ç–æ–≤ –≤ –∫–æ–¥–µ"""
        try:
            tree = ast.parse(content)
            imports = []

            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.append(
                            {
                                "type": "import",
                                "name": alias.name,
                                "alias": alias.asname,
                                "line": node.lineno,
                            }
                        )
                elif isinstance(node, ast.ImportFrom):
                    for alias in node.names:
                        imports.append(
                            {
                                "type": "from_import",
                                "module": node.module,
                                "name": alias.name,
                                "alias": alias.asname,
                                "line": node.lineno,
                            }
                        )

            return {
                "imports": imports,
                "total_imports": len(imports),
                "unused_imports": [],  # –ë—É–¥–µ—Ç —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–æ –æ—Ç–¥–µ–ª—å–Ω–æ
            }
        except Exception as e:
            return {"error": str(e), "imports": [], "total_imports": 0}


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
universal_quality_system = UniversalQualitySystem()


def analyze_file_universally(file_path: str) -> Dict[str, Any]:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞ –Ω–∞ 100%"""
    return universal_quality_system.analyze_file_completely(file_path)


if __name__ == "__main__":
    if len(sys.argv) > 1:
        file_path = sys.argv[1]
        print("üöÄ –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ö–ê–ß–ï–°–¢–í–ê –ù–ê 100%")
        print("=" * 60)
        print(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª: {file_path}")
        print()

        result = analyze_file_universally(file_path)
        if "error" in result:
            print(f"‚ùå –û—à–∏–±–∫–∞: {result['error']}")
        else:
            print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            print(f"üìä –û–±—â–∏–π –±–∞–ª–ª: {result['overall_score']:.1f}")
            print(f"üìÅ –§–∞–π–ª: {result['file_path']}")
            print(f"üìè –°—Ç—Ä–æ–∫: {result['line_count']}")
            print(f"üìä –†–∞–∑–º–µ—Ä: {result['file_size']} –±–∞–π—Ç")
    else:
        print(
            "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python3 universal_quality_system.py <–ø—É—Ç—å_–∫_—Ñ–∞–π–ª—É>"
        )
