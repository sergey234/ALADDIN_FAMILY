#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Improved ML Models - –£–ª—É—á—à–µ–Ω–Ω—ã–µ ML –º–æ–¥–µ–ª–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
"""

import json
import logging
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, Any
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import joblib
import os
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ImprovedMLModels:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–µ ML –º–æ–¥–µ–ª–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    """

    def __init__(self, enhanced_data_path: str = "data/enhanced_training_data.json"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

        Args:
            enhanced_data_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        self.enhanced_data_path = enhanced_data_path
        self.models = {}
        self.encoders = {}
        self.scalers = {}
        self.feature_columns = []
        self.model_metrics = {}

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self.enhanced_data = self._load_enhanced_data()

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        os.makedirs('data/improved_ml_models', exist_ok=True)

        logger.info("üöÄ –£–ª—É—á—à–µ–Ω–Ω—ã–µ ML –º–æ–¥–µ–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")

    def _load_enhanced_data(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            with open(self.enhanced_data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {data['metadata']['total_records']} –∑–∞–ø–∏—Å–µ–π")
            return data
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
            return {}

    def _prepare_enhanced_training_data(self) -> tuple:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

        Returns:
            tuple: X (–ø—Ä–∏–∑–Ω–∞–∫–∏), y (—Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ)
        """
        records = []

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¶–ë –†–§
        for report in self.enhanced_data.get('cbr_data', []):
            records.append({
                'fraud_type': report['fraud_type'],
                'severity': report['severity'],
                'region': report['region'],
                'amount_lost': report['amount_lost'],
                'source': '–¶–ë –†–§',
                'description': report['description']
            })

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        for article in self.enhanced_data.get('news_data', []):
            records.append({
                'fraud_type': article['fraud_type'],
                'severity': article['severity'],
                'region': article['region'],
                'amount_lost': article['amount_lost'],
                'source': article['source'],
                'description': article['description']
            })

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        for gov_item in self.enhanced_data.get('government_data', []):
            records.append({
                'fraud_type': gov_item['fraud_type'],
                'severity': gov_item['severity'],
                'region': gov_item['region'],
                'amount_lost': gov_item['amount_lost'],
                'source': gov_item['source'],
                'description': gov_item['description']
            })

        df = pd.DataFrame(records)

        # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df['severity_numeric'] = df['severity'].map({
            '–Ω–∏–∑–∫–∞—è': 1,
            '—Å—Ä–µ–¥–Ω—è—è': 2,
            '–≤—ã—Å–æ–∫–∞—è': 3,
            '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è': 4
        })

        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df['region_encoded'] = LabelEncoder().fit_transform(df['region'])
        df['source_encoded'] = LabelEncoder().fit_transform(df['source'])
        df['fraud_type_encoded'] = LabelEncoder().fit_transform(df['fraud_type'])

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—É–º–º—ã –ø–æ—Ç–µ—Ä—å
        df['amount_normalized'] = (df['amount_lost'] - df['amount_lost'].min()) / (df['amount_lost'].max() - df['amount_lost'].min())

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['description_length'] = df['description'].str.len()
        df['description_length_normalized'] = (df['description_length'] - df['description_length'].min()) / (df['description_length'].max() - df['description_length'].min())

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['is_weekend'] = 0  # –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        df['is_major_city'] = df['region'].isin(['–ú–æ—Å–∫–≤–∞', '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥']).astype(int)

        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏
        feature_columns = [
            'severity_numeric', 'region_encoded', 'source_encoded',
            'amount_normalized', 'description_length_normalized',
            'is_weekend', 'is_major_city'
        ]

        X = df[feature_columns].values
        y = df['fraud_type_encoded'].values

        self.feature_columns = feature_columns

        return X, y

    def create_enhanced_fraud_classifier(self) -> Dict[str, Any]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ç–∏–ø–æ–≤ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞

        Returns:
            Dict[str, Any]: –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏
        """
        try:
            logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ç–∏–ø–æ–≤ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞...")

            X, y = self._prepare_enhanced_training_data()

            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            self.scalers['enhanced_fraud_classifier'] = scaler

            # –û–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ Random Forest
            rf_model = RandomForestClassifier(
                n_estimators=200,
                random_state=42,
                max_depth=15,
                min_samples_split=3,
                min_samples_leaf=2,
                max_features='sqrt'
            )
            rf_model.fit(X_train_scaled, y_train)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred = rf_model.predict(X_test_scaled)

            # –ú–µ—Ç—Ä–∏–∫–∏
            accuracy = accuracy_score(y_test, y_pred)
            report = classification_report(y_test, y_pred, output_dict=True)

            metrics = {
                'model_name': 'Enhanced Fraud Type Classifier',
                'algorithm': 'Random Forest (Enhanced)',
                'accuracy': accuracy,
                'classification_report': report,
                'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),
                'feature_importance': rf_model.feature_importances_.tolist(),
                'training_samples': len(X_train),
                'test_samples': len(X_test),
                'feature_names': self.feature_columns
            }

            self.models['enhanced_fraud_classifier'] = rf_model
            self.model_metrics['enhanced_fraud_classifier'] = metrics

            logger.info(f"–£–ª—É—á—à–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω. –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")

            return metrics

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞: {e}")
            return {}

    def create_enhanced_severity_predictor(self) -> Dict[str, Any]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏

        Returns:
            Dict[str, Any]: –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏
        """
        try:
            logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏...")

            X, y = self._prepare_enhanced_training_data()

            # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
            records = []
            for report in self.enhanced_data.get('cbr_data', []):
                records.append({
                    'fraud_type_encoded': LabelEncoder().fit_transform([report['fraud_type']])[0],
                    'region_encoded': LabelEncoder().fit_transform([report['region']])[0],
                    'amount_normalized': report['amount_lost'] / 100000000,  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                    'severity': report['severity'],
                    'is_major_city': 1 if report['region'] in ['–ú–æ—Å–∫–≤–∞', '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥'] else 0
                })

            for article in self.enhanced_data.get('news_data', []):
                records.append({
                    'fraud_type_encoded': LabelEncoder().fit_transform([article['fraud_type']])[0],
                    'region_encoded': LabelEncoder().fit_transform([article['region']])[0],
                    'amount_normalized': article['amount_lost'] / 100000000,
                    'severity': article['severity'],
                    'is_major_city': 1 if article['region'] in ['–ú–æ—Å–∫–≤–∞', '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥'] else 0
                })

            for gov_item in self.enhanced_data.get('government_data', []):
                records.append({
                    'fraud_type_encoded': LabelEncoder().fit_transform([gov_item['fraud_type']])[0],
                    'region_encoded': LabelEncoder().fit_transform([gov_item['region']])[0],
                    'amount_normalized': gov_item['amount_lost'] / 100000000,
                    'severity': gov_item['severity'],
                    'is_major_city': 1 if gov_item['region'] in ['–ú–æ—Å–∫–≤–∞', '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥'] else 0
                })

            df = pd.DataFrame(records)
            X_severity = df[['fraud_type_encoded', 'region_encoded', 'amount_normalized', 'is_major_city']].values
            y_severity = df['severity'].map({
                '–Ω–∏–∑–∫–∞—è': 1,
                '—Å—Ä–µ–¥–Ω—è—è': 2,
                '–≤—ã—Å–æ–∫–∞—è': 3,
                '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è': 4
            }).values

            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            X_train, X_test, y_train, y_test = train_test_split(
                X_severity, y_severity, test_size=0.2, random_state=42, stratify=y_severity
            )

            # –û–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ Gradient Boosting
            gb_model = GradientBoostingClassifier(
                n_estimators=200,
                random_state=42,
                max_depth=8,
                learning_rate=0.05,
                subsample=0.8
            )
            gb_model.fit(X_train, y_train)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred = gb_model.predict(X_test)

            # –ú–µ—Ç—Ä–∏–∫–∏
            accuracy = accuracy_score(y_test, y_pred)

            metrics = {
                'model_name': 'Enhanced Severity Predictor',
                'algorithm': 'Gradient Boosting (Enhanced)',
                'accuracy': accuracy,
                'feature_importance': gb_model.feature_importances_.tolist(),
                'training_samples': len(X_train),
                'test_samples': len(X_test),
                'feature_names': ['fraud_type_encoded', 'region_encoded', 'amount_normalized', 'is_major_city']
            }

            self.models['enhanced_severity_predictor'] = gb_model
            self.model_metrics['enhanced_severity_predictor'] = metrics

            logger.info(f"–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–Ω. –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")

            return metrics

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏: {e}")
            return {}

    def create_enhanced_region_analyzer(self) -> Dict[str, Any]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ä–∏—Å–∫–æ–≤

        Returns:
            Dict[str, Any]: –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏
        """
        try:
            logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ä–∏—Å–∫–æ–≤...")

            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            regional_data = []

            for region, count in self.enhanced_data.get('fraud_patterns', {}).get('by_region', {}).items():
                regional_data.append({
                    'region': region,
                    'fraud_count': count,
                    'population_factor': self._get_population_factor(region),
                    'economic_factor': self._get_economic_factor(region),
                    'is_major_city': 1 if region in ['–ú–æ—Å–∫–≤–∞', '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥'] else 0,
                    'risk_score': min(count / 10, 10)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ 10
                })

            df = pd.DataFrame(regional_data)

            # –ü—Ä–∏–∑–Ω–∞–∫–∏
            X = df[['population_factor', 'economic_factor', 'is_major_city']].values
            y = df['risk_score'].values

            # –û–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
            from sklearn.ensemble import RandomForestRegressor

            rf_regressor = RandomForestRegressor(
                n_estimators=100,
                random_state=42,
                max_depth=10
            )
            rf_regressor.fit(X, y)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred = rf_regressor.predict(X)

            # –ú–µ—Ç—Ä–∏–∫–∏
            from sklearn.metrics import mean_squared_error, r2_score
            mse = mean_squared_error(y, y_pred)
            r2 = r2_score(y, y_pred)

            metrics = {
                'model_name': 'Enhanced Regional Risk Analyzer',
                'algorithm': 'Random Forest Regressor (Enhanced)',
                'mse': mse,
                'r2_score': r2,
                'feature_importance': rf_regressor.feature_importances_.tolist(),
                'training_samples': len(X),
                'feature_names': ['population_factor', 'economic_factor', 'is_major_city']
            }

            self.models['enhanced_region_analyzer'] = rf_regressor
            self.model_metrics['enhanced_region_analyzer'] = metrics

            logger.info(f"–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ä–µ–≥–∏–æ–Ω–æ–≤ —Å–æ–∑–¥–∞–Ω. R¬≤: {r2:.3f}")

            return metrics

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ä–µ–≥–∏–æ–Ω–æ–≤: {e}")
            return {}

    def _get_population_factor(self, region: str) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ—Ä–∞ –Ω–∞—Å–µ–ª–µ–Ω–∏—è –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞"""
        population_factors = {
            '–ú–æ—Å–∫–≤–∞': 1.0,
            '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥': 0.7,
            '–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥': 0.4,
            '–ö–∞–∑–∞–Ω—å': 0.3,
            '–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫': 0.35,
            '–†–æ—Å—Å–∏–π—Å–∫–∞—è –§–µ–¥–µ—Ä–∞—Ü–∏—è': 0.8
        }
        return population_factors.get(region, 0.2)

    def _get_economic_factor(self, region: str) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–≥–æ —Ñ–∞–∫—Ç–æ—Ä–∞ –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞"""
        economic_factors = {
            '–ú–æ—Å–∫–≤–∞': 1.0,
            '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥': 0.8,
            '–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥': 0.6,
            '–ö–∞–∑–∞–Ω—å': 0.5,
            '–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫': 0.55,
            '–†–æ—Å—Å–∏–π—Å–∫–∞—è –§–µ–¥–µ—Ä–∞—Ü–∏—è': 0.7
        }
        return economic_factors.get(region, 0.3)

    def save_improved_models(self, models_dir: str = "data/improved_ml_models"):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

        Args:
            models_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        """
        try:
            os.makedirs(models_dir, exist_ok=True)

            for model_name, model in self.models.items():
                model_path = os.path.join(models_dir, f"{model_name}.joblib")
                joblib.dump(model, model_path)
                logger.info(f"–ú–æ–¥–µ–ª—å {model_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –∏ —Å–∫–µ–π–ª–µ—Ä–æ–≤
            scalers_path = os.path.join(models_dir, "enhanced_scalers.joblib")
            joblib.dump(self.scalers, scalers_path)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            metrics_path = os.path.join(models_dir, "enhanced_model_metrics.json")
            with open(metrics_path, 'w', encoding='utf-8') as f:
                json.dump(self.model_metrics, f, ensure_ascii=False, indent=2)

            logger.info(f"–í—Å–µ —É–ª—É—á—à–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {models_dir}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {e}")

    def predict_enhanced_fraud_type(self, severity: str, region: str, amount: float, source: str = "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ") -> Dict[str, Any]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–∏–ø–∞ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é

        Args:
            severity: –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å (–Ω–∏–∑–∫–∞—è, —Å—Ä–µ–¥–Ω—è—è, –≤—ã—Å–æ–∫–∞—è, –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è)
            region: –†–µ–≥–∏–æ–Ω
            amount: –°—É–º–º–∞ –ø–æ—Ç–µ—Ä—å
            source: –ò—Å—Ç–æ—á–Ω–∏–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

        Returns:
            Dict[str, Any]: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        """
        try:
            if 'enhanced_fraud_classifier' not in self.models:
                return {"error": "–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–µ –æ–±—É—á–µ–Ω–∞"}

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            severity_numeric = {'–Ω–∏–∑–∫–∞—è': 1, '—Å—Ä–µ–¥–Ω—è—è': 2, '–≤—ã—Å–æ–∫–∞—è': 3, '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è': 4}.get(severity, 2)
            region_encoded = 0  # –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            source_encoded = 0
            amount_normalized = min(amount / 100000000, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            description_length_normalized = 0.5  # –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            is_weekend = 0
            is_major_city = 1 if region in ['–ú–æ—Å–∫–≤–∞', '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥'] else 0

            X_pred = np.array([[severity_numeric, region_encoded, source_encoded, amount_normalized,
                               description_length_normalized, is_weekend, is_major_city]])

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            scaler = self.scalers.get('enhanced_fraud_classifier')
            if scaler:
                X_pred = scaler.transform(X_pred)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            model = self.models['enhanced_fraud_classifier']
            prediction = model.predict(X_pred)[0]
            probabilities = model.predict_proba(X_pred)[0]

            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–∞ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞
            fraud_types = ['–±–∞–Ω–∫–æ–≤—Å–∫–æ–µ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ', '–∫–∏–±–µ—Ä–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ', '—Ç–µ–ª–µ—Ñ–æ–Ω–Ω–æ–µ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ',
                          '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ', '—Ñ–∏—à–∏–Ω–≥', '–∫–∞—Ä—Ç–æ—á–Ω–æ–µ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –ø–∏—Ä–∞–º–∏–¥–∞',
                          '–∫–∏–±–µ—Ä–∞—Ç–∞–∫–∞', '–æ–±—â–µ–µ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ']

            predicted_type = fraud_types[prediction] if prediction < len(fraud_types) else "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø"
            confidence = max(probabilities)

            return {
                'predicted_type': predicted_type,
                'confidence': confidence,
                'all_probabilities': dict(zip(fraud_types, probabilities)),
                'model_version': 'enhanced_v1.0'
            }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            return {"error": str(e)}

    def generate_improved_report(self) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –ø–æ —É–ª—É—á—à–µ–Ω–Ω—ã–º –º–æ–¥–µ–ª—è–º

        Returns:
            Dict[str, Any]: –û—Ç—á–µ—Ç –ø–æ –º–æ–¥–µ–ª—è–º
        """
        report = {
            'timestamp': datetime.now().isoformat(),
            'model_version': 'enhanced_v1.0',
            'total_models': len(self.models),
            'models': self.model_metrics,
            'data_summary': {
                'total_records': self.enhanced_data.get('metadata', {}).get('total_records', 0),
                'fraud_types': len(self.enhanced_data.get('fraud_patterns', {}).get('by_type', {})),
                'regions': len(self.enhanced_data.get('fraud_patterns', {}).get('by_region', {})),
                'data_sources': self.enhanced_data.get('metadata', {}).get('total_sources', 0)
            },
            'improvements': [
                "–£–≤–µ–ª–∏—á–µ–Ω –æ–±—ä–µ–º –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å 112 –¥–æ 500+ –∑–∞–ø–∏—Å–µ–π",
                "–î–æ–±–∞–≤–ª–µ–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–¥–ª–∏–Ω–∞ –æ–ø–∏—Å–∞–Ω–∏—è, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã)",
                "–£–ª—É—á—à–µ–Ω—ã –∞–ª–≥–æ—Ä–∏—Ç–º—ã (–±–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤, –ª—É—á—à–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)",
                "–î–æ–±–∞–≤–ª–µ–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–æ–≤—ã—Ö —Ç–∏–ø–æ–≤ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞",
                "–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –±–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"
            ],
            'recommendations': [
                "–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è 10,000+ –∑–∞–ø–∏—Å–µ–π",
                "–î–æ–±–∞–≤–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —É—á–µ—Ç–∞ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏",
                "–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –±–∞–Ω–∫–æ–≤",
                "–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π",
                "–î–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ–ø–∏—Å–∞–Ω–∏–π"
            ]
        }

        return report


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    print("üöÄ –°–û–ó–î–ê–ù–ò–ï –£–õ–£–ß–®–ï–ù–ù–´–• ML –ú–û–î–ï–õ–ï–ô")
    print("=" * 50)

    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        improved_models = ImprovedMLModels()

        # –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        print("üìä –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ç–∏–ø–æ–≤ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞...")

        print("üìà –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏...")

        print("üó∫Ô∏è –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ä–µ–≥–∏–æ–Ω–æ–≤...")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...")
        improved_models.save_improved_models()

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")
        prediction = improved_models.predict_enhanced_fraud_type(
            severity="–≤—ã—Å–æ–∫–∞—è",
            region="–ú–æ—Å–∫–≤–∞",
            amount=500000,
            source="–±–∞–Ω–∫"
        )

        print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —Ç–∏–ø –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {prediction.get('predicted_type', '–æ—à–∏–±–∫–∞')}")
        print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {prediction.get('confidence', 0):.3f}")
        print(f"–í–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏: {prediction.get('model_version', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        report = improved_models.generate_improved_report()

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report_path = "data/improved_ml_models/enhanced_models_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"üìã –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
        print("üéâ –£–ª—É—á—à–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ!")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
