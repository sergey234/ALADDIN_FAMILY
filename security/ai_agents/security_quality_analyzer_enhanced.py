#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–¶–ï–õ–ï–í–ê–Ø –°–ò–°–¢–ï–ú–ê –ö–ê–ß–ï–°–¢–í–ê –î–õ–Ø –°–ò–°–¢–ï–ú–´ –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–ò ALADDIN
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¢–û–õ–¨–ö–û –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏:
- –ú–µ–Ω–µ–¥–∂–µ—Ä—ã (8 –∫–ª–∞—Å—Å–æ–≤)
- –ê–≥–µ–Ω—Ç—ã (8 –∫–ª–∞—Å—Å–æ–≤)
- –ë–æ—Ç—ã (8 –∫–ª–∞—Å—Å–æ–≤)
- SFM (Safe Function Manager)
- –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
"""

import ast
import subprocess
import json
import sys
import os
from datetime import datetime
from typing import Dict, Any, List, Tuple
import concurrent.futures
import time

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –î–õ–Ø –°–ò–°–¢–ï–ú–´ –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–ò ---
MAX_LINE_LENGTH = 79
FLAKE8_SAFE_CODES = ["E501", "E302", "E305", "W292", "W293", "W291", "E128", "E129"]

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã ALADDIN
SECURITY_KEYWORDS = [
    "encrypt",
    "decrypt",
    "auth",
    "authenticate",
    "authorize",
    "permission",
    "security",
    "hash",
    "ssl",
    "tls",
    "cipher",
    "key",
    "token",
    "session",
    "firewall",
    "intrusion",
    "threat",
    "vulnerability",
    "malware",
    "virus",
    "circuit_breaker",
    "monitoring",
    "alert",
    "incident",
    "response",
    "compliance",
    "audit",
    "log",
    "trace",
    "forensic",
    "analysis",
]

# –ö–ª–∞—Å—Å—ã —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
SECURITY_CLASSES = [
    "SafeFunctionManager",
    "AnalyticsManager",
    "DashboardManager",
    "MonitorManager",
    "ReportManager",
    "APIGateway",
    "LoadBalancer",
    "UniversalPrivacyManager",
    "BehavioralAnalysisAgent",
    "ThreatDetectionAgent",
    "PasswordSecurityAgent",
    "IncidentResponseAgent",
    "ThreatIntelligenceAgent",
    "NetworkSecurityAgent",
    "DataProtectionAgent",
    "ComplianceAgent",
    "MobileNavigationBot",
    "GamingSecurityBot",
    "EmergencyResponseBot",
    "ParentalControlBot",
    "NotificationBot",
    "WhatsAppSecurityBot",
    "TelegramSecurityBot",
    "InstagramSecurityBot",
    "CircuitBreakerMain",
]

# –§—É–Ω–∫—Ü–∏–∏ SFM –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
SFM_FUNCTIONS = [
    "register_function",
    "unregister_function",
    "enable_function",
    "disable_function",
    "get_function_status",
    "execute_function",
    "validate_function",
    "backup_function",
    "restore_function",
    "monitor_function",
    "analyze_function",
    "optimize_function",
]

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---


def run_command(cmd: List[str]) -> Tuple[int, str, str]:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–º–∞–Ω–¥—É –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–¥ –≤—ã—Ö–æ–¥–∞, stdout, stderr."""
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=False)
        return result.returncode, result.stdout, result.stderr
    except Exception as e:
        return 1, "", str(e)


def is_security_file(file_path: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏."""
    # –ò—Å–∫–ª—é—á–∞–µ–º –±—ç–∫–∞–ø—ã –∏ —Å–∫—Ä–∏–ø—Ç—ã
    if any(exclude in file_path.lower() for exclude in ["backup", "script", "test", "temp", "old"]):
        return False

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
    security_dirs = ["security/", "ai_agents/", "bots/", "managers/", "agents/"]
    if not any(security_dir in file_path for security_dir in security_dirs):
        return False

    return True


def classify_flake8_error(error_code: str) -> Dict[str, str]:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –æ—à–∏–±–∫—É flake8 –ø–æ —É—Ä–æ–≤–Ω—é –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏."""
    if error_code in FLAKE8_SAFE_CODES:
        severity = "low" if error_code.startswith("W") else "medium"
        return {"security_level": "safe", "severity": severity, "category": "formatting", "auto_fixable": True}
    elif error_code.startswith("F"):  # F401, F821 etc.
        return {"security_level": "manual", "severity": "high", "category": "fatal", "auto_fixable": False}
    else:
        return {"security_level": "manual", "severity": "medium", "category": "error", "auto_fixable": False}


def get_flake8_suggestion(error: Dict[str, Any]) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é –¥–ª—è –æ—à–∏–±–∫–∏ flake8."""
    code = error["code"]
    message = error["message"]
    line = error["line"]

    if code == "E501":
        return f"–°—Ç—Ä–æ–∫–∞ {line} —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è. –†–∞–∑–±–µ–π—Ç–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –∏—Å–ø–æ–ª—å–∑—É—è \\ –∏–ª–∏ —Å–∫–æ–±–∫–∏"
    elif code == "E302":
        return f"–°—Ç—Ä–æ–∫–∞ {line}: –û–∂–∏–¥–∞–µ—Ç—Å—è 2 –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –ø–µ—Ä–µ–¥ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º"
    elif code == "E305":
        return f"–°—Ç—Ä–æ–∫–∞ {line}: –û–∂–∏–¥–∞–µ—Ç—Å—è 2 –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"
    elif code == "W292":
        return "–§–∞–π–ª –Ω–µ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ø–µ—Ä–µ–≤–æ–¥–æ–º —Å—Ç—Ä–æ–∫–∏. –î–æ–±–∞–≤—å—Ç–µ \\n –≤ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞"
    elif code == "W293":
        return f"–°—Ç—Ä–æ–∫–∞ {line} —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ–±–µ–ª—ã –≤ –∫–æ–Ω—Ü–µ. –£–¥–∞–ª–∏—Ç–µ –∏—Ö"
    elif code == "W291":
        return f"–°—Ç—Ä–æ–∫–∞ {line} —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ–±–µ–ª—ã –≤ –∫–æ–Ω—Ü–µ. –£–¥–∞–ª–∏—Ç–µ –∏—Ö"
    elif code.startswith("F4"):  # F401
        return f"–°—Ç—Ä–æ–∫–∞ {line}: –ò–º–ø–æ—Ä—Ç –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è. –£–¥–∞–ª–∏—Ç–µ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ"
    elif code.startswith("F8"):  # F821
        return f"–°—Ç—Ä–æ–∫–∞ {line}: –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –∏–º—è. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –∏–ª–∏ –∏–º–ø–æ—Ä—Ç"
    elif code.startswith("E1"):  # E128, E129
        return f"–°—Ç—Ä–æ–∫–∞ {line}: –ü—Ä–æ–±–ª–µ–º–∞ —Å –æ—Ç—Å—Ç—É–ø–∞–º–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ PEP8"
    else:
        return f"–°—Ç—Ä–æ–∫–∞ {line}: {message}"


# --- –§—É–Ω–∫—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ ---


def analyze_syntax(file_path: str) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞."""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            source = f.read()
        ast.parse(source)
        return {"status": "success", "message": "–°–∏–Ω—Ç–∞–∫—Å–∏—Å –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω"}
    except SyntaxError as e:
        return {
            "status": "error",
            "message": f"–û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞: {e}",
            "security_level": "critical",
            "severity": "high",
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞: {e}",
            "security_level": "critical",
            "severity": "high",
        }


def analyze_imports(file_path: str) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ –∏–º–ø–æ—Ä—Ç–æ–≤."""
    file_dir = os.path.dirname(os.path.abspath(file_path))
    module_name = os.path.basename(file_path).replace(".py", "")
    original_sys_path = list(sys.path)
    if file_dir not in sys.path:
        sys.path.insert(0, file_dir)
    try:
        __import__(module_name)
        return {"status": "success", "message": "–ò–º–ø–æ—Ä—Ç—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã"}
    except ImportError as e:
        return {"status": "error", "message": f"–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}", "security_level": "critical", "severity": "high"}
    except Exception as e:
        return {
            "status": "error",
            "message": f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}",
            "security_level": "critical",
            "severity": "high",
        }
    finally:
        sys.path = original_sys_path


def analyze_flake8(file_path: str) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ flake8."""
    _, stdout, _ = run_command(["flake8", file_path])
    errors = []
    for line in stdout.splitlines():
        parts = line.split(":", 3)
        if len(parts) == 4:
            error_code = parts[3].strip().split(" ")[0]
            classification = classify_flake8_error(error_code)
            errors.append(
                {
                    "line": int(parts[1]),
                    "col": int(parts[2]),
                    "code": error_code,
                    "message": parts[3].strip(),
                    **classification,
                }
            )
    return {
        "status": "success" if not errors else "warning",
        "errors": errors,
        "message": f"–ù–∞–π–¥–µ–Ω–æ {len(errors)} –æ—à–∏–±–æ–∫ flake8",
    }


def analyze_security_keywords(file_path: str) -> Dict[str, Any]:
    """–ü–æ–∏—Å–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏."""
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()
    found_keywords = []
    for keyword in SECURITY_KEYWORDS:
        if keyword in content:
            found_keywords.append(keyword)
    return {
        "status": "success" if found_keywords else "info",
        "found_keywords": found_keywords,
        "message": f"–ù–∞–π–¥–µ–Ω–æ {len(found_keywords)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
    }


def analyze_security_classes(file_path: str) -> Dict[str, Any]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–ª–∞—Å—Å–æ–≤ —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏."""
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()
    found_classes = []
    for class_name in SECURITY_CLASSES:
        if f"class {class_name}" in content:
            found_classes.append(class_name)
    return {
        "status": "success" if found_classes else "warning",
        "found_classes": found_classes,
        "message": f"–ù–∞–π–¥–µ–Ω–æ {len(found_classes)} –∫–ª–∞—Å—Å–æ–≤ —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
    }


def analyze_sfm_functions(file_path: str) -> Dict[str, Any]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ SFM —Ä–µ–µ—Å—Ç—Ä–∞ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π."""
    # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    file_name = os.path.basename(file_path).replace(".py", "")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º SFM —Ä–µ–µ—Å—Ç—Ä
    sfm_registry_path = "data/sfm/function_registry.json"
    if os.path.exists(sfm_registry_path):
        try:
            with open(sfm_registry_path, "r", encoding="utf-8") as f:
                sfm_data = json.load(f)

            # –ü–æ–ª—É—á–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É SFM
            functions = sfm_data.get("functions", {})
            total_functions = len(functions)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ –ª–∏ —Ç–µ–∫—É—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è
            is_registered = file_name in functions
            function_data = functions.get(file_name, {}) if is_registered else {}

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç–∞—Ç—É—Å–∞–º
            active_functions = len([f for f in functions.values() if f.get("status") == "active"])
            critical_functions = len([f for f in functions.values() if f.get("is_critical", False)])

            return {
                "status": "success",
                "total_sfm_functions": total_functions,
                "active_functions": active_functions,
                "critical_functions": critical_functions,
                "is_current_registered": is_registered,
                "current_function_data": function_data,
                "message": (
                    f"SFM —Ä–µ–µ—Å—Ç—Ä: {total_functions} —Ñ—É–Ω–∫—Ü–∏–π, "
                    f"{active_functions} –∞–∫—Ç–∏–≤–Ω—ã—Ö, {critical_functions} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö"
                ),
            }
        except Exception as e:
            return {"status": "error", "total_sfm_functions": 0, "message": f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è SFM —Ä–µ–µ—Å—Ç—Ä–∞: {e}"}
    else:
        return {"status": "error", "total_sfm_functions": 0, "message": "SFM —Ä–µ–µ—Å—Ç—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω"}


def analyze_docstrings(file_path: str) -> Dict[str, Any]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è docstrings."""
    with open(file_path, "r", encoding="utf-8") as f:
        tree = ast.parse(f.read())
    missing_docstrings = []
    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):
            if not (ast.get_docstring(node)):
                missing_docstrings.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç docstring –¥–ª—è {node.name} (—Å—Ç—Ä–æ–∫–∞ {node.lineno})")
    return {
        "status": "success" if not missing_docstrings else "warning",
        "missing_docstrings": missing_docstrings,
        "message": f"–ù–∞–π–¥–µ–Ω–æ {len(missing_docstrings)} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö docstring",
    }


def analyze_type_hints(file_path: str) -> Dict[str, Any]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è type hints (–±–∞–∑–æ–≤–∞—è)."""
    with open(file_path, "r", encoding="utf-8") as f:
        tree = ast.parse(f.read())
    missing_type_hints = []
    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            if not node.returns:
                missing_type_hints.append(
                    f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–∏–ø –≤–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ {node.name} (—Å—Ç—Ä–æ–∫–∞ {node.lineno})"
                )
            for arg in node.args.args:
                if arg.arg != "self" and not arg.annotation:
                    missing_type_hints.append(
                        f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–∏–ø –¥–ª—è –∞—Ä–≥—É–º–µ–Ω—Ç–∞ '{arg.arg}' –≤ —Ñ—É–Ω–∫—Ü–∏–∏ {node.name} (—Å—Ç—Ä–æ–∫–∞ {node.lineno})"
                    )
    return {
        "status": "success" if not missing_type_hints else "warning",
        "missing_type_hints": missing_type_hints,
        "message": f"–ù–∞–π–¥–µ–Ω–æ {len(missing_type_hints)} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö type hints",
    }


def analyze_complexity(file_path: str) -> Dict[str, Any]:
    """–ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—É–Ω–∫—Ü–∏–π/–∫–ª–∞—Å—Å–æ–≤)."""
    with open(file_path, "r", encoding="utf-8") as f:
        tree = ast.parse(f.read())
    num_functions = 0
    num_classes = 0
    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            num_functions += 1
        elif isinstance(node, ast.ClassDef):
            num_classes += 1
    return {
        "status": "info",
        "num_functions": num_functions,
        "num_classes": num_classes,
        "message": f"–§—É–Ω–∫—Ü–∏–π: {num_functions}, –ö–ª–∞—Å—Å–æ–≤: {num_classes}",
    }


def analyze_file_size(file_path: str) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫."""
    size_bytes = os.path.getsize(file_path)
    lines = sum(1 for line in open(file_path, "r", encoding="utf-8"))
    return {
        "status": "info",
        "size_bytes": size_bytes,
        "lines": lines,
        "message": f"–†–∞–∑–º–µ—Ä: {size_bytes} –±–∞–π—Ç, –°—Ç—Ä–æ–∫: {lines}",
    }


def analyze_dependencies(file_path: str) -> Dict[str, Any]:
    """–ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (–∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏)."""
    with open(file_path, "r", encoding="utf-8") as f:
        tree = ast.parse(f.read())
    dependencies = set()
    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for alias in node.names:
                dependencies.add(alias.name.split(".")[0])
        elif isinstance(node, ast.ImportFrom):
            dependencies.add(node.module.split(".")[0])
    return {"status": "info", "dependencies": list(dependencies), "message": f"–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: {len(dependencies)}"}


# --- –û—Å–Ω–æ–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ ---


def security_quality_analyzer(file_path: str) -> Dict[str, Any]:
    """
    –¶–ï–õ–ï–í–ê–Ø –°–ò–°–¢–ï–ú–ê –ö–ê–ß–ï–°–¢–í–ê –î–õ–Ø –°–ò–°–¢–ï–ú–´ –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–ò ALADDIN.
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¢–û–õ–¨–ö–û –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.
    """
    print("üõ°Ô∏è –¶–ï–õ–ï–í–ê–Ø –°–ò–°–¢–ï–ú–ê –ö–ê–ß–ï–°–¢–í–ê –î–õ–Ø –°–ò–°–¢–ï–ú–´ –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–ò ALADDIN")
    print("=" * 80)
    print(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª: {file_path}\n")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ —Å–∏—Å—Ç–µ–º–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
    if not is_security_file(file_path):
        print("‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏!")
        print("‚ùå –ò—Å–∫–ª—é—á–µ–Ω—ã: –±—ç–∫–∞–ø—ã, —Å–∫—Ä–∏–ø—Ç—ã, —Ç–µ—Å—Ç—ã, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã")
        print("‚úÖ –†–∞–∑—Ä–µ—à–µ–Ω—ã: security/, ai_agents/, bots/, managers/, agents/")
        return {"error": "–§–∞–π–ª –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"}

    start_time = time.time()

    analysis_tasks = {
        "syntax": analyze_syntax,
        "imports": analyze_imports,
        "flake8": analyze_flake8,
        "security_keywords": analyze_security_keywords,
        "security_classes": analyze_security_classes,
        "sfm_functions": analyze_sfm_functions,
        "docstrings": analyze_docstrings,
        "type_hints": analyze_type_hints,
        "complexity": analyze_complexity,
        "file_info": analyze_file_size,
        "dependencies": analyze_dependencies,
    }

    results = {}
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future_to_analysis = {executor.submit(task_func, file_path): name for name, task_func in analysis_tasks.items()}
        for future in concurrent.futures.as_completed(future_to_analysis):
            name = future_to_analysis[future]
            try:
                results[name] = future.result()
            except Exception as exc:
                results[name] = {"status": "error", "message": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ {name}: {exc}"}

    end_time = time.time()
    total_time = end_time - start_time

    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    flake8_errors = results.get("flake8", {}).get("errors", [])
    total_errors = len(flake8_errors)
    safe_errors = [e for e in flake8_errors if e.get("security_level") == "safe"]
    safe_to_fix_errors = len(safe_errors)
    manual_review_errors = total_errors - safe_to_fix_errors

    security_keywords = results.get("security_keywords", {}).get("found_keywords", [])
    security_keywords_found = len(security_keywords)

    security_classes = results.get("security_classes", {}).get("found_classes", [])
    security_classes_found = len(security_classes)

    sfm_functions = results.get("sfm_functions", {})
    sfm_total_functions = sfm_functions.get("total_sfm_functions", 0)
    sfm_active_functions = sfm_functions.get("active_functions", 0)
    sfm_critical_functions = sfm_functions.get("critical_functions", 0)
    is_sfm_registered = sfm_functions.get("is_current_registered", False)

    # –†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –±–∞–ª–ª–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
    quality_score = 100.0

    syntax_status = results.get("syntax", {}).get("status")
    if syntax_status == "error":
        quality_score -= 30

    imports_status = results.get("imports", {}).get("status")
    if imports_status == "error":
        quality_score -= 20

    # –ö–∞–∂–¥–∞—è –æ—à–∏–±–∫–∞ flake8 —Å–Ω–∏–∂–∞–µ—Ç –±–∞–ª–ª
    quality_score -= total_errors * 1.5

    missing_docstrings = results.get("docstrings", {}).get("missing_docstrings")
    if missing_docstrings:
        quality_score -= len(missing_docstrings) * 0.5

    missing_type_hints = results.get("type_hints", {}).get("missing_type_hints")
    if missing_type_hints:
        quality_score -= len(missing_type_hints) * 0.3

    # –ë–æ–Ω—É—Å—ã –∑–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
    if security_classes_found > 0:
        quality_score += 5
    if is_sfm_registered:
        quality_score += 5
    if security_keywords_found > 5:
        quality_score += 5

    quality_score = max(0, min(100, quality_score))  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 0-100

    report = {
        "file": file_path,
        "timestamp": datetime.now().isoformat(),
        "total_analysis_time_seconds": total_time,
        "overall_quality_score": round(quality_score, 1),
        "analysis_details": results,
        "summary": {
            "total_flake8_errors": total_errors,
            "safe_to_fix_errors": safe_to_fix_errors,
            "manual_review_errors": manual_review_errors,
            "security_keywords_found": security_keywords_found,
            "security_classes_found": security_classes_found,
            "sfm_total_functions": sfm_total_functions,
            "sfm_active_functions": sfm_active_functions,
            "sfm_critical_functions": sfm_critical_functions,
            "is_sfm_registered": is_sfm_registered,
            "syntax_ok": results.get("syntax", {}).get("status") == "success",
            "imports_ok": results.get("imports", {}).get("status") == "success",
            "docstrings_ok": results.get("docstrings", {}).get("status") == "success",
            "type_hints_ok": results.get("type_hints", {}).get("status") == "success",
        },
        "recommendations": [],
        "fix_plan": {"safe_auto_fixes": [], "manual_fixes_required": []},
    }

    if not report["summary"]["syntax_ok"]:
        report["recommendations"].append("–ò—Å–ø—Ä–∞–≤—å—Ç–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏.")
    if not report["summary"]["imports_ok"]:
        report["recommendations"].append("–ò—Å–ø—Ä–∞–≤—å—Ç–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –∏–º–ø–æ—Ä—Ç–∞.")
    if total_errors > 0:
        report["recommendations"].append(f"–ò—Å–ø—Ä–∞–≤—å—Ç–µ {total_errors} –æ—à–∏–±–æ–∫ flake8.")
    if security_classes_found == 0:
        report["recommendations"].append("–î–æ–±–∞–≤—å—Ç–µ –∫–ª–∞—Å—Å—ã —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.")
    if not is_sfm_registered:
        report["recommendations"].append("–ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –≤ SFM —Ä–µ–µ—Å—Ç—Ä–µ.")
    if not report["summary"]["docstrings_ok"]:
        report["recommendations"].append("–î–æ–±–∞–≤—å—Ç–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ docstrings –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.")
    if not report["summary"]["type_hints_ok"]:
        report["recommendations"].append("–î–æ–±–∞–≤—å—Ç–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ type hints –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏.")

    if not report["recommendations"]:
        report["recommendations"].append("–ö–æ–º–ø–æ–Ω–µ–Ω—Ç —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ –æ—Ç–ª–∏—á–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏!")

    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–∞–Ω–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
    for error in results.get("flake8", {}).get("errors", []):
        if error.get("auto_fixable"):
            report["fix_plan"]["safe_auto_fixes"].append(
                {
                    "line": error["line"],
                    "code": error["code"],
                    "message": error["message"],
                    "suggestion": get_flake8_suggestion(error),
                }
            )
        else:
            report["fix_plan"]["manual_fixes_required"].append(
                {
                    "line": error["line"],
                    "code": error["code"],
                    "message": error["message"],
                    "suggestion": get_flake8_suggestion(error),
                    "security_level": error["security_level"],
                }
            )

    print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
    print(f"üìä –û–±—â–∏–π –±–∞–ª–ª: {report['overall_quality_score']}")
    print(f"üìÅ –§–∞–π–ª: {report['file']}")
    print(f"üìè –°—Ç—Ä–æ–∫: {report['analysis_details']['file_info']['lines']}")
    print(f"üìä –†–∞–∑–º–µ—Ä: {report['analysis_details']['file_info']['size_bytes']} –±–∞–π—Ç")
    print(f"üõ°Ô∏è –ö–ª–∞—Å—Å–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: {security_classes_found}")
    print(f"üîß –í—Å–µ–≥–æ SFM —Ñ—É–Ω–∫—Ü–∏–π: {sfm_total_functions}")
    print(f"üîß –ê–∫—Ç–∏–≤–Ω—ã—Ö SFM —Ñ—É–Ω–∫—Ü–∏–π: {sfm_active_functions}")
    print(f"üîß –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö SFM —Ñ—É–Ω–∫—Ü–∏–π: {sfm_critical_functions}")
    print(f"üîß –¢–µ–∫—É—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤ SFM: {'‚úÖ –î–∞' if is_sfm_registered else '‚ùå –ù–µ—Ç'}")
    print(f"üîë –ö–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: {security_keywords_found}")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
    report_filename = f"security_quality_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    report_path = os.path.join("formatting_work", report_filename)
    os.makedirs("formatting_work", exist_ok=True)
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    print(f"üìÑ –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")

    return report


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python3 security_quality_analyzer.py <file_path>")
        print("–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¢–û–õ–¨–ö–û –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏!")
        sys.exit(1)

    file_to_analyze = sys.argv[1]
    if not os.path.exists(file_to_analyze):
        print(f"–û—à–∏–±–∫–∞: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {file_to_analyze}")
        sys.exit(1)

    security_quality_analyzer(file_to_analyze)
